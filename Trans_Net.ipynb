{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch110\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 使用第二块GPU（从0开始）\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from modelDesign import *\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from math import *\n",
    "import torch.utils.data as Data\n",
    "#from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提出的端到端Trans-Net在Nc=6的条件下的网络在不同反馈比特下的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.type(torch.uint8)\n",
    "\n",
    "    def integer2bit(integer, num_bits=B * 2):\n",
    "        dtype = integer.type()\n",
    "        exponent_bits = -torch.arange(-(num_bits - 1), 1).type(dtype)\n",
    "        exponent_bits = exponent_bits.repeat(integer.shape + (1,))\n",
    "        out = integer.unsqueeze(-1) // 2 ** exponent_bits\n",
    "        return (out - (out % 1)) % 2\n",
    "\n",
    "    bit = integer2bit(Num_)\n",
    "    bit = (bit[:, :, B:]).reshape(-1, Num_.shape[1] * B)\n",
    "    return bit.type(torch.float32)\n",
    "\n",
    "\n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.type(torch.float32)\n",
    "    Bit_ = torch.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = torch.zeros(Bit_[:, :, 0].shape).cuda()\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return num\n",
    "\n",
    "\n",
    "class Quantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = torch.round(x * step - 0.5)\n",
    "        out = Num2Bit(out, B)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of constant arguments to forward must be None.\n",
    "        # Gradient of a number is the sum of its four bits.\n",
    "        b, _ = grad_output.shape\n",
    "        grad_num = torch.sum(grad_output.reshape(b, -1, ctx.constant), dim=2)\n",
    "        return grad_num, None\n",
    "\n",
    "\n",
    "class Dequantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = Bit2Num(x, B)\n",
    "        out = (out + 0.5) / step\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        # repeat the gradient of a Num for four time.\n",
    "        #b, c = grad_output.shape\n",
    "        #grad_bit = grad_output.repeat(1, 1, ctx.constant)\n",
    "        grad_bit = grad_output.repeat_interleave(ctx.constant,dim=1)\n",
    "        return grad_bit, None\n",
    "\n",
    "\n",
    "class QuantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Quantization.apply(x, self.B)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DequantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(DequantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Dequantization.apply(x, self.B)\n",
    "        return out\n",
    "\n",
    "class MyLoss_OFDM(torch.nn.Module):  # 输入是信道和整个F,输出是频谱效率\n",
    "    def __init__(self):\n",
    "        super(MyLoss_OFDM, self).__init__()\n",
    "\n",
    "    def forward(self, H0, out, parm_set):  # H0第0个维度是样本 第1个维度是用户，第2个维度是子载波，第3个维度是天线\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "\n",
    "        H = H0.permute(0, 2, 1, 3)  # H第0个维度是样本 第1个维度是子载波，第2个维度是用户，第3个维度是天线\n",
    "        num = out.shape[0]\n",
    "        Nc = H.shape[1]\n",
    "        H_real = H[:, :, :, 0:Nt]\n",
    "        H_imag = H[:, :, :, Nt:2 * Nt]\n",
    "        Hs = torch.zeros([num, Nc, K, Nt * 2])\n",
    "        Hs = Hs.cuda()\n",
    "        Hs[:, :, 0:K, 0:Nt] = H_real\n",
    "        # Hs[:, :, K:2 * K, Nt:2 * Nt] = H_real\n",
    "        Hs[:, :, 0:K, Nt:2 * Nt] = H_imag\n",
    "        # Hs[:, :, K:2 * K, 0:Nt] = -H_imag\n",
    "\n",
    "        F = torch.zeros([num, Nc, Nt * 2, K * 2])\n",
    "        F = F.cuda()\n",
    "        F[:, :, 0:Nt, 0:K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, K:2 * K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, 0:Nt, K:2 * K] = out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, 0:K] = -out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        R = 0\n",
    "        Hk = torch.matmul(Hs, F)\n",
    "        noise = 1 / snr\n",
    "        for i in range(K):\n",
    "            signal = Hk[:, :, i, i] * Hk[:, :, i, i] + Hk[:, :, i, i + K] * Hk[:, :, i, i + K]\n",
    "            interference = torch.zeros(num, Nc)\n",
    "            interference = interference.cuda()\n",
    "            for j in range(K):\n",
    "                if j != i:\n",
    "                    interference = interference + Hk[:, :, i, j] * Hk[:, :, i, j] + Hk[:, :, i, j + K] * Hk[:, :, i,\n",
    "                                                                                                         j + K]\n",
    "            SINR = signal / (noise + interference)\n",
    "            R = R + torch.sum(torch.log2(1 + SINR))\n",
    "        R = -R / num / Nc\n",
    "        return R\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    x = x * (torch.tanh(F.softplus(x)))\n",
    "    return x\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    #         print(\"Mish activation loaded...\")\n",
    "    def forward(self, x):\n",
    "        x = x * (torch.tanh(F.softplus(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class RES_BLOCK(nn.Module):  # 输入信道输出 量化后的B比特反馈信息\n",
    "    def __init__(self, channel_list):\n",
    "        super(RES_BLOCK, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=channel_list[0],  # 图片通道数\n",
    "                out_channels=channel_list[1],  # filter数量\n",
    "                kernel_size=(5, 1),  # filter大小\n",
    "                stride=1,  # 扫描步进\n",
    "                padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_list[1], eps=1e-05, momentum=0.1, affine=True),\n",
    "            Mish(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=channel_list[1],  # 图片通道数\n",
    "                out_channels=channel_list[2],  # filter数量\n",
    "                kernel_size=(5, 1),  # filter大小\n",
    "                stride=1,  # 扫描步进\n",
    "                padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_list[2], eps=1e-05, momentum=0.1, affine=True),\n",
    "            Mish(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=channel_list[2],  # 图片通道数\n",
    "                out_channels=channel_list[0],  # filter数量\n",
    "                kernel_size=(5, 1),  # filter大小\n",
    "                stride=1,  # 扫描步进\n",
    "                padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_list[0], eps=1e-05, momentum=0.1, affine=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_ini):\n",
    "        x = self.conv1(x_ini)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = mish(x + x_ini)\n",
    "        return x\n",
    "\n",
    "\n",
    "def DFT_matrix(N):\n",
    "    i, j = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    omega = np.exp(- 2 * pi * 1J / N)\n",
    "    W = np.power(omega, i * j) / sqrt(N)\n",
    "    return np.mat(W)\n",
    "\n",
    "def Hermitian(X):#torch矩阵共轭转置\n",
    "    X = torch.real(X) - 1j*torch.imag(X)\n",
    "    return X.transpose(-1,-2)  #（3,4）变（4,3）\n",
    "\n",
    "Nc = 32\n",
    "Nt = 64\n",
    "W = DFT_matrix(Nc)\n",
    "W = torch.tensor(W)\n",
    "W = Hermitian(W)\n",
    "W_real = np.real(W).cuda()\n",
    "W_imag = np.imag(W).cuda()\n",
    "W_real = W_real.float()\n",
    "W_imag = W_imag.float()\n",
    "\n",
    "W1 = DFT_matrix(Nt)\n",
    "W_real1 = torch.from_numpy(np.real(W1)).cuda()\n",
    "W_imag1 = torch.from_numpy(np.imag(W1)).cuda()\n",
    "W_real1 = W_real1.float()\n",
    "W_imag1 = W_imag1.float()\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DNN_BS_hyb_OFDM(nn.Module):  # 输入所有用户的KB比特反馈信息,输出预编码\n",
    "    def __init__(self, parm_set):\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "        super(DNN_BS_hyb_OFDM, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv_l1 = conv3x3(2,16)\n",
    "        self.conv_bn_l1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
    "        self.conv_mish_l1 = Mish()\n",
    "\n",
    "        self.conv_l2 = conv3x3(16,2)\n",
    "        self.conv_bn_l2 = nn.BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
    "        self.conv_mish_l2 = Mish()\n",
    "\n",
    "        self.FC_l1 = nn.Linear(K*Nc * 2 * Nt, 1024)  # 全连接\n",
    "        self.bn_l1 = nn.BatchNorm1d(1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.FC_l2 = nn.Linear(1024, 512)  # 全连接\n",
    "        self.bn_l2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.FC_l3 = nn.Linear(512, K*B)  # 全连接\n",
    "        self.bn_l3 = nn.BatchNorm1d(K*B)\n",
    "        self.QL = QuantizationLayer(1)  # 1bit 量化\n",
    "\n",
    "\n",
    "        self.DQL = DequantizationLayer(1)  # 解1bit 量化\n",
    "\n",
    "        self.FC1 = nn.Linear(K * B, Nc*K*Nt*2)  # 全连接\n",
    "        self.bn1 = nn.BatchNorm1d(Nc*K*Nt*2)\n",
    "        self.mish1 = Mish()\n",
    "\n",
    "        # self.FC2 = nn.Linear(2048, 1024)  # 全连接\n",
    "        # self.bn2 = nn.BatchNorm1d(1024)\n",
    "        # self.mish2 = Mish()\n",
    "        #\n",
    "        # self.FC3 = nn.Linear(1024, 2 * K * K * Nc + K * Nt)  # 全连接\n",
    "        # self.bn3 = nn.BatchNorm1d(2 * K * K * Nc)\n",
    "        # self.mish3 = Mish()\n",
    "        #\n",
    "        # self.res1 = RES_BLOCK([2 * K * K, 256, 512])\n",
    "        # self.conv = nn.Conv2d(\n",
    "        #     in_channels=2 * K * K,  # 图片通道数\n",
    "        #     out_channels=2 * K * K,  # filter数量\n",
    "        #     kernel_size=(5, 1),  # filter大小\n",
    "        #     stride=1,  # 扫描步进\n",
    "        #     padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "        # )\n",
    "        # 最后用sigm\n",
    "\n",
    "        src_vocab_size_0 = int(2*Nt*K)\n",
    "        model_dimension_0 = 128\n",
    "        dropout_probability_0 = 0.1\n",
    "        number_of_heads_0 = 8\n",
    "        log_attention_weights_0 = False\n",
    "        number_of_layers_0 = 3\n",
    "        self.src_embedding_0 = Embedding(src_vocab_size_0, model_dimension_0)  # 对输入进行embedding\n",
    "        self.src_pos_embedding_0 = PositionalEncoding(model_dimension_0, dropout_probability_0)\n",
    "        mha_0 = MultiHeadedAttention(model_dimension_0, number_of_heads_0, dropout_probability_0, log_attention_weights_0)\n",
    "        pwn_0 = PositionwiseFeedForwardNet(model_dimension_0, dropout_probability_0)\n",
    "        encoder_layer_0 = EncoderLayer(model_dimension_0, dropout_probability_0, mha_0, pwn_0)\n",
    "        self.trans_encoder_0 = Trans_Encoder(encoder_layer_0, number_of_layers_0, src_vocab_size_0)\n",
    "\n",
    "        # self.FC3 = nn.Linear(K*Nc*2, 2 * K * K * Nc + K * Nt)  # 全连接\n",
    "        # self.bn3 = nn.BatchNorm1d(2 * K * K * Nc)\n",
    "        # self.mish3 = Mish()\n",
    "        src_vocab_size_1 = int(2*Nt*Nc)\n",
    "        model_dimension_1 = 128\n",
    "        dropout_probability_1 = 0.1\n",
    "        number_of_heads_1 = 8\n",
    "        log_attention_weights_1 = False\n",
    "        number_of_layers_1 = 3\n",
    "        self.src_embedding_1 = Embedding(src_vocab_size_1, model_dimension_1)  # 对输入进行embedding\n",
    "        self.src_pos_embedding_1 = PositionalEncoding(model_dimension_1, dropout_probability_1)\n",
    "        mha_1 = MultiHeadedAttention(model_dimension_1, number_of_heads_1, dropout_probability_1, log_attention_weights_1)\n",
    "        pwn_1 = PositionwiseFeedForwardNet(model_dimension_1, dropout_probability_1)\n",
    "        encoder_layer_1 = EncoderLayer(model_dimension_1, dropout_probability_1, mha_1, pwn_1)\n",
    "        self.trans_encoder_1 = Trans_Encoder(encoder_layer_1, number_of_layers_1, Nt)\n",
    "\n",
    "        src_vocab_size = int(2*Nt*K)\n",
    "        model_dimension = 128\n",
    "        dropout_probability = 0.1\n",
    "        number_of_heads = 8\n",
    "        log_attention_weights = False\n",
    "        number_of_layers = 3\n",
    "        self.src_embedding = Embedding(src_vocab_size, model_dimension)  # 对输入进行embedding\n",
    "        self.src_pos_embedding = PositionalEncoding(model_dimension, dropout_probability)\n",
    "        mha = MultiHeadedAttention(model_dimension, number_of_heads, dropout_probability, log_attention_weights)\n",
    "        pwn = PositionwiseFeedForwardNet(model_dimension, dropout_probability)\n",
    "        encoder_layer = EncoderLayer(model_dimension, dropout_probability, mha, pwn)\n",
    "        self.trans_encoder = Trans_Encoder(encoder_layer, number_of_layers, 2*K*K)\n",
    "\n",
    "    def forward(self, h,parm_set):\n",
    "\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "       \n",
    "        \n",
    "        #将信道从空间频率域变到角度延时域\n",
    "        h_real = h[:, :, :, 0:Nt].reshape(-1, K, Nc, Nt, 1)\n",
    "        h_imag = h[:, :, :, Nt:2 * Nt].reshape(-1, K, Nc, Nt, 1)\n",
    "\n",
    "        h_real1 = (torch.matmul(W_real1, h_real) - torch.matmul(W_imag1, h_imag)).reshape(-1, 1, K, Nc, Nt)\n",
    "        h_imag1 = (torch.matmul(W_real1, h_imag) + torch.matmul(W_imag1, h_real)).reshape(-1, 1, K, Nc, Nt)\n",
    "        #h_sum1 = torch.cat((h_real1, h_imag1), 1)\n",
    "\n",
    "        h_real2 = (torch.matmul(W_real, h_real1) - torch.matmul(W_imag, h_imag1)).reshape(-1, 1, K, Nc, Nt)\n",
    "        h_imag2 = (torch.matmul(W_real, h_imag1) + torch.matmul(W_imag, h_real1)).reshape(-1, 1, K, Nc, Nt)\n",
    "        h_sum2 = torch.cat((h_real2, h_imag2), 1)   #(BATCH_SIZE,2,K,Nc,Nt)\n",
    "\n",
    "        h = h_sum2.reshape(-1,K,Nc,2,Nt)\n",
    "\n",
    "        h = h.permute(0,3,1,2,4)\n",
    "        x0 = h.contiguous().view(-1,2,Nc*K,Nt)\n",
    "\n",
    "        x = self.conv_l1(x0)\n",
    "        x = self.conv_bn_l1(x)\n",
    "        x = self.conv_mish_l1(x)\n",
    "\n",
    "        x = self.conv_l2(x)\n",
    "        x = self.conv_bn_l2(x)\n",
    "        x = self.conv_mish_l2(x+x0)\n",
    "\n",
    "        x = x.contiguous().view(-1,2*Nc*K*Nt)\n",
    "\n",
    "        x = self.FC_l1(x)\n",
    "        x = self.bn_l1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.FC_l2(x)\n",
    "        x = self.bn_l2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.FC_l3(x)\n",
    "        x = self.bn_l3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.QL(x)\n",
    "\n",
    "        x = self.DQL(x) - 0.5\n",
    "\n",
    "        x = x.contiguous().view(-1,int(K*B))\n",
    "        x = self.FC1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.mish1(x)\n",
    "\n",
    "        x = x.contiguous().view(-1,int(Nc),int(2*Nt*K))\n",
    "        x = self.src_embedding_0(x)  # get embedding vectors for src token ids\n",
    "        x = self.src_pos_embedding_0(x)  # add positional embedding\n",
    "        x = self.trans_encoder_0(x, src_mask=None)  # forward pass through the\n",
    "        x_ = x.contiguous().view(-1, Nc, int(Nt*2), K)\n",
    "        x_ = x_.permute(0,3,1,2)\n",
    "        x_ = x_.reshape(-1,K,Nc*Nt*2)\n",
    "        x_ = self.src_embedding_1(x_)  # get embedding vectors for src token ids\n",
    "        x_ = self.src_pos_embedding_1(x_)  # add positional embedding\n",
    "        x_ = self.trans_encoder_1(x_, src_mask=None)  # forward pass through the\n",
    "        x_ = x_.permute(0,2,1)\n",
    "        # x = self.FC1(x)\n",
    "        # x = self.bn1(x)\n",
    "        # x = self.mish1(x)\n",
    "        #\n",
    "        # x = self.FC2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.mish2(x)\n",
    "\n",
    "        #x_ini = self.FC3(x)\n",
    "\n",
    "        RF_real = torch.cos(x_).reshape(-1, Nt, K) / sqrt(Nt)\n",
    "        RF_imag = torch.sin(x_).reshape(-1, Nt, K) / sqrt(Nt)\n",
    "\n",
    "        # x = x_ini[:, 0:2 * K * K * Nc]\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.mish3(x)\n",
    "\n",
    "        #x = x.reshape(-1, Nc, 2 * K * K)\n",
    "        src_embeddings_batch = self.src_embedding(x)  # get embedding vectors for src token ids\n",
    "        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch)  # add positional embedding\n",
    "        x = self.trans_encoder(src_embeddings_batch, src_mask=None)  # forward pass through the encoder\n",
    "        # x = self.res1(x)\n",
    "        # x = self.conv(x)\n",
    "\n",
    "        # x = x.transpose(1, 2)\n",
    "\n",
    "        BB_real = x[:, :, 0:K * K].reshape(-1, Nc, K, K)\n",
    "        BB_imag = x[:, :, K * K:2 * K * K].reshape(-1, Nc, K, K)\n",
    "        BB_real = BB_real.permute(1, 0, 2, 3)\n",
    "        BB_imag = BB_imag.permute(1, 0, 2, 3)\n",
    "\n",
    "        F_real = (torch.matmul(RF_real, BB_real) - torch.matmul(RF_imag, BB_imag)).reshape(Nc, -1, K * Nt)\n",
    "        F_imag = (torch.matmul(RF_imag, BB_real) + torch.matmul(RF_imag, BB_real)).reshape(Nc, -1, K * Nt)\n",
    "        F_real = F_real.permute(1, 0, 2)\n",
    "        F_imag = F_imag.permute(1, 0, 2)\n",
    "        F = torch.cat((F_real, F_imag), 2)\n",
    "\n",
    "        F_sigma = torch.sqrt(torch.sum(F * F, [2]))\n",
    "        sigma2 = torch.FloatTensor([sqrt(K)]).cuda()\n",
    "        F = F / F_sigma.reshape(-1, Nc, 1) * torch.min(F_sigma, sigma2).reshape(-1, Nc, 1)\n",
    "\n",
    "        F_real = F[:, :, 0:K * Nt].reshape(-1, K * Nt * Nc)\n",
    "        F_imag = F[:, :, K * Nt:2 * K * Nt].reshape(-1, K * Nt * Nc)\n",
    "        F = torch.cat((F_real, F_imag), 1)\n",
    "\n",
    "        return F\n",
    "\n",
    "class DatasetFolder(Data.Dataset):\n",
    "    def __init__(self, matData):\n",
    "        self.matdata = matData\n",
    "    def __getitem__(self, index):\n",
    "        return self.matdata[index]\n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]\n",
    "\n",
    "\n",
    "class NoamOpt(object):\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "             min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "\n",
    "def train(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):  # N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10) / K\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "\n",
    "    # H_train = torch.load('data/H_train_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_train = H_train[:, 0:K, :, :]\n",
    "    # print(H_train.shape)\n",
    "    # H_test = torch.load('data/H_test_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_test = H_test[:, 0:K, :, :]\n",
    "    # print(H_test.shape)\n",
    "    train = 'H_train_'+str(N)+'.mat'\n",
    "    mat = h5py.File(train)\n",
    "    H_train = mat['H_train']\n",
    "    H_train = np.transpose(H_train, [3, 2, 1, 0])\n",
    "    H_train = H_train.astype('float32')  # 训练变量类型转换\n",
    "    print(H_train.shape)\n",
    "    test = 'H_val_'+str(N)+'.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_val']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "    net_BS = DNN_BS_hyb_OFDM(parm_set).cuda()\n",
    "\n",
    "    print(net_BS)\n",
    "    opt = NoamOpt(256, 1, 4000, torch.optim.Adam(net_BS.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    #optimizer_BS = torch.optim.Adam(net_BS.parameters(), lr=0.001)\n",
    "    #scheduler_BS = torch.optim.lr_scheduler.MultiStepLR(optimizer_BS, milestones=[100, 150], gamma=0.6, last_epoch=-1)\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "    torch_dataset_train = DatasetFolder(H_train)\n",
    "    loader_train = Data.DataLoader(\n",
    "        dataset=torch_dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(EPOCH):\n",
    "        print('========================')\n",
    "        print('lr:%.4e' % opt.optimizer.param_groups[0]['lr'])\n",
    "        train_SE = 0\n",
    "        num_train = 0\n",
    "        test_SE = 0\n",
    "        num_test = 0\n",
    "        for step, b_x in enumerate(loader_train):\n",
    "            num_train = num_train + 1\n",
    "            net_BS.train()  # 训练模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out2 = net_BS(b_x, parm_set)\n",
    "            loss = loss_func1(b_x, out2, parm_set)\n",
    "            train_SE = train_SE - loss\n",
    "            opt.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        train_SE = train_SE / num_train\n",
    "        # scheduler_BS.step()\n",
    "        net_BS.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, b_x in enumerate(loader_test):\n",
    "                num_test = num_test + 1\n",
    "                net_BS.eval()  # 验证模式\n",
    "                b_x = b_x.cuda()\n",
    "                num = b_x.shape[0]\n",
    "                out2 = net_BS(b_x, parm_set)\n",
    "                loss = loss_func1(b_x, out2, parm_set)\n",
    "                test_SE = test_SE - loss\n",
    "            test_SE = test_SE / num_test\n",
    "\n",
    "        time0 = datetime.datetime.now() - start\n",
    "        print('Epoch:', epoch, 'time', time0, 'train SE %.3f' % train_SE.cpu(), 'test SE %.3f' % test_SE.cpu())\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        if test_SE > best_SE:\n",
    "            best_SE = test_SE\n",
    "            torch.save(net_BS,\n",
    "                       './Trans1_' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "            print('Model saved!')\n",
    "        shoulian[epoch] = test_SE.cpu()\n",
    "    print('The best SE is: %.3f' % best_SE.cpu())\n",
    "    print(shoulian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2, 32, 128)\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (conv_l1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l1): Mish()\n",
      "  (conv_l2): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l2): Mish()\n",
      "  (FC_l1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (bn_l1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (FC_l2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn_l2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC_l3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (bn_l3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=128, out_features=8192, bias=True)\n",
      "  (bn1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (src_embedding_0): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_0): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_0): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (src_embedding_1): Embedding(\n",
      "    (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_1): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_1): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (src_embedding): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y\\AppData\\Local\\Temp\\ipykernel_24024\\1130335191.py:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  out = integer.unsqueeze(-1) // 2 ** exponent_bits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 time 0:00:56.815783 train SE 3.285 test SE 5.165\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:00:50.372261 train SE 5.799 test SE 6.651\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:00:48.370613 train SE 6.903 test SE 7.243\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:00:50.638548 train SE 7.259 test SE 7.353\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:00:49.858634 train SE 7.387 test SE 7.391\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:00:48.940107 train SE 7.460 test SE 7.414\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:00:50.157834 train SE 7.506 test SE 7.430\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:00:49.287163 train SE 7.534 test SE 7.448\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:00:50.093008 train SE 7.550 test SE 7.456\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:00:50.892867 train SE 7.553 test SE 7.462\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:00:49.653183 train SE 7.561 test SE 7.465\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:00:50.281503 train SE 7.563 test SE 7.479\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:00:50.318405 train SE 7.563 test SE 7.480\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:00:49.730976 train SE 7.561 test SE 7.477\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:00:50.256570 train SE 7.565 test SE 7.485\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:00:49.530510 train SE 7.565 test SE 7.492\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:00:50.560756 train SE 7.563 test SE 7.497\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:00:50.243604 train SE 7.568 test SE 7.493\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:00:49.455712 train SE 7.568 test SE 7.505\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:00:50.624585 train SE 7.569 test SE 7.502\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:00:50.447061 train SE 7.573 test SE 7.504\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:00:49.492613 train SE 7.581 test SE 7.513\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:00:50.563748 train SE 7.586 test SE 7.518\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:00:49.966346 train SE 7.594 test SE 7.538\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:00:50.414148 train SE 7.605 test SE 7.529\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:00:50.595662 train SE 7.611 test SE 7.538\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:00:49.402853 train SE 7.612 test SE 7.548\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:00:51.292797 train SE 7.621 test SE 7.550\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:00:50.676446 train SE 7.631 test SE 7.553\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:00:50.192741 train SE 7.631 test SE 7.547\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:00:50.422127 train SE 7.636 test SE 7.552\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:00:49.653184 train SE 7.642 test SE 7.553\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:00:51.009555 train SE 7.648 test SE 7.554\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:00:50.869929 train SE 7.649 test SE 7.561\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:00:49.817744 train SE 7.655 test SE 7.560\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:00:50.658494 train SE 7.658 test SE 7.562\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:00:50.433097 train SE 7.658 test SE 7.564\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:00:49.758901 train SE 7.664 test SE 7.571\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:00:50.922788 train SE 7.667 test SE 7.559\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:00:49.623264 train SE 7.668 test SE 7.564\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:00:51.217997 train SE 7.672 test SE 7.563\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:00:50.440079 train SE 7.673 test SE 7.562\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:00:49.594341 train SE 7.678 test SE 7.575\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:00:51.008559 train SE 7.681 test SE 7.571\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:00:50.254575 train SE 7.683 test SE 7.574\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:00:50.234628 train SE 7.685 test SE 7.572\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:00:50.954703 train SE 7.688 test SE 7.579\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:00:49.838687 train SE 7.690 test SE 7.574\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:00:50.726313 train SE 7.691 test SE 7.570\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:00:50.064085 train SE 7.695 test SE 7.574\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:00:49.819739 train SE 7.695 test SE 7.583\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:00:50.874915 train SE 7.698 test SE 7.576\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:00:49.785829 train SE 7.700 test SE 7.585\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:00:50.176783 train SE 7.701 test SE 7.580\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:00:50.675449 train SE 7.703 test SE 7.584\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:00:49.585366 train SE 7.705 test SE 7.582\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:00:50.821059 train SE 7.705 test SE 7.584\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:00:50.216676 train SE 7.708 test SE 7.586\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:00:50.138885 train SE 7.711 test SE 7.588\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:00:50.303444 train SE 7.712 test SE 7.577\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:00:49.918474 train SE 7.712 test SE 7.582\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:00:50.468003 train SE 7.714 test SE 7.579\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:00:50.715347 train SE 7.715 test SE 7.582\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:00:49.486629 train SE 7.720 test SE 7.579\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:00:50.715343 train SE 7.718 test SE 7.583\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:00:50.091012 train SE 7.722 test SE 7.582\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:00:49.753914 train SE 7.722 test SE 7.582\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:00:50.304443 train SE 7.725 test SE 7.581\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:00:49.785828 train SE 7.727 test SE 7.580\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:00:50.458032 train SE 7.728 test SE 7.583\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:00:50.611619 train SE 7.730 test SE 7.577\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:00:49.306112 train SE 7.731 test SE 7.586\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:00:50.505903 train SE 7.733 test SE 7.577\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:00:50.603641 train SE 7.734 test SE 7.577\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:00:49.968340 train SE 7.736 test SE 7.581\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:00:50.951710 train SE 7.737 test SE 7.574\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:00:50.086026 train SE 7.739 test SE 7.577\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:00:50.434094 train SE 7.741 test SE 7.575\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:00:50.467007 train SE 7.742 test SE 7.569\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:00:49.377920 train SE 7.744 test SE 7.563\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:00:50.515876 train SE 7.745 test SE 7.571\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:00:50.524852 train SE 7.747 test SE 7.565\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:00:49.653183 train SE 7.748 test SE 7.571\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:00:50.380239 train SE 7.749 test SE 7.566\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:00:49.553451 train SE 7.750 test SE 7.563\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:00:50.861950 train SE 7.751 test SE 7.567\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:00:50.728308 train SE 7.753 test SE 7.560\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:00:49.488623 train SE 7.754 test SE 7.565\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:00:50.498922 train SE 7.755 test SE 7.561\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:00:49.926452 train SE 7.757 test SE 7.558\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:00:49.755910 train SE 7.758 test SE 7.563\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:00:50.647523 train SE 7.758 test SE 7.557\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:00:49.356976 train SE 7.760 test SE 7.553\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:00:50.616607 train SE 7.761 test SE 7.559\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:00:50.372260 train SE 7.763 test SE 7.560\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:00:49.464689 train SE 7.763 test SE 7.554\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:00:50.574717 train SE 7.764 test SE 7.552\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:00:49.549461 train SE 7.765 test SE 7.555\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:00:50.224655 train SE 7.766 test SE 7.553\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:00:50.037156 train SE 7.768 test SE 7.553\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:00:49.516549 train SE 7.768 test SE 7.550\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:00:50.191742 train SE 7.769 test SE 7.558\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:00:50.078048 train SE 7.770 test SE 7.552\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:00:49.479647 train SE 7.772 test SE 7.556\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:00:50.532831 train SE 7.773 test SE 7.551\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:00:50.151849 train SE 7.773 test SE 7.550\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:00:50.054112 train SE 7.775 test SE 7.544\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:00:50.506900 train SE 7.775 test SE 7.544\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:00:49.508570 train SE 7.775 test SE 7.544\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:00:50.725315 train SE 7.777 test SE 7.540\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:00:50.638549 train SE 7.778 test SE 7.540\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:00:49.516549 train SE 7.779 test SE 7.542\n",
      "========================\n",
      "lr:4.2184e-04\n",
      "Epoch: 112 time 0:00:50.888878 train SE 7.779 test SE 7.538\n",
      "========================\n",
      "lr:4.1996e-04\n",
      "Epoch: 113 time 0:00:49.902517 train SE 7.781 test SE 7.537\n",
      "========================\n",
      "lr:4.1812e-04\n",
      "Epoch: 114 time 0:00:49.875588 train SE 7.781 test SE 7.537\n",
      "========================\n",
      "lr:4.1630e-04\n",
      "Epoch: 115 time 0:00:50.377247 train SE 7.783 test SE 7.535\n",
      "========================\n",
      "lr:4.1450e-04\n",
      "Epoch: 116 time 0:00:49.978314 train SE 7.784 test SE 7.535\n",
      "========================\n",
      "lr:4.1272e-04\n",
      "Epoch: 117 time 0:00:50.371263 train SE 7.784 test SE 7.538\n",
      "========================\n",
      "lr:4.1097e-04\n",
      "Epoch: 118 time 0:00:50.605636 train SE 7.785 test SE 7.544\n",
      "========================\n",
      "lr:4.0924e-04\n",
      "Epoch: 119 time 0:00:49.839685 train SE 7.785 test SE 7.533\n",
      "========================\n",
      "lr:4.0753e-04\n",
      "Epoch: 120 time 0:00:50.505903 train SE 7.786 test SE 7.537\n",
      "========================\n",
      "lr:4.0584e-04\n",
      "Epoch: 121 time 0:00:50.268538 train SE 7.788 test SE 7.533\n",
      "========================\n",
      "lr:4.0418e-04\n",
      "Epoch: 122 time 0:00:50.169802 train SE 7.789 test SE 7.541\n",
      "========================\n",
      "lr:4.0253e-04\n",
      "Epoch: 123 time 0:00:50.902841 train SE 7.790 test SE 7.533\n",
      "========================\n",
      "lr:4.0090e-04\n",
      "Epoch: 124 time 0:00:49.594342 train SE 7.790 test SE 7.532\n",
      "========================\n",
      "lr:3.9930e-04\n",
      "Epoch: 125 time 0:00:50.750248 train SE 7.790 test SE 7.530\n",
      "========================\n",
      "lr:3.9771e-04\n",
      "Epoch: 126 time 0:00:50.404175 train SE 7.793 test SE 7.529\n",
      "========================\n",
      "lr:3.9614e-04\n",
      "Epoch: 127 time 0:00:49.673130 train SE 7.793 test SE 7.529\n",
      "========================\n",
      "lr:3.9459e-04\n",
      "Epoch: 128 time 0:00:50.608628 train SE 7.794 test SE 7.529\n",
      "========================\n",
      "lr:3.9306e-04\n",
      "Epoch: 129 time 0:00:49.653184 train SE 7.795 test SE 7.532\n",
      "========================\n",
      "lr:3.9154e-04\n",
      "Epoch: 130 time 0:00:50.728308 train SE 7.796 test SE 7.527\n",
      "========================\n",
      "lr:3.9005e-04\n",
      "Epoch: 131 time 0:00:50.751247 train SE 7.796 test SE 7.527\n",
      "========================\n",
      "lr:3.8857e-04\n",
      "Epoch: 132 time 0:00:49.459701 train SE 7.798 test SE 7.528\n",
      "========================\n",
      "lr:3.8710e-04\n",
      "Epoch: 133 time 0:00:50.960686 train SE 7.798 test SE 7.527\n",
      "========================\n",
      "lr:3.8566e-04\n",
      "Epoch: 134 time 0:00:50.461023 train SE 7.799 test SE 7.523\n",
      "========================\n",
      "lr:3.8422e-04\n",
      "Epoch: 135 time 0:00:49.605312 train SE 7.800 test SE 7.524\n",
      "========================\n",
      "lr:3.8281e-04\n",
      "Epoch: 136 time 0:00:50.474985 train SE 7.802 test SE 7.520\n",
      "========================\n",
      "lr:3.8141e-04\n",
      "Epoch: 137 time 0:00:49.701056 train SE 7.802 test SE 7.524\n",
      "========================\n",
      "lr:3.8003e-04\n",
      "Epoch: 138 time 0:00:50.434095 train SE 7.803 test SE 7.528\n",
      "========================\n",
      "lr:3.7866e-04\n",
      "Epoch: 139 time 0:00:50.541807 train SE 7.804 test SE 7.523\n",
      "========================\n",
      "lr:3.7730e-04\n",
      "Epoch: 140 time 0:00:49.650191 train SE 7.805 test SE 7.519\n",
      "========================\n",
      "lr:3.7596e-04\n",
      "Epoch: 141 time 0:00:50.224656 train SE 7.806 test SE 7.518\n",
      "========================\n",
      "lr:3.7463e-04\n",
      "Epoch: 142 time 0:00:49.793807 train SE 7.806 test SE 7.515\n",
      "========================\n",
      "lr:3.7332e-04\n",
      "Epoch: 143 time 0:00:48.775531 train SE 7.808 test SE 7.513\n",
      "========================\n",
      "lr:3.7202e-04\n",
      "Epoch: 144 time 0:00:50.233631 train SE 7.809 test SE 7.517\n",
      "========================\n",
      "lr:3.7074e-04\n",
      "Epoch: 145 time 0:00:49.507573 train SE 7.810 test SE 7.517\n",
      "========================\n",
      "lr:3.6947e-04\n",
      "Epoch: 146 time 0:00:49.344010 train SE 7.811 test SE 7.512\n",
      "========================\n",
      "lr:3.6821e-04\n",
      "Epoch: 147 time 0:00:49.811760 train SE 7.812 test SE 7.512\n",
      "========================\n",
      "lr:3.6696e-04\n",
      "Epoch: 148 time 0:00:48.864294 train SE 7.813 test SE 7.511\n",
      "========================\n",
      "lr:3.6573e-04\n",
      "Epoch: 149 time 0:00:49.870602 train SE 7.813 test SE 7.508\n",
      "========================\n",
      "lr:3.6451e-04\n",
      "Epoch: 150 time 0:00:49.705045 train SE 7.815 test SE 7.511\n",
      "========================\n",
      "lr:3.6330e-04\n",
      "Epoch: 151 time 0:00:49.192416 train SE 7.816 test SE 7.505\n",
      "========================\n",
      "lr:3.6210e-04\n",
      "Epoch: 152 time 0:00:49.885562 train SE 7.816 test SE 7.512\n",
      "========================\n",
      "lr:3.6092e-04\n",
      "Epoch: 153 time 0:00:49.825722 train SE 7.817 test SE 7.508\n",
      "========================\n",
      "lr:3.5974e-04\n",
      "Epoch: 154 time 0:00:49.207376 train SE 7.819 test SE 7.503\n",
      "========================\n",
      "lr:3.5858e-04\n",
      "Epoch: 155 time 0:00:49.809765 train SE 7.819 test SE 7.507\n",
      "========================\n",
      "lr:3.5743e-04\n",
      "Epoch: 156 time 0:00:49.017882 train SE 7.821 test SE 7.508\n",
      "========================\n",
      "lr:3.5629e-04\n",
      "Epoch: 157 time 0:00:49.927450 train SE 7.821 test SE 7.506\n",
      "========================\n",
      "lr:3.5516e-04\n",
      "Epoch: 158 time 0:00:49.579381 train SE 7.822 test SE 7.500\n",
      "========================\n",
      "lr:3.5404e-04\n",
      "Epoch: 159 time 0:00:48.776528 train SE 7.823 test SE 7.503\n",
      "========================\n",
      "lr:3.5293e-04\n",
      "Epoch: 160 time 0:00:49.771866 train SE 7.824 test SE 7.504\n",
      "========================\n",
      "lr:3.5184e-04\n",
      "Epoch: 161 time 0:00:49.409835 train SE 7.825 test SE 7.503\n",
      "========================\n",
      "lr:3.5075e-04\n",
      "Epoch: 162 time 0:00:48.900198 train SE 7.824 test SE 7.501\n",
      "========================\n",
      "lr:3.4967e-04\n",
      "Epoch: 163 time 0:00:50.451049 train SE 7.827 test SE 7.503\n",
      "========================\n",
      "lr:3.4860e-04\n",
      "Epoch: 164 time 0:00:48.869281 train SE 7.828 test SE 7.502\n",
      "========================\n",
      "lr:3.4754e-04\n",
      "Epoch: 165 time 0:00:49.355978 train SE 7.828 test SE 7.500\n",
      "========================\n",
      "lr:3.4650e-04\n",
      "Epoch: 166 time 0:00:49.992276 train SE 7.830 test SE 7.498\n",
      "========================\n",
      "lr:3.4546e-04\n",
      "Epoch: 167 time 0:00:48.672806 train SE 7.830 test SE 7.500\n",
      "========================\n",
      "lr:3.4443e-04\n",
      "Epoch: 168 time 0:00:50.218671 train SE 7.831 test SE 7.496\n",
      "========================\n",
      "lr:3.4341e-04\n",
      "Epoch: 169 time 0:00:49.564421 train SE 7.832 test SE 7.493\n",
      "========================\n",
      "lr:3.4240e-04\n",
      "Epoch: 170 time 0:00:48.925131 train SE 7.833 test SE 7.495\n",
      "========================\n",
      "lr:3.4139e-04\n",
      "Epoch: 171 time 0:00:49.781840 train SE 7.834 test SE 7.496\n",
      "========================\n",
      "lr:3.4040e-04\n",
      "Epoch: 172 time 0:00:48.867285 train SE 7.835 test SE 7.497\n",
      "========================\n",
      "lr:3.3941e-04\n",
      "Epoch: 173 time 0:00:49.642213 train SE 7.836 test SE 7.494\n",
      "========================\n",
      "lr:3.3844e-04\n",
      "Epoch: 174 time 0:00:50.296463 train SE 7.837 test SE 7.495\n",
      "========================\n",
      "lr:3.3747e-04\n",
      "Epoch: 175 time 0:00:48.872272 train SE 7.837 test SE 7.488\n",
      "========================\n",
      "lr:3.3651e-04\n",
      "Epoch: 176 time 0:00:49.597334 train SE 7.838 test SE 7.493\n",
      "========================\n",
      "lr:3.3556e-04\n",
      "Epoch: 177 time 0:00:48.836368 train SE 7.839 test SE 7.492\n",
      "========================\n",
      "lr:3.3461e-04\n",
      "Epoch: 178 time 0:00:49.092683 train SE 7.840 test SE 7.494\n",
      "========================\n",
      "lr:3.3368e-04\n",
      "Epoch: 179 time 0:00:49.616282 train SE 7.841 test SE 7.490\n",
      "The best SE is: 7.588\n",
      "[5.16513395 6.65096426 7.24293852 7.35310602 7.39074659 7.41359282\n",
      " 7.4297533  7.44787931 7.4564209  7.46164846 7.46536732 7.47861814\n",
      " 7.48024702 7.47710276 7.48530817 7.49221516 7.49662018 7.49329472\n",
      " 7.50514698 7.50195932 7.50409555 7.51257801 7.51843643 7.53847265\n",
      " 7.52911377 7.53810263 7.54768848 7.54970694 7.55277777 7.5466609\n",
      " 7.5517745  7.55307722 7.55389643 7.56136465 7.5598917  7.56226444\n",
      " 7.56422186 7.57102823 7.55903482 7.56373215 7.56321287 7.56222486\n",
      " 7.57452393 7.57093191 7.57350922 7.5724678  7.57851267 7.57391357\n",
      " 7.56992722 7.57394934 7.58334208 7.5757575  7.58536863 7.58028364\n",
      " 7.58354044 7.58161497 7.58430052 7.58645964 7.5876627  7.57684088\n",
      " 7.58220005 7.5791955  7.58207941 7.57879257 7.58300781 7.58172369\n",
      " 7.58210373 7.58113194 7.58034897 7.58269644 7.57662964 7.58608866\n",
      " 7.57693863 7.57725286 7.5810256  7.5744586  7.57739258 7.57453918\n",
      " 7.56925154 7.56255341 7.5708847  7.56495762 7.57119131 7.56570005\n",
      " 7.56300592 7.56700659 7.56039047 7.56493139 7.56145716 7.55843353\n",
      " 7.56320143 7.55719995 7.55281448 7.55890751 7.55991077 7.55402613\n",
      " 7.55234385 7.55483246 7.55331373 7.55251455 7.54993534 7.55805159\n",
      " 7.55170155 7.5564599  7.5514245  7.55011606 7.54378891 7.54413176\n",
      " 7.54443216 7.54045582 7.54020166 7.54162598 7.53765345 7.53650141\n",
      " 7.53707981 7.53504944 7.5350318  7.5378747  7.54440403 7.53293228\n",
      " 7.53676367 7.53326035 7.54100513 7.53250217 7.53205442 7.52983332\n",
      " 7.52920628 7.52909088 7.52903509 7.5317626  7.52652359 7.52705526\n",
      " 7.52817106 7.52663374 7.52346039 7.52379179 7.52022886 7.52378082\n",
      " 7.52779913 7.52309752 7.51904011 7.51786375 7.51459503 7.51336241\n",
      " 7.51687002 7.51711035 7.51218367 7.51241398 7.51069117 7.50777531\n",
      " 7.51079941 7.5049181  7.51161146 7.50753403 7.50314474 7.50702143\n",
      " 7.50835657 7.50582457 7.49985838 7.50345087 7.50387526 7.50251865\n",
      " 7.50079107 7.50314856 7.50211668 7.50015211 7.49770117 7.50017166\n",
      " 7.49572468 7.49311447 7.49463129 7.49602127 7.49722528 7.49379349\n",
      " 7.49455595 7.48838758 7.49316263 7.49228907 7.49380112 7.48994398]\n",
      "(100000, 2, 32, 128)\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (conv_l1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l1): Mish()\n",
      "  (conv_l2): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l2): Mish()\n",
      "  (FC_l1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (bn_l1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (FC_l2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn_l2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC_l3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn_l3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=256, out_features=8192, bias=True)\n",
      "  (bn1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (src_embedding_0): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_0): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_0): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (src_embedding_1): Embedding(\n",
      "    (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_1): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_1): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (src_embedding): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n",
      "Epoch: 0 time 0:00:48.872272 train SE 3.257 test SE 5.060\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:00:48.515227 train SE 5.887 test SE 6.774\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:00:49.408838 train SE 6.971 test SE 7.245\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:00:49.576389 train SE 7.278 test SE 7.353\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:00:48.624934 train SE 7.403 test SE 7.398\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:00:49.603317 train SE 7.479 test SE 7.429\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:00:49.686096 train SE 7.527 test SE 7.440\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:00:48.979984 train SE 7.557 test SE 7.458\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:00:49.601322 train SE 7.571 test SE 7.457\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:00:48.613963 train SE 7.580 test SE 7.481\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:00:49.701056 train SE 7.583 test SE 7.476\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:00:49.541483 train SE 7.582 test SE 7.479\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:00:48.536171 train SE 7.585 test SE 7.491\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:00:49.831707 train SE 7.585 test SE 7.502\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:00:49.060768 train SE 7.583 test SE 7.499\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:00:48.706716 train SE 7.581 test SE 7.501\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:00:49.919471 train SE 7.577 test SE 7.507\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:00:48.502265 train SE 7.578 test SE 7.511\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:00:49.713023 train SE 7.583 test SE 7.514\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:00:49.765883 train SE 7.583 test SE 7.524\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:00:48.290827 train SE 7.587 test SE 7.536\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:00:50.128911 train SE 7.595 test SE 7.529\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:00:49.554448 train SE 7.599 test SE 7.530\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:00:48.955051 train SE 7.605 test SE 7.540\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:00:49.663157 train SE 7.613 test SE 7.539\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:00:48.373606 train SE 7.619 test SE 7.543\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:00:49.737950 train SE 7.627 test SE 7.550\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:00:49.684101 train SE 7.632 test SE 7.548\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:00:48.742619 train SE 7.638 test SE 7.557\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:00:49.772864 train SE 7.641 test SE 7.566\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:00:48.987963 train SE 7.645 test SE 7.558\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:00:48.837366 train SE 7.652 test SE 7.569\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:00:49.791813 train SE 7.658 test SE 7.574\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:00:48.318753 train SE 7.662 test SE 7.562\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:00:49.653185 train SE 7.664 test SE 7.578\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:00:49.503584 train SE 7.667 test SE 7.572\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:00:48.275867 train SE 7.674 test SE 7.575\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:00:49.468677 train SE 7.674 test SE 7.573\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:00:48.858310 train SE 7.677 test SE 7.580\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:00:48.929120 train SE 7.682 test SE 7.579\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:00:49.199398 train SE 7.683 test SE 7.582\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:00:48.971008 train SE 7.688 test SE 7.584\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:00:49.880575 train SE 7.688 test SE 7.579\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:00:49.275195 train SE 7.690 test SE 7.580\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:00:48.844348 train SE 7.694 test SE 7.589\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:00:49.503584 train SE 7.698 test SE 7.587\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:00:48.985968 train SE 7.700 test SE 7.579\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:00:48.880251 train SE 7.703 test SE 7.585\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:00:49.341019 train SE 7.706 test SE 7.588\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:00:48.904188 train SE 7.706 test SE 7.587\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:00:49.038826 train SE 7.712 test SE 7.593\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:00:49.762890 train SE 7.714 test SE 7.588\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:00:48.499270 train SE 7.716 test SE 7.584\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:00:49.580379 train SE 7.719 test SE 7.587\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:00:49.495605 train SE 7.724 test SE 7.584\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:00:48.577062 train SE 7.725 test SE 7.588\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:00:49.288160 train SE 7.728 test SE 7.582\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:00:48.308779 train SE 7.731 test SE 7.585\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:00:49.275196 train SE 7.735 test SE 7.582\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:00:49.382906 train SE 7.735 test SE 7.585\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:00:49.064758 train SE 7.739 test SE 7.582\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:00:49.391882 train SE 7.742 test SE 7.585\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:00:49.595338 train SE 7.745 test SE 7.581\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:00:48.757579 train SE 7.748 test SE 7.583\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:00:48.898204 train SE 7.750 test SE 7.577\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:00:48.444416 train SE 7.752 test SE 7.579\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:00:49.771866 train SE 7.754 test SE 7.571\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:00:49.351994 train SE 7.757 test SE 7.575\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:00:48.702234 train SE 7.760 test SE 7.574\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:00:49.924472 train SE 7.762 test SE 7.571\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:00:49.068747 train SE 7.765 test SE 7.575\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:00:48.775531 train SE 7.767 test SE 7.568\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:00:49.293146 train SE 7.769 test SE 7.570\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:00:48.532691 train SE 7.772 test SE 7.566\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:00:49.283174 train SE 7.774 test SE 7.566\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:00:49.486642 train SE 7.777 test SE 7.558\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:00:48.385574 train SE 7.779 test SE 7.558\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:00:49.454730 train SE 7.781 test SE 7.554\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:00:49.562949 train SE 7.783 test SE 7.554\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:00:48.738630 train SE 7.786 test SE 7.553\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:00:49.209371 train SE 7.787 test SE 7.555\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:00:48.424470 train SE 7.790 test SE 7.548\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:00:49.205910 train SE 7.791 test SE 7.550\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:00:49.564929 train SE 7.794 test SE 7.546\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:00:48.532687 train SE 7.796 test SE 7.547\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:00:48.925131 train SE 7.798 test SE 7.539\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:00:49.532507 train SE 7.800 test SE 7.536\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:00:48.687274 train SE 7.802 test SE 7.537\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:00:49.359968 train SE 7.804 test SE 7.540\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:00:48.553126 train SE 7.806 test SE 7.536\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:00:49.264728 train SE 7.808 test SE 7.534\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:00:49.446244 train SE 7.810 test SE 7.531\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:00:48.468354 train SE 7.812 test SE 7.531\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:00:49.674631 train SE 7.815 test SE 7.529\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:00:49.291661 train SE 7.817 test SE 7.526\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:00:48.415497 train SE 7.817 test SE 7.525\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:00:49.814763 train SE 7.820 test SE 7.529\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:00:48.900703 train SE 7.821 test SE 7.522\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:00:49.881588 train SE 7.824 test SE 7.518\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:00:49.403357 train SE 7.825 test SE 7.520\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:00:48.544150 train SE 7.828 test SE 7.512\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:00:49.876586 train SE 7.829 test SE 7.519\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:00:49.456709 train SE 7.831 test SE 7.515\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:00:48.622449 train SE 7.833 test SE 7.513\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:00:49.943918 train SE 7.834 test SE 7.509\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:00:48.679295 train SE 7.837 test SE 7.512\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:00:49.939418 train SE 7.838 test SE 7.509\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:00:49.415831 train SE 7.839 test SE 7.510\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:00:48.744614 train SE 7.842 test SE 7.507\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:00:49.440776 train SE 7.843 test SE 7.508\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:00:49.674140 train SE 7.845 test SE 7.504\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:00:48.826395 train SE 7.847 test SE 7.504\n",
      "========================\n",
      "lr:4.2184e-04\n",
      "Epoch: 112 time 0:00:50.104975 train SE 7.849 test SE 7.505\n",
      "========================\n",
      "lr:4.1996e-04\n",
      "Epoch: 113 time 0:00:48.644892 train SE 7.850 test SE 7.501\n",
      "========================\n",
      "lr:4.1812e-04\n",
      "Epoch: 114 time 0:00:49.698063 train SE 7.852 test SE 7.495\n",
      "========================\n",
      "lr:4.1630e-04\n",
      "Epoch: 115 time 0:00:49.711534 train SE 7.854 test SE 7.500\n",
      "========================\n",
      "lr:4.1450e-04\n",
      "Epoch: 116 time 0:00:48.386081 train SE 7.855 test SE 7.502\n",
      "========================\n",
      "lr:4.1272e-04\n",
      "Epoch: 117 time 0:00:50.010753 train SE 7.857 test SE 7.495\n",
      "========================\n",
      "lr:4.1097e-04\n",
      "Epoch: 118 time 0:00:49.231821 train SE 7.858 test SE 7.497\n",
      "========================\n",
      "lr:4.0924e-04\n",
      "Epoch: 119 time 0:00:48.588537 train SE 7.860 test SE 7.500\n",
      "========================\n",
      "lr:4.0753e-04\n",
      "Epoch: 120 time 0:00:49.663157 train SE 7.862 test SE 7.499\n",
      "========================\n",
      "lr:4.0584e-04\n",
      "Epoch: 121 time 0:00:48.483324 train SE 7.863 test SE 7.495\n",
      "========================\n",
      "lr:4.0418e-04\n",
      "Epoch: 122 time 0:00:49.973836 train SE 7.864 test SE 7.495\n",
      "========================\n",
      "lr:4.0253e-04\n",
      "Epoch: 123 time 0:00:49.699072 train SE 7.866 test SE 7.487\n",
      "========================\n",
      "lr:4.0090e-04\n",
      "Epoch: 124 time 0:00:48.657846 train SE 7.867 test SE 7.496\n",
      "========================\n",
      "lr:3.9930e-04\n",
      "Epoch: 125 time 0:00:49.872608 train SE 7.869 test SE 7.490\n",
      "========================\n",
      "lr:3.9771e-04\n",
      "Epoch: 126 time 0:00:49.106666 train SE 7.871 test SE 7.492\n",
      "========================\n",
      "lr:3.9614e-04\n",
      "Epoch: 127 time 0:00:49.202389 train SE 7.872 test SE 7.489\n",
      "========================\n",
      "lr:3.9459e-04\n",
      "Epoch: 128 time 0:00:49.559446 train SE 7.873 test SE 7.491\n",
      "========================\n",
      "lr:3.9306e-04\n",
      "Epoch: 129 time 0:00:48.496278 train SE 7.875 test SE 7.492\n",
      "========================\n",
      "lr:3.9154e-04\n",
      "Epoch: 130 time 0:00:49.618277 train SE 7.877 test SE 7.487\n",
      "========================\n",
      "lr:3.9005e-04\n",
      "Epoch: 131 time 0:00:49.491124 train SE 7.878 test SE 7.486\n",
      "========================\n",
      "lr:3.8857e-04\n",
      "Epoch: 132 time 0:00:48.552140 train SE 7.879 test SE 7.487\n",
      "========================\n",
      "lr:3.8710e-04\n",
      "Epoch: 133 time 0:00:49.827224 train SE 7.880 test SE 7.485\n",
      "========================\n",
      "lr:3.8566e-04\n",
      "Epoch: 134 time 0:00:49.057776 train SE 7.882 test SE 7.484\n",
      "========================\n",
      "lr:3.8422e-04\n",
      "Epoch: 135 time 0:00:49.124598 train SE 7.883 test SE 7.482\n",
      "========================\n",
      "lr:3.8281e-04\n",
      "Epoch: 136 time 0:00:49.516562 train SE 7.884 test SE 7.484\n",
      "========================\n",
      "lr:3.8141e-04\n",
      "Epoch: 137 time 0:00:48.509243 train SE 7.885 test SE 7.483\n",
      "========================\n",
      "lr:3.8003e-04\n",
      "Epoch: 138 time 0:00:49.543982 train SE 7.887 test SE 7.483\n",
      "========================\n",
      "lr:3.7866e-04\n",
      "Epoch: 139 time 0:00:49.448730 train SE 7.888 test SE 7.481\n",
      "========================\n",
      "lr:3.7730e-04\n",
      "Epoch: 140 time 0:00:48.297809 train SE 7.889 test SE 7.482\n",
      "========================\n",
      "lr:3.7596e-04\n",
      "Epoch: 141 time 0:00:49.255248 train SE 7.890 test SE 7.481\n",
      "========================\n",
      "lr:3.7463e-04\n",
      "Epoch: 142 time 0:00:48.988961 train SE 7.892 test SE 7.477\n",
      "========================\n",
      "lr:3.7332e-04\n",
      "Epoch: 143 time 0:00:51.132240 train SE 7.893 test SE 7.480\n",
      "========================\n",
      "lr:3.7202e-04\n",
      "Epoch: 144 time 0:00:52.347483 train SE 7.894 test SE 7.478\n",
      "========================\n",
      "lr:3.7074e-04\n",
      "Epoch: 145 time 0:00:48.592035 train SE 7.895 test SE 7.479\n",
      "========================\n",
      "lr:3.6947e-04\n",
      "Epoch: 146 time 0:00:49.298643 train SE 7.894 test SE 7.475\n",
      "========================\n",
      "lr:3.6821e-04\n",
      "Epoch: 147 time 0:00:49.455711 train SE 7.897 test SE 7.478\n",
      "========================\n",
      "lr:3.6696e-04\n",
      "Epoch: 148 time 0:00:48.238979 train SE 7.898 test SE 7.475\n",
      "========================\n",
      "lr:3.6573e-04\n",
      "Epoch: 149 time 0:00:49.513557 train SE 7.900 test SE 7.482\n",
      "========================\n",
      "lr:3.6451e-04\n",
      "Epoch: 150 time 0:00:49.436798 train SE 7.901 test SE 7.478\n",
      "========================\n",
      "lr:3.6330e-04\n",
      "Epoch: 151 time 0:00:48.811435 train SE 7.902 test SE 7.473\n",
      "========================\n",
      "lr:3.6210e-04\n",
      "Epoch: 152 time 0:00:49.521045 train SE 7.901 test SE 7.473\n",
      "========================\n",
      "lr:3.6092e-04\n",
      "Epoch: 153 time 0:00:48.275374 train SE 7.903 test SE 7.478\n",
      "========================\n",
      "lr:3.5974e-04\n",
      "Epoch: 154 time 0:00:49.123108 train SE 7.904 test SE 7.473\n",
      "========================\n",
      "lr:3.5858e-04\n",
      "Epoch: 155 time 0:00:49.135076 train SE 7.905 test SE 7.474\n",
      "========================\n",
      "lr:3.5743e-04\n",
      "Epoch: 156 time 0:00:48.588537 train SE 7.907 test SE 7.470\n",
      "========================\n",
      "lr:3.5629e-04\n",
      "Epoch: 157 time 0:00:49.312098 train SE 7.907 test SE 7.472\n",
      "========================\n",
      "lr:3.5516e-04\n",
      "Epoch: 158 time 0:00:49.795818 train SE 7.907 test SE 7.476\n",
      "========================\n",
      "lr:3.5404e-04\n",
      "Epoch: 159 time 0:00:48.594522 train SE 7.909 test SE 7.475\n",
      "========================\n",
      "lr:3.5293e-04\n",
      "Epoch: 160 time 0:00:49.142562 train SE 7.909 test SE 7.471\n",
      "========================\n",
      "lr:3.5184e-04\n",
      "Epoch: 161 time 0:00:48.872273 train SE 7.910 test SE 7.473\n",
      "========================\n",
      "lr:3.5075e-04\n",
      "Epoch: 162 time 0:00:49.102179 train SE 7.911 test SE 7.474\n",
      "========================\n",
      "lr:3.4967e-04\n",
      "Epoch: 163 time 0:00:49.776361 train SE 7.912 test SE 7.470\n",
      "========================\n",
      "lr:3.4860e-04\n",
      "Epoch: 164 time 0:00:48.200084 train SE 7.913 test SE 7.469\n",
      "========================\n",
      "lr:3.4754e-04\n",
      "Epoch: 165 time 0:00:49.220380 train SE 7.911 test SE 7.475\n",
      "========================\n",
      "lr:3.4650e-04\n",
      "Epoch: 166 time 0:00:49.872597 train SE 7.913 test SE 7.472\n",
      "========================\n",
      "lr:3.4546e-04\n",
      "Epoch: 167 time 0:00:48.588551 train SE 7.915 test SE 7.471\n",
      "========================\n",
      "lr:3.4443e-04\n",
      "Epoch: 168 time 0:00:49.658683 train SE 7.916 test SE 7.472\n",
      "========================\n",
      "lr:3.4341e-04\n",
      "Epoch: 169 time 0:00:49.539991 train SE 7.916 test SE 7.470\n",
      "========================\n",
      "lr:3.4240e-04\n",
      "Epoch: 170 time 0:00:49.103654 train SE 7.917 test SE 7.469\n",
      "========================\n",
      "lr:3.4139e-04\n",
      "Epoch: 171 time 0:00:49.900575 train SE 7.916 test SE 7.474\n",
      "========================\n",
      "lr:3.4040e-04\n",
      "Epoch: 172 time 0:00:48.482316 train SE 7.917 test SE 7.467\n",
      "========================\n",
      "lr:3.3941e-04\n",
      "Epoch: 173 time 0:00:50.044660 train SE 7.918 test SE 7.470\n",
      "========================\n",
      "lr:3.3844e-04\n",
      "Epoch: 174 time 0:00:49.597333 train SE 7.918 test SE 7.469\n",
      "========================\n",
      "lr:3.3747e-04\n",
      "Epoch: 175 time 0:00:48.111327 train SE 7.919 test SE 7.462\n",
      "========================\n",
      "lr:3.3651e-04\n",
      "Epoch: 176 time 0:00:49.480659 train SE 7.920 test SE 7.464\n",
      "========================\n",
      "lr:3.3556e-04\n",
      "Epoch: 177 time 0:00:49.548463 train SE 7.921 test SE 7.471\n",
      "========================\n",
      "lr:3.3461e-04\n",
      "Epoch: 178 time 0:00:49.150543 train SE 7.921 test SE 7.464\n",
      "========================\n",
      "lr:3.3368e-04\n",
      "Epoch: 179 time 0:00:49.708037 train SE 7.923 test SE 7.466\n",
      "The best SE is: 7.593\n",
      "[5.06003094 6.77422333 7.24524832 7.35272551 7.39757919 7.4289031\n",
      " 7.44047308 7.45750046 7.45704412 7.48071623 7.4764657  7.4785347\n",
      " 7.49095774 7.50183725 7.49873734 7.50080729 7.50654459 7.51127863\n",
      " 7.51362991 7.52436781 7.53620243 7.52851057 7.53020954 7.53969336\n",
      " 7.5389123  7.54328394 7.54959726 7.54805136 7.55734015 7.56557608\n",
      " 7.55844593 7.56905985 7.57378244 7.56247663 7.57807302 7.57177877\n",
      " 7.57495451 7.57252121 7.57951593 7.57883024 7.58205557 7.58407688\n",
      " 7.5792222  7.58041382 7.58917189 7.5865531  7.57922697 7.58511496\n",
      " 7.58770895 7.5871973  7.59292459 7.58816242 7.58444071 7.58745432\n",
      " 7.58365583 7.58820057 7.58153486 7.58467865 7.58224106 7.5846138\n",
      " 7.58160353 7.58509064 7.58110046 7.58259058 7.57721806 7.57934332\n",
      " 7.57103682 7.57495451 7.57374954 7.57105589 7.57473993 7.56771088\n",
      " 7.57034254 7.56554031 7.56628656 7.55776691 7.55812073 7.55442047\n",
      " 7.55421543 7.55326986 7.55504847 7.54781437 7.55014038 7.54562235\n",
      " 7.54669142 7.53918839 7.5357399  7.5369029  7.54007339 7.53579426\n",
      " 7.53404236 7.53099775 7.53136063 7.52929306 7.52580643 7.52521086\n",
      " 7.5287509  7.52185202 7.51756001 7.51950836 7.51241302 7.51930571\n",
      " 7.51514673 7.51258564 7.50887918 7.51236105 7.50888443 7.50993824\n",
      " 7.50731659 7.50808716 7.50428629 7.50410223 7.50456476 7.50062037\n",
      " 7.49515295 7.49977112 7.50176382 7.49453735 7.49690866 7.50011301\n",
      " 7.49855757 7.49531555 7.49489927 7.48741102 7.49565887 7.49031925\n",
      " 7.49213648 7.48857355 7.49133158 7.49228811 7.48717117 7.48614359\n",
      " 7.48701859 7.48457479 7.48424244 7.48231745 7.48389053 7.48338413\n",
      " 7.48294592 7.48107386 7.4819231  7.48102283 7.47659445 7.48039484\n",
      " 7.47799158 7.47926569 7.47501898 7.47832108 7.47497511 7.48155975\n",
      " 7.47842169 7.47312546 7.47307587 7.47771788 7.47339344 7.47364283\n",
      " 7.47009802 7.47222471 7.4763751  7.47478247 7.47138739 7.47307825\n",
      " 7.47380018 7.46985626 7.46928406 7.47541189 7.4717865  7.47140598\n",
      " 7.47210312 7.47044992 7.46860647 7.47433615 7.46650648 7.46968031\n",
      " 7.46870422 7.46168852 7.46430349 7.47055197 7.46444273 7.46576405]\n",
      "(100000, 2, 32, 128)\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (conv_l1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l1): Mish()\n",
      "  (conv_l2): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l2): Mish()\n",
      "  (FC_l1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (bn_l1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (FC_l2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn_l2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC_l3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn_l3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=512, out_features=8192, bias=True)\n",
      "  (bn1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (src_embedding_0): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_0): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_0): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (src_embedding_1): Embedding(\n",
      "    (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_1): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_1): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (src_embedding): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n",
      "Epoch: 0 time 0:00:48.977496 train SE 3.344 test SE 5.327\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:00:48.778538 train SE 6.103 test SE 6.901\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:00:48.977517 train SE 7.070 test SE 7.284\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:00:49.269265 train SE 7.329 test SE 7.365\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:00:48.747606 train SE 7.437 test SE 7.401\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:00:49.719525 train SE 7.505 test SE 7.418\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:00:48.938098 train SE 7.550 test SE 7.434\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:00:49.062774 train SE 7.575 test SE 7.448\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:00:49.949427 train SE 7.592 test SE 7.453\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:00:48.919657 train SE 7.595 test SE 7.449\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:00:49.645205 train SE 7.597 test SE 7.461\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:00:49.442264 train SE 7.596 test SE 7.477\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:00:48.630930 train SE 7.596 test SE 7.478\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:00:50.159375 train SE 7.593 test SE 7.485\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:00:48.868823 train SE 7.587 test SE 7.497\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:00:49.844204 train SE 7.588 test SE 7.476\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:00:48.260908 train SE 7.590 test SE 7.472\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:00:49.249301 train SE 7.591 test SE 7.490\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:00:48.752594 train SE 7.587 test SE 7.495\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:00:48.437995 train SE 7.586 test SE 7.495\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:00:48.913186 train SE 7.587 test SE 7.507\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:00:49.074756 train SE 7.599 test SE 7.509\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:00:48.870329 train SE 7.603 test SE 7.519\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:00:49.042857 train SE 7.613 test SE 7.528\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:00:49.005952 train SE 7.619 test SE 7.525\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:00:49.465217 train SE 7.625 test SE 7.527\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:00:48.997936 train SE 7.635 test SE 7.538\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:00:49.044828 train SE 7.638 test SE 7.532\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:00:49.571402 train SE 7.637 test SE 7.542\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:00:49.196406 train SE 7.646 test SE 7.541\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:00:49.096673 train SE 7.650 test SE 7.543\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:00:49.518543 train SE 7.655 test SE 7.549\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:00:48.537169 train SE 7.660 test SE 7.551\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:00:49.441751 train SE 7.663 test SE 7.555\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:00:49.325060 train SE 7.669 test SE 7.549\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:00:48.641889 train SE 7.673 test SE 7.557\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:00:49.781839 train SE 7.677 test SE 7.552\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:00:48.854320 train SE 7.681 test SE 7.555\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:00:48.935104 train SE 7.687 test SE 7.561\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:00:49.653184 train SE 7.689 test SE 7.560\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:00:48.296812 train SE 7.690 test SE 7.559\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:00:49.686096 train SE 7.695 test SE 7.570\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:00:49.291152 train SE 7.698 test SE 7.566\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:00:48.751595 train SE 7.703 test SE 7.575\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:00:49.688090 train SE 7.709 test SE 7.571\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:00:48.832379 train SE 7.710 test SE 7.565\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:00:48.840359 train SE 7.712 test SE 7.565\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:00:48.998934 train SE 7.716 test SE 7.563\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:00:48.507249 train SE 7.720 test SE 7.573\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:00:49.240288 train SE 7.721 test SE 7.565\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:00:49.362961 train SE 7.726 test SE 7.572\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:00:48.758575 train SE 7.730 test SE 7.562\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:00:49.786827 train SE 7.734 test SE 7.561\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:00:48.613963 train SE 7.736 test SE 7.565\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:00:49.224331 train SE 7.741 test SE 7.563\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:00:49.504582 train SE 7.743 test SE 7.563\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:00:48.728656 train SE 7.746 test SE 7.564\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:00:49.570406 train SE 7.749 test SE 7.555\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:00:49.105649 train SE 7.753 test SE 7.557\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:00:48.885236 train SE 7.756 test SE 7.556\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:00:49.602320 train SE 7.760 test SE 7.550\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:00:48.779521 train SE 7.763 test SE 7.549\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:00:49.811760 train SE 7.765 test SE 7.546\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:00:49.499594 train SE 7.768 test SE 7.543\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:00:48.649868 train SE 7.771 test SE 7.540\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:00:49.579381 train SE 7.774 test SE 7.536\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:00:49.777851 train SE 7.776 test SE 7.541\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:00:51.237937 train SE 7.780 test SE 7.537\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:00:49.535498 train SE 7.783 test SE 7.533\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:00:48.520215 train SE 7.786 test SE 7.534\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:00:50.053113 train SE 7.788 test SE 7.529\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:00:49.551456 train SE 7.790 test SE 7.530\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:00:48.822406 train SE 7.793 test SE 7.521\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:00:49.566415 train SE 7.795 test SE 7.524\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:00:49.240288 train SE 7.798 test SE 7.517\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:00:49.140555 train SE 7.800 test SE 7.520\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:00:49.755909 train SE 7.803 test SE 7.530\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:00:48.595015 train SE 7.806 test SE 7.519\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:00:49.504581 train SE 7.807 test SE 7.520\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:00:49.906507 train SE 7.810 test SE 7.516\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:00:48.890225 train SE 7.813 test SE 7.514\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:00:49.852650 train SE 7.816 test SE 7.514\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:00:48.884241 train SE 7.818 test SE 7.511\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:00:49.217350 train SE 7.821 test SE 7.508\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:00:49.585365 train SE 7.823 test SE 7.510\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:00:48.686768 train SE 7.826 test SE 7.501\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:00:49.667146 train SE 7.828 test SE 7.502\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:00:48.968525 train SE 7.831 test SE 7.496\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:00:48.942086 train SE 7.834 test SE 7.496\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:00:49.630245 train SE 7.836 test SE 7.496\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:00:48.882751 train SE 7.839 test SE 7.496\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:00:49.201410 train SE 7.842 test SE 7.488\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:00:49.525525 train SE 7.844 test SE 7.493\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:00:48.749129 train SE 7.847 test SE 7.493\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:00:49.826227 train SE 7.850 test SE 7.492\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:00:49.300658 train SE 7.852 test SE 7.488\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:00:48.640895 train SE 7.854 test SE 7.486\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:00:49.685098 train SE 7.857 test SE 7.484\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:00:48.841368 train SE 7.860 test SE 7.480\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:00:49.131099 train SE 7.862 test SE 7.484\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:00:49.541987 train SE 7.865 test SE 7.487\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:00:49.120127 train SE 7.868 test SE 7.480\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:00:49.907504 train SE 7.870 test SE 7.481\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:00:49.355489 train SE 7.873 test SE 7.482\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:00:48.574575 train SE 7.874 test SE 7.479\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:00:49.597849 train SE 7.877 test SE 7.475\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:00:48.777050 train SE 7.876 test SE 7.477\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:00:49.872653 train SE 7.882 test SE 7.475\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:00:49.343519 train SE 7.883 test SE 7.476\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:00:48.676822 train SE 7.885 test SE 7.470\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:00:49.553462 train SE 7.887 test SE 7.472\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:00:49.567413 train SE 7.889 test SE 7.469\n",
      "========================\n",
      "lr:4.2184e-04\n",
      "Epoch: 112 time 0:00:48.930119 train SE 7.891 test SE 7.469\n",
      "========================\n",
      "lr:4.1996e-04\n",
      "Epoch: 113 time 0:00:50.008250 train SE 7.894 test SE 7.466\n",
      "========================\n",
      "lr:4.1812e-04\n",
      "Epoch: 114 time 0:00:48.496295 train SE 7.896 test SE 7.467\n",
      "========================\n",
      "lr:4.1630e-04\n",
      "Epoch: 115 time 0:00:49.773379 train SE 7.898 test SE 7.463\n",
      "========================\n",
      "lr:4.1450e-04\n",
      "Epoch: 116 time 0:00:49.757412 train SE 7.900 test SE 7.464\n",
      "========================\n",
      "lr:4.1272e-04\n",
      "Epoch: 117 time 0:00:49.012896 train SE 7.902 test SE 7.465\n",
      "========================\n",
      "lr:4.1097e-04\n",
      "Epoch: 118 time 0:00:49.979312 train SE 7.904 test SE 7.464\n",
      "========================\n",
      "lr:4.0924e-04\n",
      "Epoch: 119 time 0:00:48.874290 train SE 7.906 test SE 7.466\n",
      "========================\n",
      "lr:4.0753e-04\n",
      "Epoch: 120 time 0:00:48.924186 train SE 7.907 test SE 7.461\n",
      "========================\n",
      "lr:4.0584e-04\n",
      "Epoch: 121 time 0:00:49.586381 train SE 7.909 test SE 7.468\n",
      "========================\n",
      "lr:4.0418e-04\n",
      "Epoch: 122 time 0:00:48.800478 train SE 7.911 test SE 7.464\n",
      "========================\n",
      "lr:4.0253e-04\n",
      "Epoch: 123 time 0:00:49.754928 train SE 7.913 test SE 7.462\n",
      "========================\n",
      "lr:4.0090e-04\n",
      "Epoch: 124 time 0:00:49.542984 train SE 7.914 test SE 7.465\n",
      "========================\n",
      "lr:3.9930e-04\n",
      "Epoch: 125 time 0:00:48.758116 train SE 7.916 test SE 7.462\n",
      "========================\n",
      "lr:3.9771e-04\n",
      "Epoch: 126 time 0:00:49.713533 train SE 7.917 test SE 7.463\n",
      "========================\n",
      "lr:3.9614e-04\n",
      "Epoch: 127 time 0:00:48.933615 train SE 7.918 test SE 7.454\n",
      "========================\n",
      "lr:3.9459e-04\n",
      "Epoch: 128 time 0:00:49.228335 train SE 7.917 test SE 7.455\n",
      "========================\n",
      "lr:3.9306e-04\n",
      "Epoch: 129 time 0:00:49.646707 train SE 7.918 test SE 7.462\n",
      "========================\n",
      "lr:3.9154e-04\n",
      "Epoch: 130 time 0:00:49.020875 train SE 7.922 test SE 7.454\n",
      "========================\n",
      "lr:3.9005e-04\n",
      "Epoch: 131 time 0:00:50.000760 train SE 7.922 test SE 7.453\n",
      "========================\n",
      "lr:3.8857e-04\n",
      "Epoch: 132 time 0:00:49.669646 train SE 7.925 test SE 7.452\n",
      "========================\n",
      "lr:3.8710e-04\n",
      "Epoch: 133 time 0:00:48.640891 train SE 7.925 test SE 7.450\n",
      "========================\n",
      "lr:3.8566e-04\n",
      "Epoch: 134 time 0:00:49.951386 train SE 7.927 test SE 7.453\n",
      "========================\n",
      "lr:3.8422e-04\n",
      "Epoch: 135 time 0:00:48.707713 train SE 7.928 test SE 7.454\n",
      "========================\n",
      "lr:3.8281e-04\n",
      "Epoch: 136 time 0:00:49.839685 train SE 7.929 test SE 7.453\n",
      "========================\n",
      "lr:3.8141e-04\n",
      "Epoch: 137 time 0:00:49.288161 train SE 7.931 test SE 7.451\n",
      "========================\n",
      "lr:3.8003e-04\n",
      "Epoch: 138 time 0:00:48.841356 train SE 7.931 test SE 7.453\n",
      "========================\n",
      "lr:3.7866e-04\n",
      "Epoch: 139 time 0:00:49.663157 train SE 7.933 test SE 7.452\n",
      "========================\n",
      "lr:3.7730e-04\n",
      "Epoch: 140 time 0:00:49.277188 train SE 7.934 test SE 7.453\n",
      "========================\n",
      "lr:3.7596e-04\n",
      "Epoch: 141 time 0:00:48.815424 train SE 7.933 test SE 7.454\n",
      "========================\n",
      "lr:3.7463e-04\n",
      "Epoch: 142 time 0:00:49.427787 train SE 7.936 test SE 7.449\n",
      "========================\n",
      "lr:3.7332e-04\n",
      "Epoch: 143 time 0:00:48.575067 train SE 7.937 test SE 7.452\n",
      "========================\n",
      "lr:3.7202e-04\n",
      "Epoch: 144 time 0:00:49.767877 train SE 7.938 test SE 7.455\n",
      "========================\n",
      "lr:3.7074e-04\n",
      "Epoch: 145 time 0:00:49.367947 train SE 7.938 test SE 7.450\n",
      "========================\n",
      "lr:3.6947e-04\n",
      "Epoch: 146 time 0:00:48.793482 train SE 7.940 test SE 7.452\n",
      "========================\n",
      "lr:3.6821e-04\n",
      "Epoch: 147 time 0:00:49.461696 train SE 7.940 test SE 7.451\n",
      "========================\n",
      "lr:3.6696e-04\n",
      "Epoch: 148 time 0:00:49.476656 train SE 7.942 test SE 7.452\n",
      "========================\n",
      "lr:3.6573e-04\n",
      "Epoch: 149 time 0:00:49.309104 train SE 7.942 test SE 7.452\n",
      "========================\n",
      "lr:3.6451e-04\n",
      "Epoch: 150 time 0:00:49.194410 train SE 7.943 test SE 7.457\n",
      "========================\n",
      "lr:3.6330e-04\n",
      "Epoch: 151 time 0:00:48.279857 train SE 7.944 test SE 7.455\n",
      "========================\n",
      "lr:3.6210e-04\n",
      "Epoch: 152 time 0:00:49.212363 train SE 7.945 test SE 7.454\n",
      "========================\n",
      "lr:3.6092e-04\n",
      "Epoch: 153 time 0:00:49.541493 train SE 7.946 test SE 7.449\n",
      "========================\n",
      "lr:3.5974e-04\n",
      "Epoch: 154 time 0:00:49.006914 train SE 7.947 test SE 7.445\n",
      "========================\n",
      "lr:3.5858e-04\n",
      "Epoch: 155 time 0:00:49.583878 train SE 7.947 test SE 7.449\n",
      "========================\n",
      "lr:3.5743e-04\n",
      "Epoch: 156 time 0:00:48.825416 train SE 7.948 test SE 7.446\n",
      "========================\n",
      "lr:3.5629e-04\n",
      "Epoch: 157 time 0:00:49.349506 train SE 7.949 test SE 7.452\n",
      "========================\n",
      "lr:3.5516e-04\n",
      "Epoch: 158 time 0:00:49.650705 train SE 7.950 test SE 7.448\n",
      "========================\n",
      "lr:3.5404e-04\n",
      "Epoch: 159 time 0:00:48.496785 train SE 7.949 test SE 7.449\n",
      "========================\n",
      "lr:3.5293e-04\n",
      "Epoch: 160 time 0:00:49.687109 train SE 7.951 test SE 7.447\n",
      "========================\n",
      "lr:3.5184e-04\n",
      "Epoch: 161 time 0:00:49.318093 train SE 7.952 test SE 7.451\n",
      "========================\n",
      "lr:3.5075e-04\n",
      "Epoch: 162 time 0:00:48.540161 train SE 7.953 test SE 7.445\n",
      "========================\n",
      "lr:3.4967e-04\n",
      "Epoch: 163 time 0:00:50.124922 train SE 7.953 test SE 7.447\n",
      "========================\n",
      "lr:3.4860e-04\n",
      "Epoch: 164 time 0:00:49.417323 train SE 7.955 test SE 7.441\n",
      "========================\n",
      "lr:3.4754e-04\n",
      "Epoch: 165 time 0:00:49.398864 train SE 7.955 test SE 7.449\n",
      "========================\n",
      "lr:3.4650e-04\n",
      "Epoch: 166 time 0:00:49.751920 train SE 7.956 test SE 7.448\n",
      "========================\n",
      "lr:3.4546e-04\n",
      "Epoch: 167 time 0:00:49.170475 train SE 7.956 test SE 7.445\n",
      "========================\n",
      "lr:3.4443e-04\n",
      "Epoch: 168 time 0:00:49.941413 train SE 7.956 test SE 7.443\n",
      "========================\n",
      "lr:3.4341e-04\n",
      "Epoch: 169 time 0:00:49.649208 train SE 7.957 test SE 7.446\n",
      "========================\n",
      "lr:3.4240e-04\n",
      "Epoch: 170 time 0:00:49.160513 train SE 7.958 test SE 7.447\n",
      "========================\n",
      "lr:3.4139e-04\n",
      "Epoch: 171 time 0:00:49.717520 train SE 7.959 test SE 7.448\n",
      "========================\n",
      "lr:3.4040e-04\n",
      "Epoch: 172 time 0:00:49.617783 train SE 7.959 test SE 7.450\n",
      "========================\n",
      "lr:3.3941e-04\n",
      "Epoch: 173 time 0:00:49.146539 train SE 7.961 test SE 7.446\n",
      "========================\n",
      "lr:3.3844e-04\n",
      "Epoch: 174 time 0:00:50.222699 train SE 7.961 test SE 7.450\n",
      "========================\n",
      "lr:3.3747e-04\n",
      "Epoch: 175 time 0:00:49.086210 train SE 7.961 test SE 7.450\n",
      "========================\n",
      "lr:3.3651e-04\n",
      "Epoch: 176 time 0:00:49.790827 train SE 7.962 test SE 7.448\n",
      "========================\n",
      "lr:3.3556e-04\n",
      "Epoch: 177 time 0:00:49.419315 train SE 7.963 test SE 7.450\n",
      "========================\n",
      "lr:3.3461e-04\n",
      "Epoch: 178 time 0:00:49.557945 train SE 7.964 test SE 7.449\n",
      "========================\n",
      "lr:3.3368e-04\n",
      "Epoch: 179 time 0:00:50.145879 train SE 7.964 test SE 7.451\n",
      "The best SE is: 7.575\n",
      "[5.32675362 6.90091276 7.28406286 7.36477041 7.40085173 7.41799116\n",
      " 7.43383503 7.44780684 7.45303822 7.44926596 7.46146917 7.47653675\n",
      " 7.47805357 7.4848361  7.49681616 7.47554255 7.47224045 7.48998022\n",
      " 7.49535751 7.49462843 7.50709152 7.50937748 7.51902533 7.52804422\n",
      " 7.52454901 7.52653122 7.53842115 7.53167057 7.54249668 7.54108381\n",
      " 7.54280043 7.54914045 7.55131912 7.55517578 7.54931974 7.55719852\n",
      " 7.55242491 7.55481291 7.56091785 7.55952835 7.55917692 7.57029819\n",
      " 7.56569624 7.57468367 7.57078505 7.56496525 7.56454945 7.56259251\n",
      " 7.57255936 7.56528425 7.57190084 7.56228781 7.56133747 7.56469297\n",
      " 7.56346369 7.56330109 7.56374502 7.55461121 7.55707645 7.5563364\n",
      " 7.54988861 7.54909658 7.54648972 7.54331064 7.54005766 7.53626013\n",
      " 7.54058695 7.53724289 7.53321314 7.5335288  7.52906275 7.52953959\n",
      " 7.52082443 7.52410221 7.51719666 7.52043533 7.52987623 7.51859283\n",
      " 7.51985884 7.51555967 7.51408386 7.51397562 7.51052046 7.50802088\n",
      " 7.51038218 7.50116062 7.50197458 7.49615431 7.49582148 7.49574232\n",
      " 7.49583387 7.48813725 7.49287558 7.49280167 7.49159575 7.48794031\n",
      " 7.4860177  7.48357534 7.48024607 7.48378897 7.48715305 7.47989416\n",
      " 7.48118305 7.48246717 7.47869349 7.47536039 7.47694016 7.47486115\n",
      " 7.47606659 7.46953678 7.47150517 7.46892262 7.46930647 7.46567297\n",
      " 7.46673441 7.46267796 7.46385956 7.46490622 7.46351957 7.46595716\n",
      " 7.46088171 7.46781921 7.46429825 7.46158218 7.46499586 7.46219254\n",
      " 7.46266508 7.4543581  7.45541239 7.46159124 7.45443487 7.45286274\n",
      " 7.45229721 7.45003223 7.45337009 7.45367765 7.45316458 7.4513278\n",
      " 7.45306873 7.4523263  7.4534936  7.45420599 7.44921446 7.45204401\n",
      " 7.45479584 7.45026493 7.45157766 7.45148563 7.45171118 7.45202255\n",
      " 7.45678473 7.45467901 7.45392942 7.44940186 7.44482899 7.44919538\n",
      " 7.44635248 7.45156717 7.44809961 7.44933319 7.44748068 7.45063782\n",
      " 7.44524145 7.44693613 7.4406662  7.44917679 7.44757795 7.44546652\n",
      " 7.44261646 7.44621658 7.44671488 7.4478302  7.45014143 7.4458375\n",
      " 7.45047855 7.44977808 7.44809675 7.45022678 7.44928503 7.45135355]\n",
      "(100000, 2, 32, 128)\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (conv_l1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l1): Mish()\n",
      "  (conv_l2): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l2): Mish()\n",
      "  (FC_l1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (bn_l1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (FC_l2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn_l2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC_l3): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn_l3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=1024, out_features=8192, bias=True)\n",
      "  (bn1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (src_embedding_0): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_0): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_0): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (src_embedding_1): Embedding(\n",
      "    (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_1): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_1): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (src_embedding): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n",
      "Epoch: 0 time 0:00:49.430779 train SE 3.170 test SE 4.871\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:00:49.914525 train SE 5.993 test SE 7.007\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:00:49.271752 train SE 7.119 test SE 7.334\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:00:50.764237 train SE 7.359 test SE 7.397\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:00:49.767425 train SE 7.463 test SE 7.431\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:00:49.668155 train SE 7.527 test SE 7.448\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:00:50.467043 train SE 7.568 test SE 7.468\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:00:50.102492 train SE 7.591 test SE 7.479\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:00:50.516891 train SE 7.598 test SE 7.483\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:00:49.966896 train SE 7.603 test SE 7.481\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:00:49.425300 train SE 7.602 test SE 7.494\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:00:50.524864 train SE 7.599 test SE 7.496\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:00:49.690659 train SE 7.599 test SE 7.508\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:00:50.762250 train SE 7.598 test SE 7.513\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:00:50.555310 train SE 7.599 test SE 7.520\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:00:49.772863 train SE 7.600 test SE 7.512\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:00:50.209695 train SE 7.600 test SE 7.511\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:00:49.658170 train SE 7.603 test SE 7.516\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:00:50.210693 train SE 7.605 test SE 7.516\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:00:50.474986 train SE 7.604 test SE 7.525\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:00:49.411829 train SE 7.607 test SE 7.528\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:00:50.739278 train SE 7.615 test SE 7.547\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:00:50.302447 train SE 7.625 test SE 7.550\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:00:49.948394 train SE 7.632 test SE 7.553\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:00:50.285492 train SE 7.635 test SE 7.551\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:00:49.362960 train SE 7.645 test SE 7.553\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:00:50.741273 train SE 7.649 test SE 7.564\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:00:50.371263 train SE 7.658 test SE 7.572\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:00:49.977318 train SE 7.659 test SE 7.561\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:00:49.874591 train SE 7.667 test SE 7.572\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:00:50.445066 train SE 7.672 test SE 7.571\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:00:49.667146 train SE 7.676 test SE 7.585\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:00:50.931764 train SE 7.681 test SE 7.578\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:00:49.265222 train SE 7.684 test SE 7.580\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:00:50.197727 train SE 7.691 test SE 7.579\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:00:50.129908 train SE 7.694 test SE 7.587\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:00:50.174788 train SE 7.699 test SE 7.587\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:00:50.142874 train SE 7.699 test SE 7.577\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:00:49.659168 train SE 7.705 test SE 7.590\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:00:49.756906 train SE 7.711 test SE 7.592\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:00:50.897855 train SE 7.713 test SE 7.587\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:00:48.902192 train SE 7.717 test SE 7.593\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:00:50.892867 train SE 7.720 test SE 7.595\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:00:50.566740 train SE 7.723 test SE 7.589\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:00:49.068746 train SE 7.728 test SE 7.590\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:00:50.284495 train SE 7.730 test SE 7.596\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:00:49.938421 train SE 7.734 test SE 7.600\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:00:49.383903 train SE 7.737 test SE 7.599\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:00:49.730976 train SE 7.741 test SE 7.589\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:00:48.777526 train SE 7.744 test SE 7.593\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:00:50.013220 train SE 7.749 test SE 7.584\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:00:49.988287 train SE 7.752 test SE 7.583\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:00:49.188427 train SE 7.755 test SE 7.589\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:00:49.976319 train SE 7.758 test SE 7.583\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:00:49.019878 train SE 7.761 test SE 7.585\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:00:48.963030 train SE 7.765 test SE 7.583\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:00:49.647200 train SE 7.768 test SE 7.585\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:00:48.700731 train SE 7.770 test SE 7.573\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:00:50.065083 train SE 7.774 test SE 7.582\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:00:49.175461 train SE 7.776 test SE 7.575\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:00:48.973003 train SE 7.779 test SE 7.571\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:00:49.563424 train SE 7.782 test SE 7.567\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:00:49.754911 train SE 7.785 test SE 7.567\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:00:49.435765 train SE 7.787 test SE 7.567\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:00:50.014219 train SE 7.789 test SE 7.563\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:00:49.123599 train SE 7.792 test SE 7.563\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:00:50.251583 train SE 7.795 test SE 7.558\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:00:49.494608 train SE 7.796 test SE 7.557\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:00:48.970017 train SE 7.800 test SE 7.557\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:00:49.979311 train SE 7.802 test SE 7.561\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:00:49.780843 train SE 7.804 test SE 7.556\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:00:49.628250 train SE 7.807 test SE 7.548\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:00:50.084031 train SE 7.809 test SE 7.549\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:00:49.109637 train SE 7.811 test SE 7.555\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:00:49.950388 train SE 7.814 test SE 7.549\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:00:49.518545 train SE 7.817 test SE 7.544\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:00:48.993946 train SE 7.819 test SE 7.544\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:00:50.060096 train SE 7.822 test SE 7.540\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:00:49.568410 train SE 7.824 test SE 7.541\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:00:48.974000 train SE 7.827 test SE 7.539\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:00:50.167807 train SE 7.830 test SE 7.542\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:00:49.122603 train SE 7.832 test SE 7.532\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:00:50.167807 train SE 7.835 test SE 7.535\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:00:49.706042 train SE 7.838 test SE 7.530\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:00:49.067750 train SE 7.841 test SE 7.528\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:00:50.019205 train SE 7.843 test SE 7.529\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:00:50.006239 train SE 7.846 test SE 7.525\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:00:49.258240 train SE 7.848 test SE 7.521\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:00:50.195732 train SE 7.852 test SE 7.520\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:00:48.927126 train SE 7.854 test SE 7.520\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:00:49.990283 train SE 7.857 test SE 7.517\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:00:49.988792 train SE 7.860 test SE 7.515\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:00:49.380422 train SE 7.863 test SE 7.513\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:00:50.422144 train SE 7.865 test SE 7.511\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:00:49.761906 train SE 7.868 test SE 7.508\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:00:49.329051 train SE 7.871 test SE 7.509\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:00:49.837690 train SE 7.874 test SE 7.510\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:00:48.804466 train SE 7.877 test SE 7.509\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:00:49.947408 train SE 7.879 test SE 7.505\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:00:49.645210 train SE 7.882 test SE 7.510\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:00:49.405353 train SE 7.885 test SE 7.508\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:00:50.108995 train SE 7.888 test SE 7.498\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:00:49.338530 train SE 7.890 test SE 7.500\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:00:49.104176 train SE 7.893 test SE 7.498\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:00:49.301126 train SE 7.895 test SE 7.496\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:00:49.001937 train SE 7.898 test SE 7.498\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:00:49.430300 train SE 7.900 test SE 7.497\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:00:49.487626 train SE 7.903 test SE 7.493\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:00:49.181459 train SE 7.904 test SE 7.494\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:00:49.447733 train SE 7.905 test SE 7.493\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:00:49.339529 train SE 7.909 test SE 7.496\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:00:49.108640 train SE 7.911 test SE 7.489\n",
      "========================\n",
      "lr:4.2184e-04\n",
      "Epoch: 112 time 0:00:49.639221 train SE 7.912 test SE 7.487\n",
      "========================\n",
      "lr:4.1996e-04\n",
      "Epoch: 113 time 0:00:49.040845 train SE 7.914 test SE 7.487\n",
      "========================\n",
      "lr:4.1812e-04\n",
      "Epoch: 114 time 0:00:49.356976 train SE 7.915 test SE 7.491\n",
      "========================\n",
      "lr:4.1630e-04\n",
      "Epoch: 115 time 0:00:49.925473 train SE 7.917 test SE 7.483\n",
      "========================\n",
      "lr:4.1450e-04\n",
      "Epoch: 116 time 0:00:48.639894 train SE 7.919 test SE 7.485\n",
      "========================\n",
      "lr:4.1272e-04\n",
      "Epoch: 117 time 0:00:49.934936 train SE 7.920 test SE 7.480\n",
      "========================\n",
      "lr:4.1097e-04\n",
      "Epoch: 118 time 0:00:49.430777 train SE 7.921 test SE 7.480\n",
      "========================\n",
      "lr:4.0924e-04\n",
      "Epoch: 119 time 0:00:48.545161 train SE 7.923 test SE 7.487\n",
      "========================\n",
      "lr:4.0753e-04\n",
      "Epoch: 120 time 0:00:49.750923 train SE 7.924 test SE 7.482\n",
      "========================\n",
      "lr:4.0584e-04\n",
      "Epoch: 121 time 0:00:49.480152 train SE 7.926 test SE 7.481\n",
      "========================\n",
      "lr:4.0418e-04\n",
      "Epoch: 122 time 0:00:49.530523 train SE 7.927 test SE 7.482\n",
      "========================\n",
      "lr:4.0253e-04\n",
      "Epoch: 123 time 0:00:49.510587 train SE 7.927 test SE 7.486\n",
      "========================\n",
      "lr:4.0090e-04\n",
      "Epoch: 124 time 0:00:48.999930 train SE 7.928 test SE 7.482\n",
      "========================\n",
      "lr:3.9930e-04\n",
      "Epoch: 125 time 0:00:49.641577 train SE 7.930 test SE 7.479\n",
      "========================\n",
      "lr:3.9771e-04\n",
      "Epoch: 126 time 0:00:49.581376 train SE 7.930 test SE 7.472\n",
      "========================\n",
      "lr:3.9614e-04\n",
      "Epoch: 127 time 0:00:49.000442 train SE 7.931 test SE 7.483\n",
      "========================\n",
      "lr:3.9459e-04\n",
      "Epoch: 128 time 0:00:49.770880 train SE 7.932 test SE 7.481\n",
      "========================\n",
      "lr:3.9306e-04\n",
      "Epoch: 129 time 0:00:48.909174 train SE 7.934 test SE 7.479\n",
      "========================\n",
      "lr:3.9154e-04\n",
      "Epoch: 130 time 0:00:49.449728 train SE 7.935 test SE 7.484\n",
      "========================\n",
      "lr:3.9005e-04\n",
      "Epoch: 131 time 0:00:49.161007 train SE 7.933 test SE 7.476\n",
      "========================\n",
      "lr:3.8857e-04\n",
      "Epoch: 132 time 0:00:48.939109 train SE 7.936 test SE 7.478\n",
      "========================\n",
      "lr:3.8710e-04\n",
      "Epoch: 133 time 0:00:49.805775 train SE 7.937 test SE 7.480\n",
      "========================\n",
      "lr:3.8566e-04\n",
      "Epoch: 134 time 0:00:49.373931 train SE 7.938 test SE 7.476\n",
      "========================\n",
      "lr:3.8422e-04\n",
      "Epoch: 135 time 0:00:49.176459 train SE 7.939 test SE 7.470\n",
      "========================\n",
      "lr:3.8281e-04\n",
      "Epoch: 136 time 0:00:50.029178 train SE 7.940 test SE 7.476\n",
      "========================\n",
      "lr:3.8141e-04\n",
      "Epoch: 137 time 0:00:48.966531 train SE 7.941 test SE 7.473\n",
      "========================\n",
      "lr:3.8003e-04\n",
      "Epoch: 138 time 0:00:49.563425 train SE 7.942 test SE 7.475\n",
      "========================\n",
      "lr:3.7866e-04\n",
      "Epoch: 139 time 0:00:49.698062 train SE 7.943 test SE 7.479\n",
      "========================\n",
      "lr:3.7730e-04\n",
      "Epoch: 140 time 0:00:49.220342 train SE 7.944 test SE 7.474\n",
      "========================\n",
      "lr:3.7596e-04\n",
      "Epoch: 141 time 0:00:49.719020 train SE 7.945 test SE 7.476\n",
      "========================\n",
      "lr:3.7463e-04\n",
      "Epoch: 142 time 0:00:49.292654 train SE 7.946 test SE 7.469\n",
      "========================\n",
      "lr:3.7332e-04\n",
      "Epoch: 143 time 0:00:48.803965 train SE 7.946 test SE 7.475\n",
      "========================\n",
      "lr:3.7202e-04\n",
      "Epoch: 144 time 0:00:49.988287 train SE 7.948 test SE 7.473\n",
      "========================\n",
      "lr:3.7074e-04\n",
      "Epoch: 145 time 0:00:49.542480 train SE 7.949 test SE 7.473\n",
      "========================\n",
      "lr:3.6947e-04\n",
      "Epoch: 146 time 0:00:49.526522 train SE 7.948 test SE 7.474\n",
      "========================\n",
      "lr:3.6821e-04\n",
      "Epoch: 147 time 0:00:49.649715 train SE 7.950 test SE 7.474\n",
      "========================\n",
      "lr:3.6696e-04\n",
      "Epoch: 148 time 0:00:48.814426 train SE 7.952 test SE 7.469\n",
      "========================\n",
      "lr:3.6573e-04\n",
      "Epoch: 149 time 0:00:49.679153 train SE 7.952 test SE 7.475\n",
      "========================\n",
      "lr:3.6451e-04\n",
      "Epoch: 150 time 0:00:49.195409 train SE 7.953 test SE 7.472\n",
      "========================\n",
      "lr:3.6330e-04\n",
      "Epoch: 151 time 0:00:49.343013 train SE 7.954 test SE 7.474\n",
      "========================\n",
      "lr:3.6210e-04\n",
      "Epoch: 152 time 0:00:49.825722 train SE 7.955 test SE 7.475\n",
      "========================\n",
      "lr:3.6092e-04\n",
      "Epoch: 153 time 0:00:49.799299 train SE 7.956 test SE 7.472\n",
      "========================\n",
      "lr:3.5974e-04\n",
      "Epoch: 154 time 0:00:48.710704 train SE 7.956 test SE 7.474\n",
      "========================\n",
      "lr:3.5858e-04\n",
      "Epoch: 155 time 0:00:50.080547 train SE 7.957 test SE 7.472\n",
      "========================\n",
      "lr:3.5743e-04\n",
      "Epoch: 156 time 0:00:48.673331 train SE 7.958 test SE 7.469\n",
      "========================\n",
      "lr:3.5629e-04\n",
      "Epoch: 157 time 0:00:50.286490 train SE 7.959 test SE 7.470\n",
      "========================\n",
      "lr:3.5516e-04\n",
      "Epoch: 158 time 0:00:49.408838 train SE 7.959 test SE 7.473\n",
      "========================\n",
      "lr:3.5404e-04\n",
      "Epoch: 159 time 0:00:48.977989 train SE 7.960 test SE 7.469\n",
      "========================\n",
      "lr:3.5293e-04\n",
      "Epoch: 160 time 0:00:49.547466 train SE 7.960 test SE 7.466\n",
      "========================\n",
      "lr:3.5184e-04\n",
      "Epoch: 161 time 0:00:49.927979 train SE 7.961 test SE 7.463\n",
      "========================\n",
      "lr:3.5075e-04\n",
      "Epoch: 162 time 0:00:48.928122 train SE 7.961 test SE 7.472\n",
      "========================\n",
      "lr:3.4967e-04\n",
      "Epoch: 163 time 0:00:49.412851 train SE 7.962 test SE 7.472\n",
      "========================\n",
      "lr:3.4860e-04\n",
      "Epoch: 164 time 0:00:48.665333 train SE 7.963 test SE 7.471\n",
      "========================\n",
      "lr:3.4754e-04\n",
      "Epoch: 165 time 0:00:50.076053 train SE 7.964 test SE 7.470\n",
      "========================\n",
      "lr:3.4650e-04\n",
      "Epoch: 166 time 0:00:49.589365 train SE 7.964 test SE 7.470\n",
      "========================\n",
      "lr:3.4546e-04\n",
      "Epoch: 167 time 0:00:49.524540 train SE 7.964 test SE 7.467\n",
      "========================\n",
      "lr:3.4443e-04\n",
      "Epoch: 168 time 0:00:50.125931 train SE 7.965 test SE 7.467\n",
      "========================\n",
      "lr:3.4341e-04\n",
      "Epoch: 169 time 0:00:49.602320 train SE 7.966 test SE 7.470\n",
      "========================\n",
      "lr:3.4240e-04\n",
      "Epoch: 170 time 0:00:49.401363 train SE 7.966 test SE 7.471\n",
      "========================\n",
      "lr:3.4139e-04\n",
      "Epoch: 171 time 0:00:49.513079 train SE 7.968 test SE 7.466\n",
      "========================\n",
      "lr:3.4040e-04\n",
      "Epoch: 172 time 0:00:49.127589 train SE 7.969 test SE 7.465\n",
      "========================\n",
      "lr:3.3941e-04\n",
      "Epoch: 173 time 0:00:49.517064 train SE 7.967 test SE 7.465\n",
      "========================\n",
      "lr:3.3844e-04\n",
      "Epoch: 174 time 0:00:49.449259 train SE 7.968 test SE 7.467\n",
      "========================\n",
      "lr:3.3747e-04\n",
      "Epoch: 175 time 0:00:49.123600 train SE 7.968 test SE 7.471\n",
      "========================\n",
      "lr:3.3651e-04\n",
      "Epoch: 176 time 0:00:50.230639 train SE 7.970 test SE 7.467\n",
      "========================\n",
      "lr:3.3556e-04\n",
      "Epoch: 177 time 0:00:49.490126 train SE 7.970 test SE 7.464\n",
      "========================\n",
      "lr:3.3461e-04\n",
      "Epoch: 178 time 0:00:48.837366 train SE 7.970 test SE 7.463\n",
      "========================\n",
      "lr:3.3368e-04\n",
      "Epoch: 179 time 0:00:49.791321 train SE 7.971 test SE 7.471\n",
      "The best SE is: 7.600\n",
      "[4.87077999 7.00654984 7.33394575 7.39658833 7.43063736 7.44808531\n",
      " 7.46836329 7.47852945 7.48289967 7.48143482 7.49352264 7.49643326\n",
      " 7.50840473 7.51280355 7.5197444  7.51210546 7.51050043 7.51573944\n",
      " 7.51591969 7.52457047 7.52777958 7.54699945 7.5500803  7.55267668\n",
      " 7.55088139 7.55294895 7.56420088 7.57195997 7.56138468 7.57228994\n",
      " 7.57100773 7.58511972 7.57828236 7.58025312 7.57935858 7.58688879\n",
      " 7.58718109 7.57694006 7.58958387 7.59241438 7.58731985 7.59262466\n",
      " 7.59527206 7.58891058 7.58952856 7.59569502 7.60034943 7.59869337\n",
      " 7.58894348 7.59325647 7.58379126 7.58265781 7.58876753 7.58344269\n",
      " 7.58519316 7.58294249 7.58533621 7.5727067  7.58189774 7.57520056\n",
      " 7.57144928 7.56705713 7.56651545 7.56700087 7.5633173  7.56328821\n",
      " 7.55831766 7.55655527 7.55664015 7.56139135 7.55627012 7.54770041\n",
      " 7.54930973 7.55484104 7.54902411 7.54414225 7.54439688 7.53965092\n",
      " 7.540555   7.53939676 7.54164505 7.53215504 7.53491545 7.53032398\n",
      " 7.5279727  7.52888441 7.52495575 7.52059698 7.52027988 7.52049112\n",
      " 7.51662064 7.51472187 7.51257658 7.51052952 7.50776768 7.50900412\n",
      " 7.50971985 7.50865889 7.50450373 7.51036167 7.50847721 7.49776554\n",
      " 7.49950123 7.49838257 7.49610138 7.49783468 7.49730682 7.49277973\n",
      " 7.49411392 7.49335861 7.49646235 7.4891634  7.48741293 7.48723173\n",
      " 7.49060059 7.48307657 7.48490667 7.4800849  7.48038816 7.4874773\n",
      " 7.48214579 7.48052073 7.48235083 7.48571253 7.48158789 7.47908354\n",
      " 7.47197056 7.48271799 7.48129272 7.47863626 7.48405218 7.47588682\n",
      " 7.47753239 7.47961426 7.47576618 7.46960068 7.47634125 7.47335434\n",
      " 7.47470331 7.47895384 7.47367096 7.47623396 7.46851301 7.47535801\n",
      " 7.47318602 7.47265482 7.4737277  7.47382307 7.46907902 7.475492\n",
      " 7.47202253 7.47363615 7.4747529  7.47190571 7.47383833 7.47172928\n",
      " 7.46924067 7.46994734 7.47269297 7.46936893 7.46593618 7.46259546\n",
      " 7.4721489  7.47207785 7.47144699 7.47047377 7.46965647 7.46675968\n",
      " 7.46742868 7.4701128  7.47096729 7.46639585 7.46467304 7.46505213\n",
      " 7.46681547 7.47132349 7.46746016 7.46393061 7.46299601 7.47136831]\n",
      "(100000, 2, 32, 128)\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (conv_l1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l1): Mish()\n",
      "  (conv_l2): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_bn_l2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_mish_l2): Mish()\n",
      "  (FC_l1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (bn_l1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (FC_l2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn_l2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC_l3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (bn_l3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "  (bn1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (src_embedding_0): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_0): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_0): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (src_embedding_1): Embedding(\n",
      "    (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding_1): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder_1): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (src_embedding): Embedding(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (src_pos_embedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): Trans_Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (sublayers): ModuleList(\n",
      "          (0): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerLogic(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (multi_headed_attention): MultiHeadedAttention(\n",
      "          (qkv_nets): ModuleList(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (pointwise_net): PositionwiseFeedForwardNet(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n",
      "Epoch: 0 time 0:00:51.225977 train SE 3.349 test SE 5.030\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:00:51.149241 train SE 6.142 test SE 7.036\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:00:50.390757 train SE 7.148 test SE 7.333\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:00:50.933758 train SE 7.368 test SE 7.384\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:00:49.821291 train SE 7.465 test SE 7.418\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:00:51.217522 train SE 7.528 test SE 7.434\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:00:50.902385 train SE 7.568 test SE 7.444\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:00:50.429631 train SE 7.593 test SE 7.467\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:00:51.172153 train SE 7.611 test SE 7.470\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:00:51.161177 train SE 7.619 test SE 7.472\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:00:50.832555 train SE 7.626 test SE 7.485\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:00:50.901843 train SE 7.630 test SE 7.493\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:00:50.455563 train SE 7.632 test SE 7.490\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:00:50.906338 train SE 7.634 test SE 7.494\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:00:50.494963 train SE 7.636 test SE 7.504\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:00:50.737295 train SE 7.639 test SE 7.500\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:00:50.646546 train SE 7.639 test SE 7.499\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:00:50.417140 train SE 7.641 test SE 7.510\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:00:50.564266 train SE 7.643 test SE 7.512\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:00:51.218996 train SE 7.644 test SE 7.512\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:00:50.478019 train SE 7.648 test SE 7.527\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:00:50.624673 train SE 7.657 test SE 7.521\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:00:50.319419 train SE 7.666 test SE 7.541\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:00:51.044991 train SE 7.672 test SE 7.546\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:00:49.780869 train SE 7.684 test SE 7.545\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:00:50.072091 train SE 7.691 test SE 7.559\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:00:49.259746 train SE 7.700 test SE 7.544\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:00:49.961359 train SE 7.708 test SE 7.554\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:00:49.226325 train SE 7.715 test SE 7.561\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:00:49.807800 train SE 7.722 test SE 7.561\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:00:49.845669 train SE 7.728 test SE 7.553\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:00:49.516071 train SE 7.735 test SE 7.555\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:00:50.065591 train SE 7.742 test SE 7.555\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:00:49.650214 train SE 7.749 test SE 7.553\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:00:49.932952 train SE 7.755 test SE 7.548\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:00:50.118953 train SE 7.760 test SE 7.545\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:00:49.946917 train SE 7.765 test SE 7.547\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:00:50.430610 train SE 7.772 test SE 7.545\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:00:49.665152 train SE 7.775 test SE 7.543\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:00:49.452268 train SE 7.782 test SE 7.540\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:00:50.484490 train SE 7.786 test SE 7.542\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:00:50.289494 train SE 7.791 test SE 7.535\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:00:49.720510 train SE 7.796 test SE 7.529\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:00:50.069578 train SE 7.801 test SE 7.523\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:00:49.694111 train SE 7.805 test SE 7.525\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:00:49.734485 train SE 7.812 test SE 7.527\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:00:49.663666 train SE 7.816 test SE 7.523\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:00:49.942410 train SE 7.820 test SE 7.518\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:00:50.111482 train SE 7.826 test SE 7.520\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:00:50.083034 train SE 7.832 test SE 7.510\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:00:49.872106 train SE 7.836 test SE 7.510\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:00:50.351334 train SE 7.841 test SE 7.509\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:00:49.295174 train SE 7.847 test SE 7.508\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:00:50.452071 train SE 7.852 test SE 7.509\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:00:49.972330 train SE 7.857 test SE 7.503\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:00:49.740475 train SE 7.862 test SE 7.500\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:00:50.171305 train SE 7.866 test SE 7.505\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:00:49.207899 train SE 7.871 test SE 7.496\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:00:49.930443 train SE 7.876 test SE 7.493\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:00:50.078089 train SE 7.881 test SE 7.497\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:00:49.631748 train SE 7.886 test SE 7.492\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:00:50.170318 train SE 7.890 test SE 7.493\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:00:50.244602 train SE 7.894 test SE 7.487\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:00:49.358478 train SE 7.898 test SE 7.489\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:00:50.366781 train SE 7.902 test SE 7.490\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:00:49.491140 train SE 7.906 test SE 7.485\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:00:50.361322 train SE 7.910 test SE 7.481\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:00:50.194244 train SE 7.913 test SE 7.485\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:00:49.309610 train SE 7.917 test SE 7.478\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:00:50.293989 train SE 7.919 test SE 7.483\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:00:49.593356 train SE 7.923 test SE 7.481\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:00:50.451558 train SE 7.926 test SE 7.477\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:00:49.954887 train SE 7.929 test SE 7.477\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:00:49.471680 train SE 7.932 test SE 7.479\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:00:50.658002 train SE 7.935 test SE 7.470\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:00:49.784339 train SE 7.938 test SE 7.474\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:00:49.712534 train SE 7.940 test SE 7.471\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:00:50.103979 train SE 7.943 test SE 7.477\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:00:49.448730 train SE 7.945 test SE 7.472\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:00:50.282500 train SE 7.947 test SE 7.465\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:00:50.093512 train SE 7.950 test SE 7.470\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:00:49.675138 train SE 7.952 test SE 7.469\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:00:50.216183 train SE 7.954 test SE 7.469\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:00:49.565936 train SE 7.956 test SE 7.470\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:00:49.911527 train SE 7.958 test SE 7.470\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:00:50.111464 train SE 7.960 test SE 7.471\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:00:49.594846 train SE 7.962 test SE 7.469\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:00:50.426132 train SE 7.963 test SE 7.468\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:00:50.080563 train SE 7.966 test SE 7.466\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:00:49.724499 train SE 7.967 test SE 7.469\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:00:50.551319 train SE 7.969 test SE 7.467\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:00:49.381927 train SE 7.971 test SE 7.465\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:00:50.471502 train SE 7.972 test SE 7.463\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:00:49.836705 train SE 7.973 test SE 7.463\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:00:50.138392 train SE 7.975 test SE 7.470\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:00:49.914519 train SE 7.976 test SE 7.466\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:00:50.154350 train SE 7.978 test SE 7.465\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:00:49.750429 train SE 7.980 test SE 7.465\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:00:49.788821 train SE 7.981 test SE 7.459\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:00:49.739952 train SE 7.983 test SE 7.465\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:00:50.534826 train SE 7.983 test SE 7.465\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:00:50.097994 train SE 7.985 test SE 7.462\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:00:49.407851 train SE 7.986 test SE 7.465\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:00:50.439592 train SE 7.987 test SE 7.466\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:00:49.098186 train SE 7.987 test SE 7.458\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:00:50.131940 train SE 7.988 test SE 7.462\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:00:50.139893 train SE 7.990 test SE 7.460\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:00:49.441749 train SE 7.990 test SE 7.462\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:00:50.568748 train SE 7.990 test SE 7.455\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:00:49.825723 train SE 7.991 test SE 7.458\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:00:49.581375 train SE 7.992 test SE 7.456\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:00:50.074563 train SE 7.993 test SE 7.459\n",
      "========================\n",
      "lr:4.2184e-04\n",
      "Epoch: 112 time 0:00:49.489661 train SE 7.992 test SE 7.458\n",
      "========================\n",
      "lr:4.1996e-04\n",
      "Epoch: 113 time 0:00:50.352313 train SE 7.986 test SE 7.449\n",
      "========================\n",
      "lr:4.1812e-04\n",
      "Epoch: 114 time 0:00:50.692404 train SE 7.970 test SE 7.455\n",
      "========================\n",
      "lr:4.1630e-04\n",
      "Epoch: 115 time 0:00:49.612811 train SE 7.976 test SE 7.455\n",
      "========================\n",
      "lr:4.1450e-04\n",
      "Epoch: 116 time 0:00:50.123432 train SE 7.986 test SE 7.460\n",
      "========================\n",
      "lr:4.1272e-04\n",
      "Epoch: 117 time 0:00:49.298133 train SE 7.993 test SE 7.453\n",
      "========================\n",
      "lr:4.1097e-04\n",
      "Epoch: 118 time 0:00:50.002262 train SE 7.993 test SE 7.456\n",
      "========================\n",
      "lr:4.0924e-04\n",
      "Epoch: 119 time 0:00:49.846174 train SE 7.990 test SE 7.455\n",
      "========================\n",
      "lr:4.0753e-04\n",
      "Epoch: 120 time 0:00:49.476662 train SE 7.995 test SE 7.457\n",
      "========================\n",
      "lr:4.0584e-04\n",
      "Epoch: 121 time 0:00:50.532338 train SE 7.991 test SE 7.449\n",
      "========================\n",
      "lr:4.0418e-04\n",
      "Epoch: 122 time 0:00:50.080547 train SE 7.981 test SE 7.452\n",
      "========================\n",
      "lr:4.0253e-04\n",
      "Epoch: 123 time 0:00:49.880096 train SE 7.990 test SE 7.452\n",
      "========================\n",
      "lr:4.0090e-04\n",
      "Epoch: 124 time 0:00:50.758240 train SE 7.996 test SE 7.453\n",
      "========================\n",
      "lr:3.9930e-04\n",
      "Epoch: 125 time 0:00:49.926470 train SE 7.997 test SE 7.457\n",
      "========================\n",
      "lr:3.9771e-04\n",
      "Epoch: 126 time 0:00:50.481474 train SE 7.996 test SE 7.457\n",
      "========================\n",
      "lr:3.9614e-04\n",
      "Epoch: 127 time 0:00:49.836693 train SE 7.998 test SE 7.459\n",
      "========================\n",
      "lr:3.9459e-04\n",
      "Epoch: 128 time 0:00:50.069103 train SE 7.999 test SE 7.456\n",
      "========================\n",
      "lr:3.9306e-04\n",
      "Epoch: 129 time 0:00:51.387544 train SE 7.999 test SE 7.454\n",
      "========================\n",
      "lr:3.9154e-04\n",
      "Epoch: 130 time 0:00:49.533012 train SE 8.000 test SE 7.453\n",
      "========================\n",
      "lr:3.9005e-04\n",
      "Epoch: 131 time 0:00:49.653690 train SE 8.001 test SE 7.455\n",
      "========================\n",
      "lr:3.8857e-04\n",
      "Epoch: 132 time 0:00:49.570929 train SE 8.002 test SE 7.460\n",
      "========================\n",
      "lr:3.8710e-04\n",
      "Epoch: 133 time 0:00:50.579750 train SE 7.999 test SE 7.459\n",
      "========================\n",
      "lr:3.8566e-04\n",
      "Epoch: 134 time 0:00:50.421660 train SE 8.002 test SE 7.454\n",
      "========================\n",
      "lr:3.8422e-04\n",
      "Epoch: 135 time 0:00:49.435274 train SE 8.003 test SE 7.453\n",
      "========================\n",
      "lr:3.8281e-04\n",
      "Epoch: 136 time 0:00:50.383764 train SE 8.004 test SE 7.455\n",
      "========================\n",
      "lr:3.8141e-04\n",
      "Epoch: 137 time 0:00:50.338351 train SE 8.003 test SE 7.451\n",
      "========================\n",
      "lr:3.8003e-04\n",
      "Epoch: 138 time 0:00:49.886559 train SE 8.003 test SE 7.451\n",
      "========================\n",
      "lr:3.7866e-04\n",
      "Epoch: 139 time 0:00:50.376249 train SE 8.004 test SE 7.453\n",
      "========================\n",
      "lr:3.7730e-04\n",
      "Epoch: 140 time 0:00:49.782837 train SE 8.005 test SE 7.453\n",
      "========================\n",
      "lr:3.7596e-04\n",
      "Epoch: 141 time 0:00:50.668468 train SE 8.005 test SE 7.453\n",
      "========================\n",
      "lr:3.7463e-04\n",
      "Epoch: 142 time 0:00:50.639545 train SE 8.006 test SE 7.450\n",
      "========================\n",
      "lr:3.7332e-04\n",
      "Epoch: 143 time 0:00:49.892544 train SE 8.006 test SE 7.451\n",
      "========================\n",
      "lr:3.7202e-04\n",
      "Epoch: 144 time 0:00:50.909822 train SE 8.005 test SE 7.450\n",
      "========================\n",
      "lr:3.7074e-04\n",
      "Epoch: 145 time 0:00:50.214682 train SE 8.004 test SE 7.451\n",
      "========================\n",
      "lr:3.6947e-04\n",
      "Epoch: 146 time 0:00:50.480969 train SE 8.004 test SE 7.449\n",
      "========================\n",
      "lr:3.6821e-04\n",
      "Epoch: 147 time 0:00:50.608628 train SE 8.006 test SE 7.449\n",
      "========================\n",
      "lr:3.6696e-04\n",
      "Epoch: 148 time 0:00:50.667472 train SE 8.006 test SE 7.448\n",
      "========================\n",
      "lr:3.6573e-04\n",
      "Epoch: 149 time 0:00:50.060094 train SE 8.007 test SE 7.447\n",
      "========================\n",
      "lr:3.6451e-04\n",
      "Epoch: 150 time 0:00:50.570730 train SE 8.007 test SE 7.452\n",
      "========================\n",
      "lr:3.6330e-04\n",
      "Epoch: 151 time 0:00:50.188751 train SE 8.007 test SE 7.447\n",
      "========================\n",
      "lr:3.6210e-04\n",
      "Epoch: 152 time 0:00:50.871924 train SE 8.007 test SE 7.442\n",
      "========================\n",
      "lr:3.6092e-04\n",
      "Epoch: 153 time 0:00:50.584692 train SE 8.005 test SE 7.448\n",
      "========================\n",
      "lr:3.5974e-04\n",
      "Epoch: 154 time 0:00:50.153845 train SE 8.007 test SE 7.449\n",
      "========================\n",
      "lr:3.5858e-04\n",
      "Epoch: 155 time 0:00:50.799119 train SE 8.008 test SE 7.445\n",
      "========================\n",
      "lr:3.5743e-04\n",
      "Epoch: 156 time 0:00:50.497923 train SE 8.009 test SE 7.448\n",
      "========================\n",
      "lr:3.5629e-04\n",
      "Epoch: 157 time 0:00:50.941737 train SE 8.009 test SE 7.443\n",
      "========================\n",
      "lr:3.5516e-04\n",
      "Epoch: 158 time 0:00:50.351316 train SE 8.010 test SE 7.445\n",
      "========================\n",
      "lr:3.5404e-04\n",
      "Epoch: 159 time 0:00:50.656499 train SE 8.009 test SE 7.445\n",
      "========================\n",
      "lr:3.5293e-04\n",
      "Epoch: 160 time 0:00:50.967668 train SE 8.009 test SE 7.444\n",
      "========================\n",
      "lr:3.5184e-04\n",
      "Epoch: 161 time 0:00:50.614611 train SE 8.010 test SE 7.450\n",
      "========================\n",
      "lr:3.5075e-04\n",
      "Epoch: 162 time 0:00:50.530836 train SE 8.009 test SE 7.445\n",
      "========================\n",
      "lr:3.4967e-04\n",
      "Epoch: 163 time 0:00:50.787151 train SE 8.008 test SE 7.443\n",
      "========================\n",
      "lr:3.4860e-04\n",
      "Epoch: 164 time 0:00:50.406169 train SE 8.010 test SE 7.446\n",
      "========================\n",
      "lr:3.4754e-04\n",
      "Epoch: 165 time 0:00:50.679438 train SE 8.011 test SE 7.445\n",
      "========================\n",
      "lr:3.4650e-04\n",
      "Epoch: 166 time 0:00:50.907828 train SE 8.012 test SE 7.446\n",
      "========================\n",
      "lr:3.4546e-04\n",
      "Epoch: 167 time 0:00:50.450051 train SE 8.009 test SE 7.443\n",
      "========================\n",
      "lr:3.4443e-04\n",
      "Epoch: 168 time 0:00:51.110286 train SE 8.011 test SE 7.441\n",
      "========================\n",
      "lr:3.4341e-04\n",
      "Epoch: 169 time 0:00:50.727312 train SE 8.013 test SE 7.444\n",
      "========================\n",
      "lr:3.4240e-04\n",
      "Epoch: 170 time 0:00:50.273524 train SE 8.013 test SE 7.440\n",
      "========================\n",
      "lr:3.4139e-04\n",
      "Epoch: 171 time 0:00:50.899848 train SE 8.011 test SE 7.442\n",
      "========================\n",
      "lr:3.4040e-04\n",
      "Epoch: 172 time 0:00:50.848985 train SE 8.013 test SE 7.447\n",
      "========================\n",
      "lr:3.3941e-04\n",
      "Epoch: 173 time 0:00:50.570729 train SE 8.012 test SE 7.448\n",
      "========================\n",
      "lr:3.3844e-04\n",
      "Epoch: 174 time 0:00:52.847639 train SE 8.014 test SE 7.446\n",
      "========================\n",
      "lr:3.3747e-04\n",
      "Epoch: 175 time 0:00:51.945053 train SE 8.013 test SE 7.447\n",
      "========================\n",
      "lr:3.3651e-04\n",
      "Epoch: 176 time 0:00:52.569383 train SE 8.014 test SE 7.442\n",
      "========================\n",
      "lr:3.3556e-04\n",
      "Epoch: 177 time 0:00:52.730200 train SE 8.014 test SE 7.448\n",
      "========================\n",
      "lr:3.3461e-04\n",
      "Epoch: 178 time 0:00:51.078371 train SE 8.015 test SE 7.444\n",
      "========================\n",
      "lr:3.3368e-04\n",
      "Epoch: 179 time 0:00:50.873918 train SE 8.015 test SE 7.446\n",
      "The best SE is: 7.561\n",
      "[5.02979946 7.03602457 7.33299494 7.38365936 7.41830206 7.43442631\n",
      " 7.44417906 7.46720648 7.46971607 7.47179127 7.48541117 7.49304199\n",
      " 7.4901619  7.49398279 7.5039773  7.49999571 7.49947596 7.51049757\n",
      " 7.51212692 7.51214552 7.52704096 7.5214715  7.54145193 7.54555368\n",
      " 7.54521942 7.55924177 7.54432154 7.55367279 7.56115103 7.56055164\n",
      " 7.55311584 7.55467987 7.55485249 7.55338049 7.54795551 7.54541111\n",
      " 7.54691029 7.54455662 7.54307032 7.54042578 7.54193211 7.53471851\n",
      " 7.52907515 7.52266836 7.52454233 7.52668762 7.52265787 7.51823139\n",
      " 7.51959705 7.51047659 7.50977802 7.50893164 7.50814009 7.50871658\n",
      " 7.50326777 7.50012302 7.50485468 7.4961648  7.49279785 7.49717474\n",
      " 7.49184179 7.49337244 7.48696232 7.48946381 7.48980808 7.48503876\n",
      " 7.48121214 7.48539209 7.47763395 7.48293829 7.480546   7.47661448\n",
      " 7.47683191 7.47858906 7.4701767  7.47394657 7.47112989 7.47696304\n",
      " 7.4722023  7.46548462 7.47014952 7.46933222 7.46911955 7.47021103\n",
      " 7.46957397 7.47123337 7.46866179 7.46752119 7.46565104 7.46919012\n",
      " 7.46652555 7.46476746 7.46289539 7.46294355 7.46981955 7.46587229\n",
      " 7.46549988 7.46497583 7.45939255 7.46501875 7.46492243 7.46165228\n",
      " 7.46488523 7.46639252 7.45807981 7.46167374 7.45973682 7.46161795\n",
      " 7.45453262 7.45832825 7.45592976 7.45922565 7.45802259 7.44938803\n",
      " 7.45500898 7.45458698 7.45958853 7.45264053 7.4561553  7.45453215\n",
      " 7.4569726  7.44922876 7.45169926 7.45167637 7.45314884 7.45702505\n",
      " 7.45660925 7.45934296 7.45575809 7.45417261 7.45313215 7.45467377\n",
      " 7.45983458 7.45909119 7.45443821 7.45277119 7.45478964 7.4508605\n",
      " 7.45054579 7.45264006 7.45253897 7.45321417 7.45000935 7.45118475\n",
      " 7.44985676 7.45063925 7.44923115 7.44886017 7.44825363 7.44731092\n",
      " 7.45224905 7.44741583 7.44203806 7.44780588 7.44933653 7.44522047\n",
      " 7.44761133 7.44274664 7.44521856 7.44459295 7.4435091  7.44962788\n",
      " 7.44462061 7.44305801 7.44610596 7.44542456 7.44590998 7.44333029\n",
      " 7.44118261 7.4441638  7.44048929 7.44227743 7.44652414 7.4477334\n",
      " 7.44623184 7.44732618 7.44191837 7.44823599 7.44380283 7.44604969]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Nc = 32\n",
    "# N = 2  # 多径数\n",
    "Nt = 64 # 基站端天线数\n",
    "Nr = 1 # 用户端天线数\n",
    "# B = 30\n",
    "\n",
    "L = 8  # 用户端接收的观测，即OFDM个数\n",
    "SNR_dB = 10\n",
    "K = 2  # 用户数\n",
    "snr = 10**(SNR_dB/10)/K\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 180\n",
    "\n",
    "#B = 40  # 反馈bit数\n",
    "#N_=[5,6]\n",
    "N=6\n",
    "Bs = [64,128,256,512,1024]\n",
    "if __name__ == '__main__':\n",
    "    for B in Bs:\n",
    "        train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
