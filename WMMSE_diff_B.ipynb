{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第二块GPU（从0开始）\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from IPython import display\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from matplotlib import cm\n",
    "from scipy.linalg import block_diag\n",
    "import datetime\n",
    "from torch.nn.utils import *\n",
    "from Transformer_model import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "Nc = 32       # 子载波数目\n",
    "N = 2         # 路径数目\n",
    "Nt = 64       # 基站侧天线数目\n",
    "Nr = 1        # 用户端天线数目\n",
    "\n",
    "L = 8         # OFDM导频数目\n",
    "SNR_dB = 10   # SNR_dB\n",
    "K = 2         # 用户数目\n",
    "snr = 10**(SNR_dB/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.type(torch.uint8)\n",
    "\n",
    "    def integer2bit(integer, num_bits=B * 2):\n",
    "        dtype = integer.type()\n",
    "        exponent_bits = -torch.arange(-(num_bits - 1), 1).type(dtype)\n",
    "        exponent_bits = exponent_bits.repeat(integer.shape + (1,))\n",
    "        out = integer.unsqueeze(-1) // 2 ** exponent_bits\n",
    "        return (out - (out % 1)) % 2\n",
    "\n",
    "    bit = integer2bit(Num_)\n",
    "    bit = (bit[:, :, B:]).reshape(-1, Num_.shape[1] * B)\n",
    "    return bit.type(torch.float32)\n",
    "\n",
    "\n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.type(torch.float32)\n",
    "    Bit_ = torch.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = torch.zeros(Bit_[:, :, 0].shape).cuda()\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return num\n",
    "\n",
    "\n",
    "class Quantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = torch.round(x * step - 0.5)\n",
    "        out = Num2Bit(out, B)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of constant arguments to forward must be None.\n",
    "        # Gradient of a number is the sum of its four bits.\n",
    "        b, _ = grad_output.shape\n",
    "        grad_num = torch.sum(grad_output.reshape(b, -1, ctx.constant), dim=2)\n",
    "        return grad_num, None\n",
    "\n",
    "\n",
    "class Dequantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = Bit2Num(x, B)\n",
    "        out = (out + 0.5) / step\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        # repeat the gradient of a Num for four time.\n",
    "        #b, c = grad_output.shape\n",
    "        #grad_bit = grad_output.repeat(1, 1, ctx.constant)\n",
    "        grad_bit = grad_output.repeat_interleave(ctx.constant,dim=1)\n",
    "        return grad_bit, None\n",
    "\n",
    "\n",
    "class QuantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Quantization.apply(x, self.B)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DequantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(DequantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Dequantization.apply(x, self.B)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\CODE_WQF\\WMMSE_diff_B.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 162>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W3sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m W \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(W)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W3sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m W \u001b[39m=\u001b[39m Hermitian(W)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W3sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m W_real \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreal(W)\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W3sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m W_imag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mimag(W)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W3sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m W_real \u001b[39m=\u001b[39m W_real\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def Hermitian(X):#torch矩阵共轭转置\n",
    "    X = torch.real(X) - 1j*torch.imag(X)\n",
    "    return X.transpose(-1,-2)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_list):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=channel_list[0],  # 图片通道数  ?????\n",
    "                out_channels=channel_list[1],  # filter数量\n",
    "                kernel_size=(5, 1),  # filter大小\n",
    "                stride=1,  # 扫描步进\n",
    "                padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_list[1], eps=1e-05, momentum=0.1, affine=True),\n",
    "            Mish(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=channel_list[1],  # 图片通道数\n",
    "                out_channels=channel_list[2],  # filter数量\n",
    "                kernel_size=(5, 1),  # filter大小\n",
    "                stride=1,  # 扫描步进\n",
    "                padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_list[2], eps=1e-05, momentum=0.1, affine=True),\n",
    "            Mish(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=channel_list[2],  # 图片通道数\n",
    "                out_channels=channel_list[0],  # filter数量\n",
    "                kernel_size=(5, 1),  # filter大小\n",
    "                stride=1,  # 扫描步进\n",
    "                padding=(2, 0),  # 周围围上2圈0 使得输出的宽和高和之前一样不变小\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_list[0], eps=1e-05, momentum=0.1, affine=True),\n",
    "        )\n",
    "    def forward(self, x_in):\n",
    "        #residual = x_in\n",
    "        x = self.conv1(x_in)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        #实例化对象\n",
    "        mish = Mish()\n",
    "        #数据输入，传播\n",
    "        x = mish(x + x_in)\n",
    "        return x\n",
    "\n",
    "class MyLoss_OFDM(torch.nn.Module):  # 输入是信道和整个F,输出是频谱效率\n",
    "    def __init__(self):\n",
    "        super(MyLoss_OFDM, self).__init__()\n",
    "\n",
    "    def forward(self, H0, out, parm_set):  # H0第0个维度是样本 第1个维度是用户，第2个维度是子载波，第3个维度是天线\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "\n",
    "        H = H0.permute(0, 2, 1, 3)  # H第0个维度是样本 第1个维度是子载波，第2个维度是用户，第3个维度是天线\n",
    "        num = out.shape[0]\n",
    "        Nc = H.shape[1]\n",
    "        H_real = H[:, :, :, 0:Nt]\n",
    "        H_imag = H[:, :, :, Nt:2 * Nt]\n",
    "        Hs = torch.zeros([num, Nc, K, Nt * 2])\n",
    "        Hs = Hs.cuda()\n",
    "        Hs[:, :, 0:K, 0:Nt] = H_real\n",
    "        # Hs[:, :, K:2 * K, Nt:2 * Nt] = H_real\n",
    "        Hs[:, :, 0:K, Nt:2 * Nt] = H_imag\n",
    "        # Hs[:, :, K:2 * K, 0:Nt] = -H_imag\n",
    "\n",
    "        F = torch.zeros([num, Nc, Nt * 2, K * 2])\n",
    "        F = F.cuda()\n",
    "        F[:, :, 0:Nt, 0:K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, K:2 * K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, 0:Nt, K:2 * K] = out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, 0:K] = -out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        R = 0\n",
    "        Hk = torch.matmul(Hs, F)\n",
    "        noise = 1 / snr\n",
    "        for i in range(K):\n",
    "            signal = Hk[:, :, i, i] * Hk[:, :, i, i] + Hk[:, :, i, i + K] * Hk[:, :, i, i + K]\n",
    "            interference = torch.zeros(num, Nc)\n",
    "            interference = interference.cuda()\n",
    "            for j in range(K):\n",
    "                if j != i:\n",
    "                    interference = interference + Hk[:, :, i, j] * Hk[:, :, i, j] + Hk[:, :, i, j + K] * Hk[:, :, i,\n",
    "                                                                                                         j + K]\n",
    "            SINR = signal / (noise + interference)\n",
    "            R = R + torch.sum(torch.log2(1 + SINR))\n",
    "        R = -R / num / Nc\n",
    "        return R\n",
    "    \n",
    "class DatasetFolder(Data.Dataset):\n",
    "    def __init__(self, matData):\n",
    "        self.matdata = matData\n",
    "    def __getitem__(self, index):\n",
    "        return self.matdata[index]\n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]\n",
    "\n",
    "class NoamOpt(object):\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "             min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "def mish(x):\n",
    "    x = x * (torch.tanh(F.softplus(x)))\n",
    "    return x\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    #         print(\"Mish activation loaded...\")\n",
    "    def forward(self, x):\n",
    "        x = x * (torch.tanh(F.softplus(x)))\n",
    "        return x\n",
    "    \n",
    "def DFT_matrix(N):\n",
    "    i, j = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    omega = np.exp(- 2 * pi * 1J / N)\n",
    "    W = np.power(omega, i * j) / sqrt(N)\n",
    "    return np.mat(W)\n",
    "\n",
    "Nc = 32\n",
    "Nt = 64\n",
    "W = DFT_matrix(Nc)\n",
    "W = torch.tensor(W)\n",
    "W = Hermitian(W)\n",
    "W_real = np.real(W).cuda()\n",
    "W_imag = np.imag(W).cuda()\n",
    "W_real = W_real.float()\n",
    "W_imag = W_imag.float()\n",
    "\n",
    "W1 = DFT_matrix(Nt)\n",
    "W_real1 = torch.from_numpy(np.real(W1)).cuda()\n",
    "W_imag1 = torch.from_numpy(np.imag(W1)).cuda()\n",
    "W_real1 = W_real1.float()\n",
    "W_imag1 = W_imag1.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_US_RF_OFDM(nn.Module):\n",
    "    def __init__(self, pram_set):\n",
    "        Nc = pram_set[0]  #子载波数目\n",
    "        Nt = pram_set[1]  #发送天线数目\n",
    "        Nr = pram_set[2]  #接收天线数目\n",
    "        snr = pram_set[3] #信噪比\n",
    "        B = pram_set[4]   #量化的比特数目\n",
    "        K = pram_set[5]   #用户数目\n",
    "        super(DNN_US_RF_OFDM, self).__init__()\n",
    "\n",
    "        self.pilot = nn.Linear(Nt, L, bias=False)  # 全连接\n",
    "\n",
    "        self.res = ResBlock([2 * L, 256, 512])\n",
    "\n",
    "        self.FC2 = nn.Linear(2 * Nc * L, 1024)  # 全连接\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "        self.mish2 = Mish()\n",
    "\n",
    "        self.FC3 = nn.Linear(1024, 512)  # 全连接\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.mish3 = Mish()\n",
    "\n",
    "        self.FC4 = nn.Linear(512, B)\n",
    "        self.bn4 = nn.BatchNorm1d(B)\n",
    "\n",
    "        self.QL = QuantizationLayer(1)\n",
    "    def forward(self, h, pram_set):\n",
    "        Nc = pram_set[0]\n",
    "        Nt = pram_set[1]\n",
    "        Nr = pram_set[2]\n",
    "        snr = pram_set[3]\n",
    "        B = pram_set[4]\n",
    "        K = pram_set[5]\n",
    "\n",
    "        # Fig3 HPN网络\n",
    "        h_real = h[:, :, 0:Nt].reshape(-1, Nc, Nt, 1)\n",
    "        h_imag = h[:, :, Nt:2 * Nt].reshape(-1, Nc, Nt, 1)\n",
    "\n",
    "        F_real = torch.cos(self.pilot.weight) / sqrt(Nt) * sqrt(K)  #改成NC子载波，一共有两处要更改,还是除以K\n",
    "        F_imag = torch.sin(self.pilot.weight) / sqrt(Nt) * sqrt(K)\n",
    "\n",
    "        L_real = (torch.matmul(F_real, h_real) - torch.matmul(F_imag, h_imag)).reshape(-1, 1, Nc, L)\n",
    "        L_imag = (torch.matmul(F_real, h_imag) + torch.matmul(F_imag, h_real)).reshape(-1, 1, Nc, L)\n",
    "\n",
    "        L_sum = torch.cat((L_real, L_imag), 1)\n",
    "        # shape[0]输出矩阵行数\n",
    "        # shape[1]输出矩阵列数\n",
    "        num = h.shape[0]\n",
    "        noise = torch.randn(num, 2, Nc, L) / sqrt(2 * snr)\n",
    "        noise = noise.cuda()\n",
    "        L_sum = L_sum + noise   #由H得到Y\n",
    "        # DFT运算\n",
    "        L_real = (torch.matmul(W_real, L_sum[:, 0, :, :]) - torch.matmul(W_imag, L_sum[:, 1, :, :])).reshape(-1, Nc, L)\n",
    "        L_imag = (torch.matmul(W_imag, L_sum[:, 0, :, :]) + torch.matmul(W_real, L_sum[:, 1, :, :])).reshape(-1, Nc, L)\n",
    "        L_sum = torch.cat((L_real, L_imag), 2)\n",
    "\n",
    "        x = L_sum.transpose(1, 2)\n",
    "        x = x.reshape(-1, 2 * L, Nc, 1)\n",
    "        x = self.res(x)\n",
    "        x = x.reshape(-1, 2 * L * Nc)    # reshape操作，真值表示\n",
    "\n",
    "        x = self.FC2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.mish2(x)\n",
    "\n",
    "        x = self.FC3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.mish3(x)\n",
    "\n",
    "        x = self.FC4(x)\n",
    "        x = self.bn4(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.QL(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIS_SDMA_Precoding(nn.Module): #单层\n",
    "    def __init__(self, parm_set): \n",
    "        super(RIS_SDMA_Precoding,self).__init__()\n",
    "        \n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "\n",
    "        self.DQL = DequantizationLayer(1)  # 解1bit 量化\n",
    "\n",
    "        self.FC1 = nn.Linear(K * B, Nc*K*Nt*2)  # 全连接\n",
    "        self.bn1 = nn.BatchNorm1d(Nc*K*Nt*2)\n",
    "        self.mish1 = Mish()\n",
    "        \n",
    "        \n",
    "        self.linear_H  = nn.Linear(Nt*2, 32)\n",
    "        \n",
    "        self.trans1  = TRANS_BLOCK(32*K,32,256,3)\n",
    "        self.linear_RIS = nn.Linear(Nc*32, Nt)   #生成RIS相位\n",
    "\n",
    "        self.linear_RF = nn.Linear(Nc*32, Nt*K)   #生成RIS相位\n",
    "        \n",
    "        self.trans2  = TRANS_BLOCK(2*K*K,4*K,256,3) #4×4等效信道\n",
    "        \n",
    "    \n",
    "    def forward(self, out,parm_set):\n",
    "        # H(batch,K,Nc,Nt)\n",
    "        #H_RU [batch,K,Nc,1,M_ant]  s [batch,Nc,2*K] H_BR[1,Nc,M_ant,M_ant]\n",
    "        # param_list = [fc,B,Nc,M,N,D_sub,D_ant,R,M_ant,h1,h2,L,SNR]\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "        \n",
    "\n",
    "        x = self.DQL(out) - 0.5\n",
    "\n",
    "        x = x.contiguous().view(-1,int(K*B))\n",
    "        x = self.FC1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.mish1(x)\n",
    "\n",
    "        H = x.contiguous().view(-1,int(K),int(Nc),int(2*Nt))   #不完美信道\n",
    "        batch = H.shape[0]\n",
    "\n",
    "\n",
    "        '''\n",
    "        #H [batch,K,Nc,2*Nt]\n",
    "        x = self.linear_H(H)   #[batch,K,Nc,32]\n",
    "        x = x.permute(0,2,1,3) #[batch,Nc,K,32]\n",
    "        x = x.reshape(-1,Nc,K*32)#[batch,Nc,K*32]\n",
    "        \n",
    "        out_RF = x[:,:,32:64].reshape(-1,Nc*32)\n",
    "        RF_phase = self.linear_RF(out_RF)\n",
    "        F_RF = torch.exp(1j*RF_phase.reshape(-1,Nt,K))/sqrt(Nt) #[batch,1,M_ant,K]\n",
    "        '''\n",
    "        #H[batch,K,Nc,Nt]\n",
    "        H1 = torch.complex(H[:,:,:,0:Nt],H[:,:,:,:Nt:2*Nt])     #合并完成为复数矩阵\n",
    "        \n",
    "        F_RF = (torch.zeros(batch,Nt,K)+ 0j).cuda()\n",
    "        #不同用户的模拟预编码矩阵不同\n",
    "        for i in range(K):\n",
    "            F_RF[:,:,i] = H1[:,i,Nc//2,:]\n",
    "        F_RF = torch.real(F_RF) - 1j*torch.imag(F_RF)        #共轭转置\n",
    "        F_RF = (F_RF/torch.abs(F_RF))/sqrt(Nt)\n",
    "        \n",
    "        #print(H1.size())\n",
    "        #print(F_RF.size())\n",
    "        H_equ = (torch.zeros(batch,Nc,K,K) + 0j).cuda()\n",
    "        H1 = H1.reshape([Nc,batch,K,Nt])\n",
    "        H_equ = (H1 @ F_RF).reshape([batch,Nc,K,K])    #(batch,K,Nc,K)\n",
    "           \n",
    "        n = (torch.randn(H.shape[0],Nc,K) + 1j*torch.randn(H.shape[0],Nc,K)).cuda()/sqrt(2 * snr)   #噪声\n",
    "        \n",
    "        power = torch.sum(torch.abs(H_equ*H_equ),[2,3])\n",
    "        H_equ = H_equ/torch.sqrt(power.reshape(-1,Nc,1,1))\n",
    "        \n",
    "\n",
    "        H_equ = H_equ.reshape(-1,Nc,K*K)\n",
    "        x = torch.cat((torch.real(H_equ),torch.imag(H_equ)), 2) #[batch,Nc,2*K*K] 实数\n",
    "        x = self.trans2(x)   #[batch,Nc,4*K+1]\n",
    "\n",
    "        H_equ = H_equ.reshape(-1,Nc,K,K)\n",
    "\n",
    "        H_hat = H_equ.to(torch.complex128)\n",
    "        U = (x[:,:,0:K] + 1j*x[:,:,K:2*K]).to(torch.complex128)    #[batch,Nc,K]\n",
    "        W = (x[:,:,2*K:3*K] + 1j*x[:,:,3*K:4*K]).to(torch.complex128)   #[batch,Nc,K] 相当于是U、W的更新函数\n",
    "        # pub1 = (x[:,:,4*K:5*K] + 1j*x[:,:,5*K:6*K]).to(torch.complex128) #[batch,Nc,K]\n",
    "        # pub2 = (x[:,:,6*K:7*K] + 1j*x[:,:,7*K:8*K]).to(torch.complex128) #[batch,Nc,K]\n",
    "\n",
    "        #sigma_pri = (x[:,:,4*K]).to(torch.complex128)  #[batch,Nc]\n",
    "        # sigma_pub = (x[:,:,8*K+1]).to(torch.complex128)#[batch,Nc]\n",
    "\n",
    "        # 现在来更新V\n",
    "        max_iter = 100\n",
    "        iter1=0\n",
    "        min_mu = 0\n",
    "        max_mu = 10\n",
    "        P = 1  #发射功率\n",
    "        while 1:\n",
    "            mu = (min_mu + max_mu) / 2\n",
    "            \n",
    "            for k in range(K):\n",
    "                hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "                if(k==0):\n",
    "                    B_pri = W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                    B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "                else:\n",
    "                #二分法算mu\n",
    "\n",
    "                    B_pri = B_pri + W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + pri2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + (sigma_pri.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "                    B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = B_pub + pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "        \n",
    "            V_pri = (torch.zeros((batch,Nc,K,K)) + 0j).cuda().to(torch.complex128)\n",
    "        # V_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)\n",
    "        # B_inv_pub = torch.inverse(B_pub)\n",
    "            B_inv_pri = torch.inverse(B_pri)\n",
    "        # print(np.sum(np.abs(A_inv)**2))\n",
    "            for k in range(K):\n",
    "                hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            #V_pri[:,:,:,k] = (B_inv_pri @ (pri1[:,:,k].reshape(batch,Nc,1,1) * hk)).reshape(batch,Nc,K)\n",
    "                V_pri[:,:,:,k] = ((U[:,:,k].reshape(batch,Nc,1,1) * W[:,:,k].reshape(batch,Nc,1,1) * B_inv_pri) @ hk).reshape(batch,Nc,K)\n",
    "        # for k in range(K):\n",
    "        #     hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "        #     if k==0:\n",
    "        #         A = pub1[:,:,k].reshape(batch,Nc,1,1) * hk\n",
    "        #     else:\n",
    "        #         A = A + pub1[:,:,k].reshape(batch,Nc,1,1) * hk\n",
    "        # V_pub = (B_inv_pub @ A)\n",
    "            Pitem = torch.sum(torch.abs(V_pri*V_pri))  #\n",
    "            if Pitem> P:   #功率过大 mu在分母上\n",
    "                min_mu = mu\n",
    "            else:\n",
    "                max_mu = mu\n",
    "            iter1 = iter1 + 1\n",
    "            if ((max_mu - min_mu) < 1e-5 or iter1 > max_iter):\n",
    "                break\n",
    "            #找到mu\n",
    "        for k in range(K):\n",
    "            hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            if(k==0):\n",
    "                B_pri = W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "            else:\n",
    "                #二分法算mu\n",
    "\n",
    "                B_pri = B_pri + W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + pri2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + (sigma_pri.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "                B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = B_pub + pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "        \n",
    "        V_pri = (torch.zeros((batch,Nc,K,K)) + 0j).cuda().to(torch.complex128)\n",
    "        # V_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)\n",
    "        # B_inv_pub = torch.inverse(B_pub)\n",
    "        B_inv_pri = torch.inverse(B_pri)\n",
    "        # print(np.sum(np.abs(A_inv)**2))\n",
    "        for k in range(K):\n",
    "            hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            #V_pri[:,:,:,k] = (B_inv_pri @ (pri1[:,:,k].reshape(batch,Nc,1,1) * hk)).reshape(batch,Nc,K)\n",
    "            V_pri[:,:,:,k] = ((U[:,:,k].reshape(batch,Nc,1,1) * W[:,:,k].reshape(batch,Nc,1,1) * B_inv_pri) @ hk).reshape(batch,Nc,K)\n",
    "\n",
    "        w_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)  #没有public signal\n",
    "        W_pri = V_pri\n",
    "\n",
    "\n",
    "        F_BB_RS = (w_pub + 0).to(torch.complex64)\n",
    "\n",
    "        F_BB_SDMA = (W_pri + 0).to(torch.complex64)   #[batch,Nc,K,K]\n",
    "        F_BB_SDMA = F_BB_SDMA.reshape([Nc,batch,K,K]) # digital precoding\n",
    "        F_SDMA = (F_RF @ F_BB_SDMA).to(torch.complex64)  #[batch,Nc,M_ant,K]         [batch,Nt,K] * [Nc,batch,K,K] = [Nc,batch,Nt,K]广播机制\n",
    "        #F_RS = F_RF @ F_BB_RS  #[batch,Nc,M_ant,N_RS1]\n",
    "        F_SDMA = F_SDMA.reshape([batch,Nc,Nt,K])\n",
    "        Power = (torch.sum(torch.abs(F_SDMA)**2,[2,3])).reshape(-1,Nc,1,1)   #[batch,Nc,1,1]  [batch,Nc,Nt,K]\n",
    "        #Power1 = (torch.sum(torch.abs(F_SDMA)**2,[1,2,3])).reshape(-1,1,1,1)\n",
    "        #print(Power1.size())\n",
    "        #print(F_BB_SDMA.size())\n",
    "        F_BB_SDMA = F_BB_SDMA.permute(1,0,2,3)\n",
    "        #F_SDMA = F_SDMA / torch.sqrt(Power) * sqrt(K)\n",
    "        F_BB_SDMA = F_BB_SDMA / torch.sqrt(Power)  #F_BB做归一化\n",
    "        #F_BB_RS = F_BB_RS / torch.sqrt(Power) * sqrt(K)\n",
    "        #归一化之后的预编码矩阵\n",
    "        \n",
    "        F_BB_SDMA = F_BB_SDMA.permute(1,0,2,3)\n",
    "        F_SDMA = F_RF @ F_BB_SDMA\n",
    "        F_SDMA = F_SDMA.permute(1,0,2,3)\n",
    "        F_SDMA = F_SDMA.reshape([batch,K*Nc*Nt])\n",
    "\n",
    "    \n",
    "        #abs(A)**2\n",
    "        F_real = torch.real(F_SDMA).reshape(batch, K * Nt * Nc)\n",
    "        F_imag = torch.imag(F_SDMA).reshape(batch, K * Nt * Nc)\n",
    "        F = torch.cat((F_real, F_imag), 1)\n",
    "        return F  #Phi[batch,1,M_ant,M_ant]  F_RF[batch,1,M_ant,K]  F_BB_SDMA[batch,Nc,K,K] F_BB_RS[batch,Nc,K,1] \n",
    "#         print(H_RU.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2, 32, 128)\n",
      "========================\n",
      "lr:0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y\\AppData\\Local\\Temp\\ipykernel_19460\\230855352.py:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  out = integer.unsqueeze(-1) // 2 ** exponent_bits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 time 0:02:10.344344 train SE 3.326 test SE 6.235\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:02:07.559301 train SE 5.963 test SE 9.141\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:02:05.470381 train SE 7.470 test SE 10.668\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:02:06.925993 train SE 8.686 test SE 10.971\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:02:06.821785 train SE 9.743 test SE 11.776\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:02:06.445280 train SE 10.714 test SE 12.359\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:02:07.541349 train SE 11.555 test SE 12.733\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:02:06.248301 train SE 12.005 test SE 12.433\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:02:06.362994 train SE 12.100 test SE 11.824\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:02:06.290694 train SE 12.441 test SE 13.248\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:02:05.861373 train SE 13.089 test SE 12.880\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:02:06.306652 train SE 13.297 test SE 14.097\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:02:06.538540 train SE 14.277 test SE 13.933\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:02:05.794514 train SE 14.879 test SE 14.557\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:02:06.346544 train SE 15.099 test SE 15.026\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:02:06.821789 train SE 15.310 test SE 15.768\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:02:06.523588 train SE 15.445 test SE 14.302\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:02:05.942624 train SE 15.377 test SE 15.514\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:02:06.623313 train SE 15.250 test SE 15.351\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:02:07.443610 train SE 15.593 test SE 15.079\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:02:06.130124 train SE 15.147 test SE 13.729\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:02:06.354535 train SE 15.309 test SE 14.395\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:02:06.561968 train SE 15.480 test SE 14.615\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:02:07.357345 train SE 15.774 test SE 14.211\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:02:05.758119 train SE 15.776 test SE 15.425\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:02:05.327763 train SE 15.830 test SE 16.715\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:02:05.761151 train SE 16.172 test SE 14.981\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:02:06.495148 train SE 15.881 test SE 15.009\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:02:05.679333 train SE 15.919 test SE 17.060\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:02:05.851387 train SE 16.443 test SE 16.419\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:02:06.090231 train SE 16.185 test SE 14.268\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:02:06.837761 train SE 15.968 test SE 14.949\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:02:05.898237 train SE 16.256 test SE 15.130\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:02:05.953090 train SE 16.836 test SE 16.748\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:02:05.915203 train SE 16.110 test SE 16.718\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:02:06.439297 train SE 16.518 test SE 15.516\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:02:07.211724 train SE 16.635 test SE 16.608\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:02:05.580594 train SE 16.447 test SE 17.675\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:02:06.503660 train SE 16.647 test SE 16.540\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:02:05.584611 train SE 16.893 test SE 16.900\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:02:07.434658 train SE 17.427 test SE 16.696\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:02:05.455431 train SE 16.655 test SE 16.045\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:02:06.292195 train SE 17.370 test SE 16.382\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:02:06.267767 train SE 17.436 test SE 16.837\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:02:06.170036 train SE 16.616 test SE 14.856\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:02:05.994978 train SE 16.215 test SE 18.164\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:02:05.271459 train SE 17.248 test SE 16.943\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:02:06.470716 train SE 16.863 test SE 16.537\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:02:01.465096 train SE 16.786 test SE 17.812\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:02:06.104685 train SE 17.005 test SE 14.920\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:02:07.804644 train SE 17.294 test SE 18.177\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:02:06.862674 train SE 17.581 test SE 15.826\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:02:06.387944 train SE 17.570 test SE 17.361\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:02:05.679821 train SE 17.844 test SE 17.250\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:02:06.432328 train SE 17.929 test SE 16.852\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:02:05.995976 train SE 17.732 test SE 16.240\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:02:05.953595 train SE 17.874 test SE 19.626\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:02:06.950452 train SE 18.017 test SE 17.821\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:02:07.060634 train SE 18.243 test SE 17.397\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:02:08.106354 train SE 17.646 test SE 17.870\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:02:07.221204 train SE 18.038 test SE 15.826\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:02:05.899234 train SE 18.129 test SE 16.689\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:02:06.206413 train SE 17.611 test SE 15.276\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:02:06.792363 train SE 17.528 test SE 16.615\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:02:05.996481 train SE 17.298 test SE 18.789\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:02:11.509756 train SE 17.517 test SE 16.518\n",
      "========================\n",
      "lr:5.4952e-04\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\CODE_WQF\\WMMSE_diff_B.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 141>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39mfor\u001b[39;00m B \u001b[39min\u001b[39;00m Bs:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)\n",
      "\u001b[1;32md:\\CODE_WQF\\WMMSE_diff_B.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m optimizer_US\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m opt\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m optimizer_US\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch110\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch110\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch110\\lib\\site-packages\\torch\\autograd\\function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "\u001b[1;32md:\\CODE_WQF\\WMMSE_diff_B.ipynb Cell 7\u001b[0m in \u001b[0;36mDequantization.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(ctx, grad_output):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# return as many input gradients as there were arguments.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m#b, c = grad_output.shape\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m#grad_bit = grad_output.repeat(1, 1, ctx.constant)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     grad_bit \u001b[39m=\u001b[39m grad_output\u001b[39m.\u001b[39;49mrepeat_interleave(ctx\u001b[39m.\u001b[39;49mconstant,dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/WMMSE_diff_B.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_bit, \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def train(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):  # N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10)\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "\n",
    "    # H_train = torch.load('data/H_train_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_train = H_train[:, 0:K, :, :]\n",
    "    # print(H_train.shape)\n",
    "    # H_test = torch.load('data/H_test_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_test = H_test[:, 0:K, :, :]\n",
    "    # print(H_test.shape)\n",
    "    train = 'H_train_'+str(N)+'.mat'\n",
    "    mat = h5py.File(train)\n",
    "    H_train = mat['H_train']\n",
    "    H_train = np.transpose(H_train, [3, 2, 1, 0])\n",
    "    H_train = H_train.astype('float32')  # 训练变量类型转换\n",
    "    print(H_train.shape)\n",
    "    test = 'H_val_'+str(N)+'.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_val']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "    net_US = DNN_US_RF_OFDM(parm_set).cuda()\n",
    "    net_BS = RIS_SDMA_Precoding(parm_set).cuda()\n",
    "\n",
    "    optimizer_US = torch.optim.Adam(net_US.parameters(), lr=0.001)\n",
    "    scheduler_US = torch.optim.lr_scheduler.MultiStepLR(optimizer_US, milestones=[100, 150], gamma=0.6,\n",
    "                                                        last_epoch=-1)  # 训练过程中动态调整学习率\n",
    "    \n",
    "    opt = NoamOpt(256, 1, 4000, torch.optim.Adam(net_BS.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    #optimizer_BS = torch.optim.Adam(net_BS.parameters(), lr=0.001)\n",
    "    #scheduler_BS = torch.optim.lr_scheduler.MultiStepLR(optimizer_BS, milestones=[100, 150], gamma=0.6, last_epoch=-1)\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "    torch_dataset_train = DatasetFolder(H_train)\n",
    "    loader_train = Data.DataLoader(\n",
    "        dataset=torch_dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(EPOCH):\n",
    "        print('========================')\n",
    "        print('lr:%.4e' % opt.optimizer.param_groups[0]['lr'])\n",
    "        train_SE = 0\n",
    "        num_train = 0\n",
    "        test_SE = 0\n",
    "        num_test = 0\n",
    "        for step, b_x in enumerate(loader_train):\n",
    "            num_train = num_train + 1\n",
    "            net_BS.train()  # 训练模式\n",
    "            net_US.train()\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out1 = torch.zeros([num, B*K]) # 用户端K个用户量化比特输出，K个用户共用一个网络\n",
    "            out1 = out1.cuda()\n",
    "            #print(out1.dtype)\n",
    "            for i in range(K):\n",
    "                out1k = net_US(b_x[:,i,:,:],parm_set)   #信道是四维矩阵，pram_set为五维矩阵,PFN网络，生成B比特数据\n",
    "                #print(net_US(b_x[:,i,:,:],parm_set))\n",
    "                out1[:, i*B : (i * B + B)] = out1k\n",
    "            out2 = net_BS(out1, parm_set)\n",
    "            loss = loss_func1(b_x, out2, parm_set)\n",
    "            train_SE = train_SE - loss\n",
    "            optimizer_US.zero_grad()\n",
    "            opt.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_US.step()\n",
    "            opt.step()\n",
    "        train_SE = train_SE / num_train\n",
    "        scheduler_US.step()\n",
    "        # scheduler_BS.step()\n",
    "        net_BS.eval()\n",
    "        net_US.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, b_x in enumerate(loader_test):\n",
    "                num_test = num_test + 1\n",
    "                net_BS.eval()  # 验证模式\n",
    "                net_US.eval()\n",
    "                b_x = b_x.cuda()\n",
    "                num = b_x.shape[0]\n",
    "                out1 = torch.zeros([num, B*K]) # 用户端K个用户量化比特输出，K个用户共用一个网络\n",
    "                out1 = out1.cuda()\n",
    "            #print(out1.dtype)\n",
    "                for i in range(K):\n",
    "                    out1k = net_US(b_x[:,i,:,:],parm_set)   #信道是四维矩阵，pram_set为五维矩阵,PFN网络，生成B比特数据\n",
    "                #print(net_US(b_x[:,i,:,:],parm_set))\n",
    "                    out1[:, i*B : (i * B + B)] = out1k\n",
    "                out2 = net_BS(out1, parm_set)\n",
    "                loss = loss_func1(b_x, out2, parm_set)\n",
    "                test_SE = test_SE - loss\n",
    "            test_SE = test_SE / num_test\n",
    "\n",
    "        time0 = datetime.datetime.now() - start\n",
    "        print('Epoch:', epoch, 'time', time0, 'train SE %.3f' % train_SE.cpu(), 'test SE %.3f' % test_SE.cpu())\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        if test_SE > best_SE:\n",
    "            best_SE = test_SE\n",
    "            torch.save(net_US,\n",
    "                       './US_WMMSE_diff_' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "            torch.save(net_BS,\n",
    "                       './BS_WMMSE_diff_' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "            print('Model saved!')\n",
    "        shoulian[epoch] = test_SE.cpu()\n",
    "    print('The best SE is: %.3f' % best_SE.cpu())\n",
    "    print(shoulian)\n",
    "\n",
    "Nc = 32\n",
    "# N = 2  # 多径数\n",
    "Nt = 64 # 基站端天线数\n",
    "Nr = 1 # 用户端天线数\n",
    "# B = 30\n",
    "\n",
    "L = 8  # 用户端接收的观测，即OFDM个数\n",
    "SNR_dB = 10\n",
    "K = 2  # 用户数\n",
    "snr = 10**(SNR_dB/10)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 180\n",
    "\n",
    "#B = 40  # 反馈bit数\n",
    "#N_=[5,6]\n",
    "Bs = [10,20,30,40,50,60,70]\n",
    "N=6\n",
    "if __name__ == '__main__':\n",
    "    for B in Bs:\n",
    "        train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
