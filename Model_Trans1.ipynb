{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第二块GPU（从0开始）\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from IPython import display\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from matplotlib import cm\n",
    "from scipy.linalg import block_diag\n",
    "import datetime\n",
    "from torch.nn.utils import *\n",
    "from Transformer_model import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Hermitian(X):#torch矩阵共轭转置\n",
    "    X = torch.real(X) - 1j*torch.imag(X)\n",
    "    return X.transpose(-1,-2)\n",
    "\n",
    "\n",
    "\n",
    "class MyLoss_OFDM(torch.nn.Module):  # 输入是信道和整个F,输出是频谱效率\n",
    "    def __init__(self):\n",
    "        super(MyLoss_OFDM, self).__init__()\n",
    "\n",
    "    def forward(self, H0, out, parm_set):  # H0第0个维度是样本 第1个维度是用户，第2个维度是子载波，第3个维度是天线\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "\n",
    "        H = H0.permute(0, 2, 1, 3)  # H第0个维度是样本 第1个维度是子载波，第2个维度是用户，第3个维度是天线\n",
    "        num = out.shape[0]\n",
    "        Nc = H.shape[1]\n",
    "        H_real = H[:, :, :, 0:Nt]\n",
    "        H_imag = H[:, :, :, Nt:2 * Nt]\n",
    "        Hs = torch.zeros([num, Nc, K, Nt * 2])\n",
    "        Hs = Hs.cuda()\n",
    "        Hs[:, :, 0:K, 0:Nt] = H_real\n",
    "        # Hs[:, :, K:2 * K, Nt:2 * Nt] = H_real\n",
    "        Hs[:, :, 0:K, Nt:2 * Nt] = H_imag\n",
    "        # Hs[:, :, K:2 * K, 0:Nt] = -H_imag\n",
    "\n",
    "        F = torch.zeros([num, Nc, Nt * 2, K * 2])\n",
    "        F = F.cuda()\n",
    "        F[:, :, 0:Nt, 0:K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, K:2 * K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, 0:Nt, K:2 * K] = out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, 0:K] = -out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        R = 0\n",
    "        Hk = torch.matmul(Hs, F)\n",
    "        noise = 1 / snr\n",
    "        for i in range(K):\n",
    "            signal = Hk[:, :, i, i] * Hk[:, :, i, i] + Hk[:, :, i, i + K] * Hk[:, :, i, i + K]\n",
    "            interference = torch.zeros(num, Nc)\n",
    "            interference = interference.cuda()\n",
    "            for j in range(K):\n",
    "                if j != i:\n",
    "                    interference = interference + Hk[:, :, i, j] * Hk[:, :, i, j] + Hk[:, :, i, j + K] * Hk[:, :, i,\n",
    "                                                                                                         j + K]\n",
    "            SINR = signal / (noise + interference)\n",
    "            R = R + torch.sum(torch.log2(1 + SINR))\n",
    "        R = -R / num / Nc\n",
    "        return R\n",
    "    \n",
    "class DatasetFolder(Data.Dataset):\n",
    "    def __init__(self, matData):\n",
    "        self.matdata = matData\n",
    "    def __getitem__(self, index):\n",
    "        return self.matdata[index]\n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]\n",
    "\n",
    "class NoamOpt(object):\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "             min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIS_SDMA_Precoding(nn.Module): #单层\n",
    "    def __init__(self, parm_set): \n",
    "        super(RIS_SDMA_Precoding,self).__init__()\n",
    "        \n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "        \n",
    "        self.trans_block1 = TRANS_BLOCK(2*Nt*K,2*Nt*K,512,6)\n",
    "        self.trans_block2 = TRANS_BLOCK(2*Nt*Nc,Nt,128,3)\n",
    "        \n",
    "        #self.linear_H  = nn.Linear(Nt*2, 32)\n",
    "        \n",
    "        #self.trans1  = TRANS_BLOCK(32*K,32,256,3)\n",
    "        #self.linear_RIS = nn.Linear(Nc*32, Nt)   #生成RIS相位\n",
    "\n",
    "        #self.linear_RF = nn.Linear(Nc*32, Nt*K)   #生成RIS相位\n",
    "        \n",
    "        self.trans2  = TRANS_BLOCK(2*K*K,4*K+1,256,1) #4×4等效信道\n",
    "        \n",
    "    \n",
    "    def forward(self, H,parm_set):\n",
    "        # H(batch,K,Nc,Nt)\n",
    "        #H_RU [batch,K,Nc,1,M_ant]  s [batch,Nc,2*K] H_BR[1,Nc,M_ant,M_ant]\n",
    "        # param_list = [fc,B,Nc,M,N,D_sub,D_ant,R,M_ant,h1,h2,L,SNR]\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "        \n",
    "        batch = H.shape[0]\n",
    "        '''\n",
    "        h = H.permute(0,2,1,3)\n",
    "        x = h.contiguous().view(-1,int(Nc),int(2*Nt*K))\n",
    "\n",
    "        #H [batch,K,Nc,2*Nt]\n",
    "        x_ = self.trans_block1(x)\n",
    "        x_ = x.contiguous().view(-1, Nc, int(Nt*2), K)\n",
    "        x_ = x_.permute(0,3,1,2)\n",
    "        x_ = x_.reshape(-1,K,Nc*Nt*2)\n",
    "        x_ = self.trans_block2(x_)\n",
    "        x_ = x_.permute(0,2,1) #(-1,Nt,K)\n",
    "\n",
    "        F_RF = (torch.zeros(batch,Nt,K)+ 0j).cuda()\n",
    "\n",
    "        RF_real = torch.cos(x_).reshape(-1, Nt, K) / sqrt(Nt)\n",
    "        RF_imag = torch.sin(x_).reshape(-1, Nt, K) / sqrt(Nt)\n",
    "\n",
    "        F_RF = torch.complex(RF_real,RF_imag)\n",
    "        '''\n",
    "\n",
    "\n",
    "        #x = self.trans1(x)   #[batch,Nc,32]\n",
    "        #out_RIS = x.reshape(-1,Nc*32)\n",
    "        #Phi_phase = self.linear_RIS(out_RIS)\n",
    "        #Phi = torch.exp(1j*Phi_phase.reshape(-1,1,M_ant))\n",
    "        #Phi = torch.diag_embed(Phi)  #[batch,1,M_ant,M_ant]\n",
    "        '''\n",
    "        out_RF = x[:,:,32:64].reshape(-1,Nc*32)\n",
    "        RF_phase = self.linear_RF(out_RF)\n",
    "        F_RF = torch.exp(1j*RF_phase.reshape(-1,Nt,K))/sqrt(Nt) #[batch,1,M_ant,K]\n",
    "        '''\n",
    "        #H[batch,K,Nc,Nt]\n",
    "        H1 = (torch.zeros(batch,K,Nc,Nt)+ 0j).cuda()\n",
    "        H1 = torch.complex(H[:,:,:,0:Nt],H[:,:,:,:Nt:2*Nt])     #合并完成为复数矩阵\n",
    "        \n",
    "        F_RF = (torch.zeros(batch,Nt,K)+ 0j).cuda()\n",
    "        #不同用户的模拟预编码矩阵不同\n",
    "        for i in range(K):\n",
    "            F_RF[:,:,i] = H1[:,i,Nc//2,:]\n",
    "        F_RF = F_RF.reshape(batch,Nt,K)\n",
    "        F_RF = torch.real(F_RF) - 1j*torch.imag(F_RF)        #共轭转置\n",
    "        F_RF = (F_RF/torch.abs(F_RF))/sqrt(Nt)\n",
    "        \n",
    "        #print(H1.size())\n",
    "        #print(F_RF.size())\n",
    "        H_equ = (torch.zeros(batch,Nc,K,K) + 0j).cuda()\n",
    "        H1 = H1.permute(2,0,1,3)\n",
    "        #H1 = H1.reshape([Nc,batch,K,Nt])\n",
    "        H_equ = (H1 @ F_RF).permute(1,0,2,3)    #(batch,K,Nc,K)\n",
    "           \n",
    "        n = (torch.randn(H.shape[0],Nc,K) + 1j*torch.randn(H.shape[0],Nc,K)).cuda()/sqrt(2 * snr)   #噪声\n",
    "        \n",
    "        power = torch.sum(torch.abs(H_equ*H_equ),[2,3])\n",
    "        H_equ = H_equ/torch.sqrt(power.reshape(-1,Nc,1,1))*sqrt(K)\n",
    "        \n",
    "\n",
    "        H_equ1 = H_equ.reshape(-1,Nc,K*K)\n",
    "        x2 = torch.cat((torch.real(H_equ1),torch.imag(H_equ1)), 2) #[batch,Nc,2*K*K]\n",
    "        x3 = self.trans2(x2)   #[batch,Nc,4*K+1]\n",
    "    \n",
    "        H_hat = H_equ.to(torch.complex128)\n",
    "        pri1 = (x3[:,:,0:K] + 1j*x3[:,:,K:2*K]).to(torch.complex128)    #[batch,Nc,K]\n",
    "        pri2 = (x3[:,:,2*K:3*K] + 1j*x3[:,:,3*K:4*K]).to(torch.complex128)   #[batch,Nc,K]\n",
    "        # pub1 = (x[:,:,4*K:5*K] + 1j*x[:,:,5*K:6*K]).to(torch.complex128) #[batch,Nc,K]\n",
    "        # pub2 = (x[:,:,6*K:7*K] + 1j*x[:,:,7*K:8*K]).to(torch.complex128) #[batch,Nc,K]\n",
    "\n",
    "        sigma_pri = (x3[:,:,4*K]).to(torch.complex128)  #[batch,Nc]\n",
    "        # sigma_pub = (x[:,:,8*K+1]).to(torch.complex128)#[batch,Nc]\n",
    "\n",
    "\n",
    "        for k in range(K):\n",
    "            hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            if(k==0):\n",
    "                B_pri = pri2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                B_pri = B_pri + (sigma_pri.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "\n",
    "                # B_pub = pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "            else:\n",
    "                B_pri = B_pri + pri2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                B_pri = B_pri + (sigma_pri.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "\n",
    "                # B_pub = B_pub + pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "\n",
    "        V_pri = (torch.zeros((batch,Nc,K,K)) + 0j).cuda().to(torch.complex128)\n",
    "        # V_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)\n",
    "        # B_inv_pub = torch.inverse(B_pub)\n",
    "        B_inv_pri = torch.inverse(B_pri)\n",
    "        # print(np.sum(np.abs(A_inv)**2))\n",
    "        for k in range(K):\n",
    "            hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            V_pri[:,:,:,k] = (B_inv_pri @ (pri1[:,:,k].reshape(batch,Nc,1,1) * hk)).reshape(batch,Nc,K)\n",
    "        # for k in range(K):\n",
    "        #     hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "        #     if k==0:\n",
    "        #         A = pub1[:,:,k].reshape(batch,Nc,1,1) * hk\n",
    "        #     else:\n",
    "        #         A = A + pub1[:,:,k].reshape(batch,Nc,1,1) * hk\n",
    "        # V_pub = (B_inv_pub @ A)\n",
    "\n",
    "        w_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)  #没有public signal\n",
    "        W_pri = V_pri\n",
    "\n",
    "\n",
    "        F_BB_RS = (w_pub + 0).to(torch.complex64)\n",
    "\n",
    "        F_BB_SDMA = (W_pri + 0).to(torch.complex64)   #[batch,Nc,K,K]\n",
    "        F_BB_SDMA = F_BB_SDMA.reshape([Nc,batch,K,K]) # digital precoding\n",
    "        F_SDMA = (F_RF @ F_BB_SDMA).to(torch.complex64)  #[batch,Nc,M_ant,K]         [batch,Nt,K] * [Nc,batch,K,K] = [Nc,batch,Nt,K]广播机制\n",
    "        #F_RS = F_RF @ F_BB_RS  #[batch,Nc,M_ant,N_RS1]\n",
    "        F_SDMA = F_SDMA.reshape([batch,Nc,Nt,K])\n",
    "        Power = (torch.sum(torch.abs(F_SDMA)**2,[2,3])).reshape(-1,Nc,1,1)   #[batch,Nc,1,1]  [batch,Nc,Nt,K]\n",
    "        #Power1 = (torch.sum(torch.abs(F_SDMA)**2,[1,2,3])).reshape(-1,1,1,1)\n",
    "        #print(Power1.size())\n",
    "        #print(F_BB_SDMA.size())\n",
    "        F_BB_SDMA = F_BB_SDMA.permute(1,0,2,3)\n",
    "        #F_SDMA = F_SDMA / torch.sqrt(Power) * sqrt(K)\n",
    "        F_BB_SDMA = F_BB_SDMA / torch.sqrt(Power) * sqrt(K)  #F_BB做归一化 由于吴总保证SNR的原因要乘以sqrt(K)\n",
    "        #F_BB_RS = F_BB_RS / torch.sqrt(Power) * sqrt(K)\n",
    "        #归一化之后的预编码矩阵\n",
    "        \n",
    "        F_BB_SDMA = F_BB_SDMA.permute(1,0,2,3)\n",
    "        F_SDMA = F_RF @ F_BB_SDMA\n",
    "        F_SDMA = F_SDMA.permute(1,0,2,3)\n",
    "        F_SDMA = F_SDMA.reshape([batch,K*Nc*Nt])\n",
    "\n",
    "    \n",
    "        #abs(A)**2\n",
    "        F_real = torch.real(F_SDMA).reshape(batch, K * Nt * Nc)\n",
    "        F_imag = torch.imag(F_SDMA).reshape(batch, K * Nt * Nc)\n",
    "        F = torch.cat((F_real, F_imag), 1)\n",
    "        return F  #Phi[batch,1,M_ant,M_ant]  F_RF[batch,1,M_ant,K]  F_BB_SDMA[batch,Nc,K,K] F_BB_RS[batch,Nc,K,1] \n",
    "#         print(H_RU.shape)\n",
    "\n",
    "#正常做法都是F_RF除以Nt,就把他想到和码本一样，F_BB满足功率约束，看后面功率是多少定如何归一化\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):  # N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10) / K\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "\n",
    "    # H_train = torch.load('data/H_train_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_train = H_train[:, 0:K, :, :]\n",
    "    # print(H_train.shape)\n",
    "    # H_test = torch.load('data/H_test_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_test = H_test[:, 0:K, :, :]\n",
    "    # print(H_test.shape)\n",
    "    train = 'H_train_'+str(N)+'.mat'\n",
    "    mat = h5py.File(train)\n",
    "    H_train = mat['H_train']\n",
    "    H_train = np.transpose(H_train, [3, 2, 1, 0])\n",
    "    H_train = H_train.astype('float32')  # 训练变量类型转换\n",
    "    print(H_train.shape)\n",
    "    test = 'H_val_'+str(N)+'.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_val']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "    net_BS = RIS_SDMA_Precoding(parm_set).cuda()\n",
    "\n",
    "    print(net_BS)\n",
    "    opt = NoamOpt(256, 1, 4000, torch.optim.Adam(net_BS.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    #optimizer_BS = torch.optim.Adam(net_BS.parameters(), lr=0.001)\n",
    "    #scheduler_BS = torch.optim.lr_scheduler.MultiStepLR(optimizer_BS, milestones=[100, 150], gamma=0.6, last_epoch=-1)\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "    torch_dataset_train = DatasetFolder(H_train)\n",
    "    loader_train = Data.DataLoader(\n",
    "        dataset=torch_dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(EPOCH):\n",
    "        print('========================')\n",
    "        print('lr:%.4e' % opt.optimizer.param_groups[0]['lr'])\n",
    "        train_SE = 0\n",
    "        num_train = 0\n",
    "        test_SE = 0\n",
    "        num_test = 0\n",
    "        for step, b_x in enumerate(loader_train):\n",
    "            num_train = num_train + 1\n",
    "            net_BS.train()  # 训练模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out2 = net_BS(b_x, parm_set)\n",
    "            loss = loss_func1(b_x, out2, parm_set)\n",
    "            train_SE = train_SE - loss\n",
    "            opt.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        train_SE = train_SE / num_train\n",
    "        # scheduler_BS.step()\n",
    "        net_BS.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, b_x in enumerate(loader_test):\n",
    "                num_test = num_test + 1\n",
    "                net_BS.eval()  # 验证模式\n",
    "                b_x = b_x.cuda()\n",
    "                num = b_x.shape[0]\n",
    "                out2 = net_BS(b_x, parm_set)\n",
    "                loss = loss_func1(b_x, out2, parm_set)\n",
    "                test_SE = test_SE - loss\n",
    "            test_SE = test_SE / num_test\n",
    "\n",
    "        time0 = datetime.datetime.now() - start\n",
    "        print('Epoch:', epoch, 'time', time0, 'train SE %.3f' % train_SE.cpu(), 'test SE %.3f' % test_SE.cpu())\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        if test_SE > best_SE:\n",
    "            best_SE = test_SE\n",
    "            torch.save(net_BS,\n",
    "                       './Model_driven_test' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "            print('Model saved!')\n",
    "        shoulian[epoch] = test_SE.cpu()\n",
    "    print('The best SE is: %.3f' % best_SE.cpu())\n",
    "    print(shoulian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):  # N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10) / K\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "\n",
    "    # H_train = torch.load('data/H_train_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_train = H_train[:, 0:K, :, :]\n",
    "    # print(H_train.shape)\n",
    "    # H_test = torch.load('data/H_test_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_test = H_test[:, 0:K, :, :]\n",
    "    # print(H_test.shape)\n",
    "    test = 'H_test_'+str(N)+'.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_test']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "    H_test = H_test[:, 0:K, :, :]\n",
    "\n",
    "    net_BS = RIS_SDMA_Precoding(parm_set).cuda()\n",
    "\n",
    "    print(net_BS)\n",
    "\n",
    "    net_BS = torch.load(\n",
    "        'Model_driven_test' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    test_SE = 0\n",
    "    num_test = 0\n",
    "    net_BS.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, b_x in enumerate(loader_test):\n",
    "            num_test = num_test + 1\n",
    "            net_BS.eval()  # 验证模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out2 = net_BS(b_x,parm_set)\n",
    "            loss = loss_func1(b_x, out2, parm_set)\n",
    "            test_SE = test_SE - loss\n",
    "        test_SE = test_SE / num_test\n",
    "    time0 = datetime.datetime.now() - start\n",
    "    print('Epoch:', 'test SE %.3f' % test_SE.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIS_SDMA_Precoding(\n",
      "  (trans_block1): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=256, out_features=512, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (trans_block2): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (trans2): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=8, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch: test SE 19.081\n"
     ]
    }
   ],
   "source": [
    "Nc = 32\n",
    "# N = 2  # 多径数\n",
    "Nt = 64 # 基站端天线数\n",
    "Nr = 1 # 用户端天线数\n",
    "# B = 30\n",
    "\n",
    "L = 8  # 用户端接收的观测，即OFDM个数\n",
    "SNR_dB = 10\n",
    "K = 2  # 用户数\n",
    "snr = 10**(SNR_dB/10)/K\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 200\n",
    "\n",
    "B = 40  # 反馈bit数\n",
    "#N_=[5,6]\n",
    "N=6\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
