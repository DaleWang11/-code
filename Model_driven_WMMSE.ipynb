{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Driven-WMMSE-Algotirhm(U W study by Transformer, mu bisearch),Ncl=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch110\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第二块GPU（从0开始）\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from IPython import display\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from matplotlib import cm\n",
    "from scipy.linalg import block_diag\n",
    "import datetime\n",
    "from torch.nn.utils import *\n",
    "from Transformer_model import *\n",
    "import h5py\n",
    "from modelDesign import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hermitian(X):#torch矩阵共轭转置\n",
    "    X = torch.real(X) - 1j*torch.imag(X)\n",
    "    return X.transpose(-1,-2)\n",
    "\n",
    "class MyLoss_OFDM(torch.nn.Module):  # 输入是信道和整个F,输出是频谱效率\n",
    "    def __init__(self):\n",
    "        super(MyLoss_OFDM, self).__init__()\n",
    "\n",
    "    def forward(self, H0, out, parm_set):  # H0第0个维度是样本 第1个维度是用户，第2个维度是子载波，第3个维度是天线\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "\n",
    "        H = H0.permute(0, 2, 1, 3)  # H第0个维度是样本 第1个维度是子载波，第2个维度是用户，第3个维度是天线\n",
    "        num = out.shape[0]\n",
    "        Nc = H.shape[1]\n",
    "        H_real = H[:, :, :, 0:Nt]\n",
    "        H_imag = H[:, :, :, Nt:2 * Nt]\n",
    "        Hs = torch.zeros([num, Nc, K, Nt * 2])\n",
    "        Hs = Hs.cuda()\n",
    "        Hs[:, :, 0:K, 0:Nt] = H_real\n",
    "        # Hs[:, :, K:2 * K, Nt:2 * Nt] = H_real\n",
    "        Hs[:, :, 0:K, Nt:2 * Nt] = H_imag\n",
    "        # Hs[:, :, K:2 * K, 0:Nt] = -H_imag\n",
    "\n",
    "        F = torch.zeros([num, Nc, Nt * 2, K * 2])\n",
    "        F = F.cuda()\n",
    "        F[:, :, 0:Nt, 0:K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, K:2 * K] = out[:, 0:K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, 0:Nt, K:2 * K] = out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        F[:, :, Nt:2 * Nt, 0:K] = -out[:, K * Nt * Nc:2 * K * Nt * Nc].reshape(num, Nc, Nt, K)\n",
    "        R = 0\n",
    "        Hk = torch.matmul(Hs, F)\n",
    "        noise = 1 / snr\n",
    "        for i in range(K):\n",
    "            signal = Hk[:, :, i, i] * Hk[:, :, i, i] + Hk[:, :, i, i + K] * Hk[:, :, i, i + K]\n",
    "            interference = torch.zeros(num, Nc)\n",
    "            interference = interference.cuda()\n",
    "            for j in range(K):\n",
    "                if j != i:\n",
    "                    interference = interference + Hk[:, :, i, j] * Hk[:, :, i, j] + Hk[:, :, i, j + K] * Hk[:, :, i,\n",
    "                                                                                                         j + K]\n",
    "            SINR = signal / (noise + interference)\n",
    "            R = R + torch.sum(torch.log2(1 + SINR))\n",
    "        R = -R / num / Nc\n",
    "        return R\n",
    "    \n",
    "class DatasetFolder(Data.Dataset):\n",
    "    def __init__(self, matData):\n",
    "        self.matdata = matData\n",
    "    def __getitem__(self, index):\n",
    "        return self.matdata[index]\n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]\n",
    "\n",
    "class NoamOpt(object):\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "             min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIS_SDMA_Precoding(nn.Module): #单层\n",
    "    def __init__(self, parm_set): \n",
    "        super(RIS_SDMA_Precoding,self).__init__()\n",
    "        \n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "        \n",
    "        \n",
    "        self.linear_H  = nn.Linear(Nt*2, 32)\n",
    "        \n",
    "        self.trans1  = TRANS_BLOCK(32*K,32,256,3)\n",
    "        self.linear_RIS = nn.Linear(Nc*32, Nt)   #生成RIS相位\n",
    "\n",
    "        self.linear_RF = nn.Linear(Nc*32, Nt*K)   #生成RIS相位\n",
    "        \n",
    "        self.trans2  = TRANS_BLOCK(2*K*K,4*K,256,3) #4×4等效信道\n",
    "        \n",
    "    \n",
    "    def forward(self, H,parm_set):\n",
    "        # H(batch,K,Nc,Nt)\n",
    "        #H_RU [batch,K,Nc,1,M_ant]  s [batch,Nc,2*K] H_BR[1,Nc,M_ant,M_ant]\n",
    "        # param_list = [fc,B,Nc,M,N,D_sub,D_ant,R,M_ant,h1,h2,L,SNR]\n",
    "        Nc = parm_set[0]\n",
    "        Nt = parm_set[1]\n",
    "        Nr = parm_set[2]\n",
    "        snr = parm_set[3]\n",
    "        B = parm_set[4]\n",
    "        K = parm_set[5]\n",
    "        batch = H.shape[0]\n",
    "        '''\n",
    "        #H [batch,K,Nc,2*Nt]\n",
    "        x = self.linear_H(H)   #[batch,K,Nc,32]\n",
    "        x = x.permute(0,2,1,3) #[batch,Nc,K,32]\n",
    "        x = x.reshape(-1,Nc,K*32)#[batch,Nc,K*32]\n",
    "        \n",
    "        out_RF = x[:,:,32:64].reshape(-1,Nc*32)\n",
    "        RF_phase = self.linear_RF(out_RF)\n",
    "        F_RF = torch.exp(1j*RF_phase.reshape(-1,Nt,K))/sqrt(Nt) #[batch,1,M_ant,K]\n",
    "        '''\n",
    "        #H[batch,K,Nc,Nt]\n",
    "        H1 = torch.complex(H[:,:,:,0:Nt],H[:,:,:,:Nt:2*Nt])     #合并完成为复数矩阵\n",
    "        \n",
    "        F_RF = (torch.zeros(batch,Nt,K)+ 0j).cuda()\n",
    "        #不同用户的模拟预编码矩阵不同\n",
    "        for i in range(K):\n",
    "            F_RF[:,:,i] = H1[:,i,Nc//2,:]\n",
    "        F_RF = torch.real(F_RF) - 1j*torch.imag(F_RF)        #共轭转置\n",
    "        F_RF = (F_RF/torch.abs(F_RF))/sqrt(Nt)\n",
    "        \n",
    "        #print(H1.size())\n",
    "        #print(F_RF.size())\n",
    "        H_equ = (torch.zeros(batch,Nc,K,K) + 0j).cuda()\n",
    "        H1 = H1.reshape([Nc,batch,K,Nt])\n",
    "        H_equ = (H1 @ F_RF).reshape([batch,Nc,K,K])    #(batch,K,Nc,K)\n",
    "           \n",
    "        n = (torch.randn(H.shape[0],Nc,K) + 1j*torch.randn(H.shape[0],Nc,K)).cuda()/sqrt(2 * snr)   #噪声\n",
    "        \n",
    "        power = torch.sum(torch.abs(H_equ*H_equ),[2,3])\n",
    "        H_equ = H_equ/torch.sqrt(power.reshape(-1,Nc,1,1))\n",
    "        \n",
    "\n",
    "        H_equ = H_equ.reshape(-1,Nc,K*K)\n",
    "        x = torch.cat((torch.real(H_equ),torch.imag(H_equ)), 2) #[batch,Nc,2*K*K] 实数\n",
    "        x = self.trans2(x)   #[batch,Nc,4*K+1]\n",
    "\n",
    "        H_equ = H_equ.reshape(-1,Nc,K,K)\n",
    "\n",
    "        H_hat = H_equ.to(torch.complex128)\n",
    "        U = (x[:,:,0:K] + 1j*x[:,:,K:2*K]).to(torch.complex128)    #[batch,Nc,K]\n",
    "        W = (x[:,:,2*K:3*K] + 1j*x[:,:,3*K:4*K]).to(torch.complex128)   #[batch,Nc,K] 相当于是U、W的更新函数\n",
    "        # pub1 = (x[:,:,4*K:5*K] + 1j*x[:,:,5*K:6*K]).to(torch.complex128) #[batch,Nc,K]\n",
    "        # pub2 = (x[:,:,6*K:7*K] + 1j*x[:,:,7*K:8*K]).to(torch.complex128) #[batch,Nc,K]\n",
    "\n",
    "        #sigma_pri = (x[:,:,4*K]).to(torch.complex128)  #[batch,Nc]\n",
    "        # sigma_pub = (x[:,:,8*K+1]).to(torch.complex128)#[batch,Nc]\n",
    "\n",
    "        # 现在来更新V\n",
    "        max_iter = 100\n",
    "        iter1=0\n",
    "        min_mu = 0\n",
    "        max_mu = 10\n",
    "        P = 1  #发射功率\n",
    "        while 1:\n",
    "            mu = (min_mu + max_mu) / 2\n",
    "            \n",
    "            for k in range(K):\n",
    "                hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "                if(k==0):\n",
    "                    B_pri = W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                    B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "                else:\n",
    "                #二分法算mu\n",
    "\n",
    "                    B_pri = B_pri + W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + pri2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + (sigma_pri.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "                    B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = B_pub + pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "        \n",
    "            V_pri = (torch.zeros((batch,Nc,K,K)) + 0j).cuda().to(torch.complex128)\n",
    "        # V_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)\n",
    "        # B_inv_pub = torch.inverse(B_pub)\n",
    "            B_inv_pri = torch.inverse(B_pri)\n",
    "        # print(np.sum(np.abs(A_inv)**2))\n",
    "            for k in range(K):\n",
    "                hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            #V_pri[:,:,:,k] = (B_inv_pri @ (pri1[:,:,k].reshape(batch,Nc,1,1) * hk)).reshape(batch,Nc,K)\n",
    "                V_pri[:,:,:,k] = ((U[:,:,k].reshape(batch,Nc,1,1) * W[:,:,k].reshape(batch,Nc,1,1) * B_inv_pri) @ hk).reshape(batch,Nc,K)\n",
    "        # for k in range(K):\n",
    "        #     hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "        #     if k==0:\n",
    "        #         A = pub1[:,:,k].reshape(batch,Nc,1,1) * hk\n",
    "        #     else:\n",
    "        #         A = A + pub1[:,:,k].reshape(batch,Nc,1,1) * hk\n",
    "        # V_pub = (B_inv_pub @ A)\n",
    "            Pitem = torch.sum(torch.abs(V_pri*V_pri))  #\n",
    "            if Pitem> P:   #功率过大 mu在分母上\n",
    "                min_mu = mu\n",
    "            else:\n",
    "                max_mu = mu\n",
    "            iter1 = iter1 + 1\n",
    "            if ((max_mu - min_mu) < 1e-5 or iter1 > max_iter):\n",
    "                break\n",
    "            #找到mu\n",
    "        for k in range(K):\n",
    "            hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            if(k==0):\n",
    "                B_pri = W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "            else:\n",
    "                #二分法算mu\n",
    "\n",
    "                B_pri = B_pri + W[:,:,k].reshape(batch,Nc,1,1) * torch.abs(U[:,:,k].reshape(batch,Nc,1,1))**2 * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + pri2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                #B_pri = B_pri + (sigma_pri.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "                B_pri = B_pri + (mu *torch.eye(K).cuda().to(torch.complex128))\n",
    "                # B_pub = B_pub + pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\n",
    "                # B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\n",
    "        \n",
    "        V_pri = (torch.zeros((batch,Nc,K,K)) + 0j).cuda().to(torch.complex128)\n",
    "        # V_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)\n",
    "        # B_inv_pub = torch.inverse(B_pub)\n",
    "        B_inv_pri = torch.inverse(B_pri)\n",
    "        # print(np.sum(np.abs(A_inv)**2))\n",
    "        for k in range(K):\n",
    "            hk = Hermitian(H_hat[:,:,k,:].reshape(batch,Nc,1,K))\n",
    "            #V_pri[:,:,:,k] = (B_inv_pri @ (pri1[:,:,k].reshape(batch,Nc,1,1) * hk)).reshape(batch,Nc,K)\n",
    "            V_pri[:,:,:,k] = ((U[:,:,k].reshape(batch,Nc,1,1) * W[:,:,k].reshape(batch,Nc,1,1) * B_inv_pri) @ hk).reshape(batch,Nc,K)\n",
    "\n",
    "        w_pub = (torch.zeros((batch,Nc,K,1)) + 0j).cuda().to(torch.complex128)  #没有public signal\n",
    "        W_pri = V_pri\n",
    "\n",
    "\n",
    "        F_BB_RS = (w_pub + 0).to(torch.complex64)\n",
    "\n",
    "        F_BB_SDMA = (W_pri + 0).to(torch.complex64)   #[batch,Nc,K,K]\n",
    "        F_BB_SDMA = F_BB_SDMA.reshape([Nc,batch,K,K]) # digital precoding\n",
    "        F_SDMA = (F_RF @ F_BB_SDMA).to(torch.complex64)  #[batch,Nc,M_ant,K]         [batch,Nt,K] * [Nc,batch,K,K] = [Nc,batch,Nt,K]广播机制\n",
    "        #F_RS = F_RF @ F_BB_RS  #[batch,Nc,M_ant,N_RS1]\n",
    "        F_SDMA = F_SDMA.reshape([batch,Nc,Nt,K])\n",
    "        Power = (torch.sum(torch.abs(F_SDMA)**2,[2,3])).reshape(-1,Nc,1,1)   #[batch,Nc,1,1]  [batch,Nc,Nt,K]\n",
    "        #Power1 = (torch.sum(torch.abs(F_SDMA)**2,[1,2,3])).reshape(-1,1,1,1)\n",
    "        #print(Power1.size())\n",
    "        #print(F_BB_SDMA.size())\n",
    "        F_BB_SDMA = F_BB_SDMA.permute(1,0,2,3)\n",
    "        #F_SDMA = F_SDMA / torch.sqrt(Power) * sqrt(K)\n",
    "        F_BB_SDMA = F_BB_SDMA / torch.sqrt(Power)  #F_BB做归一化\n",
    "        #F_BB_RS = F_BB_RS / torch.sqrt(Power) * sqrt(K)\n",
    "        #归一化之后的预编码矩阵\n",
    "        \n",
    "        F_BB_SDMA = F_BB_SDMA.permute(1,0,2,3)\n",
    "        F_SDMA = F_RF @ F_BB_SDMA\n",
    "        F_SDMA = F_SDMA.permute(1,0,2,3)\n",
    "        F_SDMA = F_SDMA.reshape([batch,K*Nc*Nt])\n",
    "\n",
    "    \n",
    "        #abs(A)**2\n",
    "        F_real = torch.real(F_SDMA).reshape(batch, K * Nt * Nc)\n",
    "        F_imag = torch.imag(F_SDMA).reshape(batch, K * Nt * Nc)\n",
    "        F = torch.cat((F_real, F_imag), 1)\n",
    "        return F  #Phi[batch,1,M_ant,M_ant]  F_RF[batch,1,M_ant,K]  F_BB_SDMA[batch,Nc,K,K] F_BB_RS[batch,Nc,K,1] \n",
    "#         print(H_RU.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2, 32, 128)\n",
      "RIS_SDMA_Precoding(\n",
      "  (linear_H): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (trans1): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=64, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_RIS): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (linear_RF): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (trans2): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=8, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n",
      "Epoch: 0 time 0:00:59.241541 train SE 4.157 test SE 6.610\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:00:57.511560 train SE 6.776 test SE 9.507\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:00:58.078657 train SE 7.858 test SE 10.031\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:00:58.073159 train SE 8.544 test SE 10.303\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:00:57.984412 train SE 9.214 test SE 10.823\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:00:57.914038 train SE 9.715 test SE 10.293\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:00:56.779738 train SE 9.901 test SE 10.056\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:00:55.342973 train SE 9.984 test SE 10.993\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:00:55.974694 train SE 10.026 test SE 10.679\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:00:56.679395 train SE 10.183 test SE 10.391\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:00:57.338848 train SE 10.073 test SE 9.553\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:00:56.918104 train SE 10.413 test SE 11.568\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:00:57.656783 train SE 10.632 test SE 11.361\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:00:57.547212 train SE 10.693 test SE 11.518\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:00:57.979856 train SE 10.789 test SE 10.037\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:00:57.608980 train SE 10.930 test SE 11.568\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:00:57.592011 train SE 11.017 test SE 10.550\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:00:57.346607 train SE 10.919 test SE 10.871\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:00:57.477564 train SE 11.147 test SE 11.810\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:00:57.791418 train SE 11.207 test SE 10.824\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:00:57.925081 train SE 11.149 test SE 11.777\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:00:55.720431 train SE 11.236 test SE 11.921\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:00:55.358321 train SE 11.399 test SE 11.161\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:00:54.990914 train SE 11.356 test SE 12.238\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:00:55.454666 train SE 11.486 test SE 12.297\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:00:55.329998 train SE 11.680 test SE 11.797\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:00:55.204347 train SE 11.841 test SE 10.890\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:00:55.411785 train SE 11.826 test SE 13.041\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:00:55.484592 train SE 11.957 test SE 13.190\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:00:55.299084 train SE 12.204 test SE 12.688\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:00:55.096623 train SE 12.318 test SE 12.625\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:00:55.147489 train SE 12.467 test SE 12.632\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:00:55.368904 train SE 12.146 test SE 12.424\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:00:55.592297 train SE 12.343 test SE 12.835\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:00:55.250677 train SE 12.496 test SE 11.062\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:00:55.123559 train SE 12.498 test SE 13.127\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:00:55.429735 train SE 12.727 test SE 12.811\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:00:55.545907 train SE 12.707 test SE 11.886\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:00:54.847296 train SE 12.900 test SE 12.642\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:00:55.124549 train SE 12.682 test SE 12.906\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:00:55.304487 train SE 12.780 test SE 12.738\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:00:55.273159 train SE 12.556 test SE 12.017\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:00:55.172423 train SE 12.522 test SE 11.485\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:00:55.218307 train SE 12.959 test SE 11.728\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:00:55.096624 train SE 12.778 test SE 11.228\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:00:55.218308 train SE 12.427 test SE 13.020\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:00:55.214313 train SE 12.840 test SE 13.630\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:00:55.170428 train SE 12.961 test SE 13.390\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:00:55.355939 train SE 12.727 test SE 13.002\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:00:55.000880 train SE 12.819 test SE 14.518\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:00:55.463649 train SE 12.704 test SE 12.957\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:00:55.067700 train SE 12.699 test SE 12.012\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:00:55.277141 train SE 12.823 test SE 14.304\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:00:55.514872 train SE 12.819 test SE 12.176\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:00:55.476614 train SE 12.606 test SE 13.518\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:00:55.281132 train SE 13.051 test SE 13.220\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:00:55.251220 train SE 13.035 test SE 14.719\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:00:55.191373 train SE 13.225 test SE 14.094\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:00:54.953338 train SE 13.207 test SE 12.050\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:00:55.454681 train SE 13.264 test SE 13.551\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:00:55.454668 train SE 13.143 test SE 14.809\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:00:54.784896 train SE 13.074 test SE 12.821\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:00:55.005870 train SE 13.442 test SE 14.331\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:00:54.604938 train SE 13.045 test SE 13.414\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:00:55.044764 train SE 13.176 test SE 14.296\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:00:55.046759 train SE 13.436 test SE 13.492\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:00:55.414775 train SE 13.370 test SE 13.656\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:00:55.128545 train SE 13.446 test SE 14.225\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:00:54.798429 train SE 13.306 test SE 13.384\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:00:54.705067 train SE 13.414 test SE 11.987\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:00:54.827740 train SE 13.443 test SE 14.383\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:00:54.695701 train SE 13.489 test SE 14.390\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:00:54.761932 train SE 13.387 test SE 13.745\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:00:55.095636 train SE 13.620 test SE 12.813\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:00:54.841008 train SE 13.969 test SE 14.319\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:00:54.703163 train SE 13.596 test SE 13.080\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:00:54.833333 train SE 13.550 test SE 14.217\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:00:54.817814 train SE 13.044 test SE 11.887\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:00:54.551095 train SE 13.096 test SE 12.641\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:00:54.705678 train SE 13.682 test SE 14.284\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:00:54.555970 train SE 13.715 test SE 14.088\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:00:54.863642 train SE 13.319 test SE 12.505\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:00:54.997216 train SE 13.917 test SE 13.268\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:00:55.015680 train SE 13.624 test SE 14.345\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:00:54.742315 train SE 13.587 test SE 13.805\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:00:54.607428 train SE 13.945 test SE 14.540\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:00:54.824788 train SE 14.142 test SE 13.502\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:00:55.027816 train SE 13.923 test SE 13.173\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:00:55.004881 train SE 14.054 test SE 14.699\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:00:54.607933 train SE 14.178 test SE 15.272\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:00:54.852280 train SE 13.838 test SE 13.153\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:00:54.636861 train SE 13.856 test SE 14.186\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:00:55.041772 train SE 13.754 test SE 13.301\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:00:54.795429 train SE 13.902 test SE 14.129\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:00:55.341950 train SE 14.327 test SE 14.480\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:00:54.978597 train SE 14.140 test SE 13.820\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:00:54.800820 train SE 14.120 test SE 14.818\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:00:54.601347 train SE 14.088 test SE 14.113\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:00:54.744551 train SE 14.072 test SE 14.242\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:00:55.043780 train SE 14.010 test SE 15.070\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:00:54.892184 train SE 14.020 test SE 14.036\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:00:54.401813 train SE 14.020 test SE 12.709\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:00:54.902461 train SE 13.916 test SE 14.277\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:00:54.883749 train SE 14.160 test SE 14.000\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:00:54.668781 train SE 13.647 test SE 14.713\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:00:55.034258 train SE 14.445 test SE 14.740\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:00:54.847299 train SE 14.146 test SE 14.161\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:00:54.650443 train SE 14.141 test SE 15.812\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:00:54.888593 train SE 14.216 test SE 14.053\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:00:54.718026 train SE 14.524 test SE 15.958\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:00:54.793282 train SE 14.467 test SE 14.880\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:00:54.817840 train SE 14.497 test SE 15.916\n",
      "========================\n",
      "lr:4.2184e-04\n",
      "Epoch: 112 time 0:00:54.631323 train SE 14.242 test SE 14.153\n",
      "========================\n",
      "lr:4.1996e-04\n",
      "Epoch: 113 time 0:00:54.685728 train SE 14.382 test SE 13.918\n",
      "========================\n",
      "lr:4.1812e-04\n",
      "Epoch: 114 time 0:00:54.804409 train SE 14.291 test SE 14.323\n",
      "========================\n",
      "lr:4.1630e-04\n",
      "Epoch: 115 time 0:00:54.950026 train SE 14.770 test SE 15.042\n",
      "========================\n",
      "lr:4.1450e-04\n",
      "Epoch: 116 time 0:00:54.733605 train SE 14.535 test SE 14.862\n",
      "========================\n",
      "lr:4.1272e-04\n",
      "Epoch: 117 time 0:00:55.060127 train SE 14.204 test SE 13.517\n",
      "========================\n",
      "lr:4.1097e-04\n",
      "Epoch: 118 time 0:00:54.947035 train SE 14.487 test SE 15.215\n",
      "========================\n",
      "lr:4.0924e-04\n",
      "Epoch: 119 time 0:00:54.676760 train SE 14.466 test SE 12.705\n",
      "========================\n",
      "lr:4.0753e-04\n",
      "Epoch: 120 time 0:00:54.718642 train SE 14.639 test SE 15.576\n",
      "========================\n",
      "lr:4.0584e-04\n",
      "Epoch: 121 time 0:00:54.887186 train SE 14.519 test SE 13.137\n",
      "========================\n",
      "lr:4.0418e-04\n",
      "Epoch: 122 time 0:00:55.497957 train SE 14.591 test SE 15.111\n",
      "========================\n",
      "lr:4.0253e-04\n",
      "Epoch: 123 time 0:00:55.016848 train SE 14.508 test SE 13.054\n",
      "========================\n",
      "lr:4.0090e-04\n",
      "Epoch: 124 time 0:00:55.013856 train SE 14.705 test SE 15.883\n",
      "========================\n",
      "lr:3.9930e-04\n",
      "Epoch: 125 time 0:00:54.765509 train SE 14.662 test SE 14.584\n",
      "========================\n",
      "lr:3.9771e-04\n",
      "Epoch: 126 time 0:00:55.014206 train SE 14.621 test SE 14.373\n",
      "========================\n",
      "lr:3.9614e-04\n",
      "Epoch: 127 time 0:00:55.090645 train SE 14.977 test SE 13.665\n",
      "========================\n",
      "lr:3.9459e-04\n",
      "Epoch: 128 time 0:00:54.996894 train SE 14.618 test SE 14.135\n",
      "========================\n",
      "lr:3.9306e-04\n",
      "Epoch: 129 time 0:00:54.582495 train SE 14.574 test SE 14.960\n",
      "========================\n",
      "lr:3.9154e-04\n",
      "Epoch: 130 time 0:00:54.976943 train SE 14.144 test SE 15.805\n",
      "========================\n",
      "lr:3.9005e-04\n",
      "Epoch: 131 time 0:00:54.622890 train SE 14.914 test SE 15.926\n",
      "========================\n",
      "lr:3.8857e-04\n",
      "Epoch: 132 time 0:00:54.708442 train SE 14.842 test SE 14.108\n",
      "========================\n",
      "lr:3.8710e-04\n",
      "Epoch: 133 time 0:00:54.789356 train SE 14.682 test SE 15.792\n",
      "========================\n",
      "lr:3.8566e-04\n",
      "Epoch: 134 time 0:00:55.280582 train SE 14.754 test SE 15.229\n",
      "========================\n",
      "lr:3.8422e-04\n",
      "Epoch: 135 time 0:00:54.650259 train SE 14.792 test SE 14.714\n",
      "========================\n",
      "lr:3.8281e-04\n",
      "Epoch: 136 time 0:00:54.800427 train SE 14.135 test SE 13.952\n",
      "========================\n",
      "lr:3.8141e-04\n",
      "Epoch: 137 time 0:00:54.610931 train SE 14.450 test SE 14.517\n",
      "========================\n",
      "lr:3.8003e-04\n",
      "Epoch: 138 time 0:00:54.751447 train SE 14.690 test SE 15.402\n",
      "========================\n",
      "lr:3.7866e-04\n",
      "Epoch: 139 time 0:00:55.165895 train SE 14.406 test SE 14.713\n",
      "========================\n",
      "lr:3.7730e-04\n",
      "Epoch: 140 time 0:00:54.929388 train SE 14.967 test SE 14.585\n",
      "========================\n",
      "lr:3.7596e-04\n",
      "Epoch: 141 time 0:00:55.046756 train SE 14.543 test SE 14.210\n",
      "========================\n",
      "lr:3.7463e-04\n",
      "Epoch: 142 time 0:00:54.621904 train SE 14.504 test SE 12.901\n",
      "========================\n",
      "lr:3.7332e-04\n",
      "Epoch: 143 time 0:00:54.636869 train SE 14.885 test SE 15.074\n",
      "========================\n",
      "lr:3.7202e-04\n",
      "Epoch: 144 time 0:00:54.706676 train SE 14.878 test SE 13.533\n",
      "========================\n",
      "lr:3.7074e-04\n",
      "Epoch: 145 time 0:00:55.171423 train SE 14.772 test SE 15.296\n",
      "========================\n",
      "lr:3.6947e-04\n",
      "Epoch: 146 time 0:00:54.791889 train SE 14.946 test SE 16.859\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.6821e-04\n",
      "Epoch: 147 time 0:00:54.922465 train SE 14.548 test SE 13.797\n",
      "========================\n",
      "lr:3.6696e-04\n",
      "Epoch: 148 time 0:00:54.744943 train SE 14.961 test SE 15.113\n",
      "========================\n",
      "lr:3.6573e-04\n",
      "Epoch: 149 time 0:00:54.875225 train SE 14.854 test SE 15.502\n",
      "========================\n",
      "lr:3.6451e-04\n",
      "Epoch: 150 time 0:00:54.798733 train SE 14.497 test SE 14.311\n",
      "========================\n",
      "lr:3.6330e-04\n",
      "Epoch: 151 time 0:00:54.874218 train SE 14.427 test SE 13.623\n",
      "========================\n",
      "lr:3.6210e-04\n",
      "Epoch: 152 time 0:00:54.840173 train SE 14.568 test SE 15.056\n",
      "========================\n",
      "lr:3.6092e-04\n",
      "Epoch: 153 time 0:00:54.839314 train SE 14.582 test SE 14.573\n",
      "========================\n",
      "lr:3.5974e-04\n",
      "Epoch: 154 time 0:00:54.488714 train SE 14.432 test SE 14.142\n",
      "========================\n",
      "lr:3.5858e-04\n",
      "Epoch: 155 time 0:00:54.775491 train SE 14.646 test SE 14.251\n",
      "========================\n",
      "lr:3.5743e-04\n",
      "Epoch: 156 time 0:00:54.843306 train SE 15.085 test SE 15.423\n",
      "========================\n",
      "lr:3.5629e-04\n",
      "Epoch: 157 time 0:00:54.781206 train SE 14.869 test SE 13.809\n",
      "========================\n",
      "lr:3.5516e-04\n",
      "Epoch: 158 time 0:00:54.661791 train SE 14.651 test SE 13.677\n",
      "========================\n",
      "lr:3.5404e-04\n",
      "Epoch: 159 time 0:00:54.908136 train SE 14.884 test SE 15.985\n",
      "========================\n",
      "lr:3.5293e-04\n",
      "Epoch: 160 time 0:00:54.826840 train SE 14.964 test SE 15.845\n",
      "========================\n",
      "lr:3.5184e-04\n",
      "Epoch: 161 time 0:00:54.653808 train SE 14.881 test SE 15.141\n",
      "========================\n",
      "lr:3.5075e-04\n",
      "Epoch: 162 time 0:00:55.126760 train SE 15.343 test SE 15.275\n",
      "========================\n",
      "lr:3.4967e-04\n",
      "Epoch: 163 time 0:00:55.372892 train SE 15.021 test SE 15.433\n",
      "========================\n",
      "lr:3.4860e-04\n",
      "Epoch: 164 time 0:00:54.928076 train SE 14.978 test SE 14.481\n",
      "========================\n",
      "lr:3.4754e-04\n",
      "Epoch: 165 time 0:00:54.357932 train SE 15.184 test SE 16.024\n",
      "========================\n",
      "lr:3.4650e-04\n",
      "Epoch: 166 time 0:00:54.975307 train SE 14.819 test SE 14.947\n",
      "========================\n",
      "lr:3.4546e-04\n",
      "Epoch: 167 time 0:00:54.783438 train SE 14.833 test SE 15.205\n",
      "========================\n",
      "lr:3.4443e-04\n",
      "Epoch: 168 time 0:00:55.061616 train SE 14.668 test SE 15.785\n",
      "========================\n",
      "lr:3.4341e-04\n",
      "Epoch: 169 time 0:00:54.502702 train SE 14.705 test SE 14.845\n",
      "========================\n",
      "lr:3.4240e-04\n",
      "Epoch: 170 time 0:00:54.690712 train SE 14.703 test SE 14.477\n",
      "========================\n",
      "lr:3.4139e-04\n",
      "Epoch: 171 time 0:00:55.311492 train SE 14.999 test SE 15.285\n",
      "========================\n",
      "lr:3.4040e-04\n",
      "Epoch: 172 time 0:00:54.590978 train SE 14.913 test SE 15.977\n",
      "========================\n",
      "lr:3.3941e-04\n",
      "Epoch: 173 time 0:00:54.831455 train SE 15.119 test SE 15.982\n",
      "========================\n",
      "lr:3.3844e-04\n",
      "Epoch: 174 time 0:00:54.561230 train SE 15.458 test SE 15.210\n",
      "========================\n",
      "lr:3.3747e-04\n",
      "Epoch: 175 time 0:00:54.520929 train SE 15.051 test SE 13.207\n",
      "========================\n",
      "lr:3.3651e-04\n",
      "Epoch: 176 time 0:00:54.847302 train SE 14.986 test SE 14.493\n",
      "========================\n",
      "lr:3.3556e-04\n",
      "Epoch: 177 time 0:00:54.655166 train SE 15.380 test SE 15.347\n",
      "========================\n",
      "lr:3.3461e-04\n",
      "Epoch: 178 time 0:00:54.848292 train SE 15.334 test SE 14.757\n",
      "========================\n",
      "lr:3.3368e-04\n",
      "Epoch: 179 time 0:00:55.211316 train SE 15.004 test SE 15.219\n",
      "The best SE is: 16.859\n",
      "[ 6.60955381  9.50717831 10.03119469 10.3027792  10.82326889 10.29268932\n",
      " 10.05584908 10.99252605 10.6793642  10.39050961  9.55343151 11.56837082\n",
      " 11.36115074 11.5181303  10.03724384 11.56797504 10.55048561 10.87072563\n",
      " 11.81023407 10.82433128 11.77718258 11.92050076 11.16077328 12.23763657\n",
      " 12.29714298 11.79706478 10.8902483  13.04107189 13.19046688 12.68789577\n",
      " 12.62488461 12.63231945 12.42370224 12.83483791 11.06247425 13.12680817\n",
      " 12.81078815 11.88577843 12.6420536  12.90642738 12.73801231 12.01704597\n",
      " 11.48504925 11.72830486 11.22773647 13.01995087 13.63000774 13.38995838\n",
      " 13.0017395  14.51800823 12.95721149 12.01177502 14.30402565 12.17608452\n",
      " 13.51750469 13.21964931 14.7191515  14.09403419 12.04952717 13.55091095\n",
      " 14.80916309 12.82113552 14.33145809 13.413764   14.29616451 13.49169827\n",
      " 13.65567684 14.22547436 13.38358498 11.98736668 14.38347816 14.39041042\n",
      " 13.74525928 12.81344032 14.31920815 13.08044338 14.21664429 11.88715744\n",
      " 12.64102077 14.2837162  14.088274   12.50507259 13.26808071 14.34520435\n",
      " 13.80473995 14.53974628 13.50187206 13.17268276 14.69913006 15.27170849\n",
      " 13.15264034 14.18612671 13.30123901 14.12887478 14.47953796 13.82002735\n",
      " 14.81764507 14.11342049 14.24210644 15.06991577 14.03559875 12.70939732\n",
      " 14.27659512 14.00025463 14.71302795 14.74027252 14.16084194 15.81208706\n",
      " 14.05282879 15.95769787 14.88019562 15.91572762 14.1527853  13.91823292\n",
      " 14.32330322 15.04211426 14.8618288  13.51726723 15.21531963 12.70491505\n",
      " 15.57599163 13.13697243 15.11134815 13.05362415 15.88259888 14.58418179\n",
      " 14.37308979 13.66523743 14.13490582 14.95983601 15.80532551 15.9264164\n",
      " 14.10788631 15.79189491 15.22922707 14.71435452 13.9522934  14.5172987\n",
      " 15.40219879 14.71256542 14.5852356  14.20987415 12.9008503  15.07390785\n",
      " 13.53347969 15.2960453  16.85868835 13.79723644 15.11293221 15.50203419\n",
      " 14.31109619 13.62282276 15.05550671 14.57339191 14.14175892 14.25071144\n",
      " 15.42257214 13.80905437 13.6765976  15.98532391 15.84492779 15.14053249\n",
      " 15.27524281 15.43336773 14.48077583 16.02385712 14.94674873 15.20495033\n",
      " 15.78483486 14.84508801 14.47701931 15.28492641 15.97691059 15.98181343\n",
      " 15.20959759 13.20685005 14.49348736 15.34673214 14.75736713 15.21890736]\n"
     ]
    }
   ],
   "source": [
    "def train(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):  # N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10)\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "\n",
    "    # H_train = torch.load('data/H_train_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_train = H_train[:, 0:K, :, :]\n",
    "    # print(H_train.shape)\n",
    "    # H_test = torch.load('data/H_test_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_test = H_test[:, 0:K, :, :]\n",
    "    # print(H_test.shape)\n",
    "    train = 'H_train_'+str(N)+'.mat'\n",
    "    mat = h5py.File(train)\n",
    "    H_train = mat['H_train']\n",
    "    H_train = np.transpose(H_train, [3, 2, 1, 0])\n",
    "    H_train = H_train.astype('float32')  # 训练变量类型转换\n",
    "    print(H_train.shape)\n",
    "    test = 'H_val_'+str(N)+'.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_val']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "    net_BS = RIS_SDMA_Precoding(parm_set).cuda()\n",
    "\n",
    "    print(net_BS)\n",
    "    opt = NoamOpt(256, 1, 4000, torch.optim.Adam(net_BS.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    #optimizer_BS = torch.optim.Adam(net_BS.parameters(), lr=0.001)\n",
    "    #scheduler_BS = torch.optim.lr_scheduler.MultiStepLR(optimizer_BS, milestones=[100, 150], gamma=0.6, last_epoch=-1)\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "    torch_dataset_train = DatasetFolder(H_train)\n",
    "    loader_train = Data.DataLoader(\n",
    "        dataset=torch_dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(EPOCH):\n",
    "        print('========================')\n",
    "        print('lr:%.4e' % opt.optimizer.param_groups[0]['lr'])\n",
    "        train_SE = 0\n",
    "        num_train = 0\n",
    "        test_SE = 0\n",
    "        num_test = 0\n",
    "        for step, b_x in enumerate(loader_train):\n",
    "            num_train = num_train + 1\n",
    "            net_BS.train()  # 训练模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out2 = net_BS(b_x, parm_set)\n",
    "            loss = loss_func1(b_x, out2, parm_set)\n",
    "            train_SE = train_SE - loss\n",
    "            opt.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        train_SE = train_SE / num_train\n",
    "        # scheduler_BS.step()\n",
    "        net_BS.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, b_x in enumerate(loader_test):\n",
    "                num_test = num_test + 1\n",
    "                net_BS.eval()  # 验证模式\n",
    "                b_x = b_x.cuda()\n",
    "                num = b_x.shape[0]\n",
    "                out2 = net_BS(b_x, parm_set)\n",
    "                loss = loss_func1(b_x, out2, parm_set)\n",
    "                test_SE = test_SE - loss\n",
    "            test_SE = test_SE / num_test\n",
    "\n",
    "        time0 = datetime.datetime.now() - start\n",
    "        print('Epoch:', epoch, 'time', time0, 'train SE %.3f' % train_SE.cpu(), 'test SE %.3f' % test_SE.cpu())\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        if test_SE > best_SE:\n",
    "            best_SE = test_SE\n",
    "            torch.save(net_BS,\n",
    "                       './Model_driven_WMMSE' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "            print('Model saved!')\n",
    "        shoulian[epoch] = test_SE.cpu()\n",
    "    print('The best SE is: %.3f' % best_SE.cpu())\n",
    "    print(shoulian)\n",
    "'''\n",
    "Nc = 32\n",
    "# N = 2  # 多径数\n",
    "Nt = 64 # 基站端天线数\n",
    "Nr = 1 # 用户端天线数\n",
    "# B = 30\n",
    "\n",
    "L = 8  # 用户端接收的观测，即OFDM个数\n",
    "SNR_dB = 10\n",
    "K = 2  # 用户数\n",
    "snr = 10**(SNR_dB/10)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 180\n",
    "\n",
    "B = 40  # 反馈bit数\n",
    "#N_=[5,6]\n",
    "N=1\n",
    "if __name__ == '__main__':\n",
    "    train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIS_SDMA_Precoding(\n",
      "  (linear_H): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (trans1): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=64, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_RIS): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (linear_RF): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (trans2): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=8, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch: test SE 16.434\n"
     ]
    }
   ],
   "source": [
    "def test(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):  # N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10) / K\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "\n",
    "    # H_train = torch.load('data/H_train_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_train = H_train[:, 0:K, :, :]\n",
    "    # print(H_train.shape)\n",
    "    # H_test = torch.load('data/H_test_UPA' + str(N) + 'Lp.pt')\n",
    "    # H_test = H_test[:, 0:K, :, :]\n",
    "    # print(H_test.shape)\n",
    "    test = 'H_test_'+str(N)+'.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_test']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "    H_test = H_test[:, 0:K, :, :]\n",
    "\n",
    "    net_BS = RIS_SDMA_Precoding(parm_set).cuda()\n",
    "\n",
    "    print(net_BS)\n",
    "\n",
    "    net_BS = torch.load(\n",
    "        'Model_driven_WMMSE' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    test_SE = 0\n",
    "    num_test = 0\n",
    "    net_BS.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, b_x in enumerate(loader_test):\n",
    "            num_test = num_test + 1\n",
    "            net_BS.eval()  # 验证模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out2 = net_BS(b_x,parm_set)\n",
    "            loss = loss_func1(b_x, out2, parm_set)\n",
    "            test_SE = test_SE - loss\n",
    "        test_SE = test_SE / num_test\n",
    "    time0 = datetime.datetime.now() - start\n",
    "    print('Epoch:', 'test SE %.3f' % test_SE.cpu())\n",
    "'''\n",
    "Nc = 32\n",
    "#N = 2  # 多径数\n",
    "Nt = 64 # 基站端天线数\n",
    "Nr = 1 # 用户端天线数\n",
    "# B = 30\n",
    "\n",
    "L = 8  # 用户端接收的观测，即OFDM个数\n",
    "SNR_dB = 10\n",
    "K = 2  # 用户数\n",
    "snr =  10**(SNR_dB/10)/K\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 180\n",
    "\n",
    "B = 40  # 反馈bit数\n",
    "N=1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2, 32, 128)\n",
      "RIS_SDMA_Precoding(\n",
      "  (linear_H): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (trans1): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=64, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_RIS): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (linear_RF): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (trans2): TRANS_BLOCK(\n",
      "    (src_embedding): Embedding(\n",
      "      (linear): Linear(in_features=8, out_features=256, bias=True)\n",
      "    )\n",
      "    (src_pos_embedding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (trans_encoder): Trans_Encoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (sublayers): ModuleList(\n",
      "            (0): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): SublayerLogic(\n",
      "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (multi_headed_attention): MultiHeadedAttention(\n",
      "            (qkv_nets): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (out_projection_net): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (pointwise_net): PositionwiseFeedForwardNet(\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=256, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "========================\n",
      "lr:0.0000e+00\n",
      "Epoch: 0 time 0:01:59.125859 train SE 4.170 test SE 6.581\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8422e-05\n",
      "Epoch: 1 time 0:02:00.162113 train SE 6.554 test SE 9.612\n",
      "Model saved!\n",
      "========================\n",
      "lr:9.6845e-05\n",
      "Epoch: 2 time 0:02:00.634852 train SE 7.553 test SE 10.081\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.4527e-04\n",
      "Epoch: 3 time 0:02:00.399983 train SE 8.254 test SE 10.137\n",
      "Model saved!\n",
      "========================\n",
      "lr:1.9369e-04\n",
      "Epoch: 4 time 0:02:00.449332 train SE 8.916 test SE 10.766\n",
      "Model saved!\n",
      "========================\n",
      "lr:2.4211e-04\n",
      "Epoch: 5 time 0:02:00.755541 train SE 9.461 test SE 10.756\n",
      "========================\n",
      "lr:2.9053e-04\n",
      "Epoch: 6 time 0:02:00.979413 train SE 9.787 test SE 11.325\n",
      "Model saved!\n",
      "========================\n",
      "lr:3.3896e-04\n",
      "Epoch: 7 time 0:01:58.155960 train SE 9.755 test SE 10.075\n",
      "========================\n",
      "lr:3.8738e-04\n",
      "Epoch: 8 time 0:01:54.808421 train SE 9.662 test SE 9.593\n",
      "========================\n",
      "lr:4.3580e-04\n",
      "Epoch: 9 time 0:01:54.953021 train SE 9.530 test SE 9.449\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 10 time 0:01:54.019518 train SE 9.681 test SE 10.055\n",
      "========================\n",
      "lr:5.3265e-04\n",
      "Epoch: 11 time 0:01:54.663794 train SE 9.930 test SE 9.518\n",
      "========================\n",
      "lr:5.8107e-04\n",
      "Epoch: 12 time 0:01:55.125559 train SE 10.006 test SE 10.135\n",
      "========================\n",
      "lr:6.2949e-04\n",
      "Epoch: 13 time 0:01:56.334318 train SE 10.320 test SE 9.794\n",
      "========================\n",
      "lr:6.7791e-04\n",
      "Epoch: 14 time 0:01:54.310245 train SE 10.353 test SE 10.673\n",
      "========================\n",
      "lr:7.2634e-04\n",
      "Epoch: 15 time 0:01:54.463823 train SE 10.255 test SE 10.857\n",
      "========================\n",
      "lr:7.7476e-04\n",
      "Epoch: 16 time 0:01:54.762571 train SE 10.295 test SE 12.007\n",
      "Model saved!\n",
      "========================\n",
      "lr:8.2318e-04\n",
      "Epoch: 17 time 0:01:54.801425 train SE 10.467 test SE 10.854\n",
      "========================\n",
      "lr:8.7160e-04\n",
      "Epoch: 18 time 0:01:54.765534 train SE 10.496 test SE 10.019\n",
      "========================\n",
      "lr:9.2003e-04\n",
      "Epoch: 19 time 0:01:54.011034 train SE 10.527 test SE 10.077\n",
      "========================\n",
      "lr:9.6845e-04\n",
      "Epoch: 20 time 0:01:55.134030 train SE 10.539 test SE 10.102\n",
      "========================\n",
      "lr:9.7419e-04\n",
      "Epoch: 21 time 0:01:53.692405 train SE 10.321 test SE 11.170\n",
      "========================\n",
      "lr:9.5179e-04\n",
      "Epoch: 22 time 0:01:56.104939 train SE 10.723 test SE 11.925\n",
      "========================\n",
      "lr:9.3087e-04\n",
      "Epoch: 23 time 0:01:54.935574 train SE 10.627 test SE 10.494\n",
      "========================\n",
      "lr:9.1127e-04\n",
      "Epoch: 24 time 0:01:55.573372 train SE 10.652 test SE 11.136\n",
      "========================\n",
      "lr:8.9286e-04\n",
      "Epoch: 25 time 0:01:53.960170 train SE 10.907 test SE 11.431\n",
      "========================\n",
      "lr:8.7552e-04\n",
      "Epoch: 26 time 0:01:54.426922 train SE 10.883 test SE 10.198\n",
      "========================\n",
      "lr:8.5915e-04\n",
      "Epoch: 27 time 0:01:54.480778 train SE 11.097 test SE 11.844\n",
      "========================\n",
      "lr:8.4367e-04\n",
      "Epoch: 28 time 0:01:55.320038 train SE 10.785 test SE 11.119\n",
      "========================\n",
      "lr:8.2900e-04\n",
      "Epoch: 29 time 0:01:54.157150 train SE 11.047 test SE 11.103\n",
      "========================\n",
      "lr:8.1506e-04\n",
      "Epoch: 30 time 0:01:54.430419 train SE 11.115 test SE 10.966\n",
      "========================\n",
      "lr:8.0181e-04\n",
      "Epoch: 31 time 0:01:55.866072 train SE 11.165 test SE 9.734\n",
      "========================\n",
      "lr:7.8918e-04\n",
      "Epoch: 32 time 0:01:56.318368 train SE 11.190 test SE 11.437\n",
      "========================\n",
      "lr:7.7713e-04\n",
      "Epoch: 33 time 0:01:56.855425 train SE 11.532 test SE 10.952\n",
      "========================\n",
      "lr:7.6562e-04\n",
      "Epoch: 34 time 0:02:00.599422 train SE 11.634 test SE 12.643\n",
      "Model saved!\n",
      "========================\n",
      "lr:7.5460e-04\n",
      "Epoch: 35 time 0:02:01.475577 train SE 11.451 test SE 12.205\n",
      "========================\n",
      "lr:7.4405e-04\n",
      "Epoch: 36 time 0:02:02.170232 train SE 11.442 test SE 11.989\n",
      "========================\n",
      "lr:7.3392e-04\n",
      "Epoch: 37 time 0:02:01.175869 train SE 11.480 test SE 11.517\n",
      "========================\n",
      "lr:7.2420e-04\n",
      "Epoch: 38 time 0:02:01.204791 train SE 11.558 test SE 12.558\n",
      "========================\n",
      "lr:7.1486e-04\n",
      "Epoch: 39 time 0:02:01.841090 train SE 11.622 test SE 12.180\n",
      "========================\n",
      "lr:7.0587e-04\n",
      "Epoch: 40 time 0:01:58.517487 train SE 11.636 test SE 12.223\n",
      "========================\n",
      "lr:6.9720e-04\n",
      "Epoch: 41 time 0:01:53.596144 train SE 11.825 test SE 11.737\n",
      "========================\n",
      "lr:6.8885e-04\n",
      "Epoch: 42 time 0:01:54.783473 train SE 11.641 test SE 12.632\n",
      "========================\n",
      "lr:6.8080e-04\n",
      "Epoch: 43 time 0:01:55.790783 train SE 12.043 test SE 12.268\n",
      "========================\n",
      "lr:6.7302e-04\n",
      "Epoch: 44 time 0:01:53.877900 train SE 11.823 test SE 11.489\n",
      "========================\n",
      "lr:6.6550e-04\n",
      "Epoch: 45 time 0:01:54.870241 train SE 12.003 test SE 12.930\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.5822e-04\n",
      "Epoch: 46 time 0:01:55.103618 train SE 12.127 test SE 12.338\n",
      "========================\n",
      "lr:6.5118e-04\n",
      "Epoch: 47 time 0:01:55.306569 train SE 12.111 test SE 11.815\n",
      "========================\n",
      "lr:6.4436e-04\n",
      "Epoch: 48 time 0:01:55.282633 train SE 11.908 test SE 12.047\n",
      "========================\n",
      "lr:6.3776e-04\n",
      "Epoch: 49 time 0:01:54.996903 train SE 11.997 test SE 10.964\n",
      "========================\n",
      "lr:6.3135e-04\n",
      "Epoch: 50 time 0:01:54.549125 train SE 11.925 test SE 13.491\n",
      "Model saved!\n",
      "========================\n",
      "lr:6.2513e-04\n",
      "Epoch: 51 time 0:01:54.110276 train SE 11.946 test SE 12.218\n",
      "========================\n",
      "lr:6.1909e-04\n",
      "Epoch: 52 time 0:01:54.733609 train SE 12.046 test SE 12.931\n",
      "========================\n",
      "lr:6.1322e-04\n",
      "Epoch: 53 time 0:01:55.601795 train SE 12.376 test SE 12.197\n",
      "========================\n",
      "lr:6.0751e-04\n",
      "Epoch: 54 time 0:01:54.258373 train SE 11.899 test SE 10.510\n",
      "========================\n",
      "lr:6.0196e-04\n",
      "Epoch: 55 time 0:01:54.895175 train SE 12.102 test SE 12.648\n",
      "========================\n",
      "lr:5.9657e-04\n",
      "Epoch: 56 time 0:01:55.583850 train SE 12.203 test SE 13.269\n",
      "========================\n",
      "lr:5.9131e-04\n",
      "Epoch: 57 time 0:01:54.885718 train SE 12.667 test SE 11.373\n",
      "========================\n",
      "lr:5.8619e-04\n",
      "Epoch: 58 time 0:01:54.721639 train SE 12.674 test SE 12.307\n",
      "========================\n",
      "lr:5.8120e-04\n",
      "Epoch: 59 time 0:01:54.715655 train SE 12.382 test SE 12.243\n",
      "========================\n",
      "lr:5.7634e-04\n",
      "Epoch: 60 time 0:01:55.341491 train SE 12.662 test SE 12.874\n",
      "========================\n",
      "lr:5.7159e-04\n",
      "Epoch: 61 time 0:01:55.128046 train SE 12.495 test SE 12.886\n",
      "========================\n",
      "lr:5.6696e-04\n",
      "Epoch: 62 time 0:01:55.598295 train SE 12.575 test SE 13.802\n",
      "Model saved!\n",
      "========================\n",
      "lr:5.6245e-04\n",
      "Epoch: 63 time 0:01:55.765858 train SE 12.452 test SE 12.987\n",
      "========================\n",
      "lr:5.5804e-04\n",
      "Epoch: 64 time 0:01:55.129548 train SE 12.517 test SE 12.726\n",
      "========================\n",
      "lr:5.5373e-04\n",
      "Epoch: 65 time 0:01:54.039967 train SE 12.346 test SE 12.433\n",
      "========================\n",
      "lr:5.4952e-04\n",
      "Epoch: 66 time 0:01:54.363597 train SE 12.486 test SE 13.597\n",
      "========================\n",
      "lr:5.4540e-04\n",
      "Epoch: 67 time 0:01:53.577194 train SE 12.647 test SE 12.566\n",
      "========================\n",
      "lr:5.4137e-04\n",
      "Epoch: 68 time 0:01:54.460844 train SE 12.532 test SE 13.367\n",
      "========================\n",
      "lr:5.3744e-04\n",
      "Epoch: 69 time 0:01:54.187593 train SE 12.453 test SE 12.897\n",
      "========================\n",
      "lr:5.3358e-04\n",
      "Epoch: 70 time 0:01:53.814072 train SE 12.742 test SE 12.988\n",
      "========================\n",
      "lr:5.2981e-04\n",
      "Epoch: 71 time 0:01:54.665309 train SE 12.666 test SE 12.784\n",
      "========================\n",
      "lr:5.2612e-04\n",
      "Epoch: 72 time 0:01:55.754371 train SE 12.754 test SE 11.524\n",
      "========================\n",
      "lr:5.2251e-04\n",
      "Epoch: 73 time 0:01:55.063234 train SE 12.409 test SE 12.779\n",
      "========================\n",
      "lr:5.1896e-04\n",
      "Epoch: 74 time 0:01:53.998597 train SE 12.417 test SE 13.071\n",
      "========================\n",
      "lr:5.1549e-04\n",
      "Epoch: 75 time 0:01:54.600457 train SE 12.655 test SE 13.595\n",
      "========================\n",
      "lr:5.1209e-04\n",
      "Epoch: 76 time 0:01:54.723129 train SE 12.842 test SE 13.219\n",
      "========================\n",
      "lr:5.0875e-04\n",
      "Epoch: 77 time 0:01:54.137707 train SE 12.209 test SE 13.399\n",
      "========================\n",
      "lr:5.0548e-04\n",
      "Epoch: 78 time 0:01:55.267181 train SE 12.427 test SE 12.670\n",
      "========================\n",
      "lr:5.0227e-04\n",
      "Epoch: 79 time 0:01:54.190554 train SE 12.797 test SE 12.232\n",
      "========================\n",
      "lr:4.9912e-04\n",
      "Epoch: 80 time 0:01:55.191875 train SE 12.744 test SE 11.004\n",
      "========================\n",
      "lr:4.9603e-04\n",
      "Epoch: 81 time 0:01:54.215529 train SE 12.773 test SE 12.188\n",
      "========================\n",
      "lr:4.9300e-04\n",
      "Epoch: 82 time 0:01:54.454366 train SE 12.899 test SE 13.501\n",
      "========================\n",
      "lr:4.9002e-04\n",
      "Epoch: 83 time 0:01:54.297790 train SE 12.693 test SE 13.967\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.8709e-04\n",
      "Epoch: 84 time 0:01:54.065428 train SE 12.941 test SE 13.567\n",
      "========================\n",
      "lr:4.8422e-04\n",
      "Epoch: 85 time 0:01:54.067882 train SE 12.836 test SE 13.426\n",
      "========================\n",
      "lr:4.8140e-04\n",
      "Epoch: 86 time 0:01:55.209357 train SE 12.974 test SE 12.666\n",
      "========================\n",
      "lr:4.7862e-04\n",
      "Epoch: 87 time 0:01:54.581508 train SE 12.911 test SE 11.346\n",
      "========================\n",
      "lr:4.7589e-04\n",
      "Epoch: 88 time 0:01:54.654333 train SE 12.854 test SE 12.324\n",
      "========================\n",
      "lr:4.7321e-04\n",
      "Epoch: 89 time 0:01:54.411468 train SE 12.834 test SE 13.219\n",
      "========================\n",
      "lr:4.7058e-04\n",
      "Epoch: 90 time 0:01:54.303795 train SE 12.853 test SE 13.414\n",
      "========================\n",
      "lr:4.6798e-04\n",
      "Epoch: 91 time 0:01:54.766026 train SE 13.269 test SE 12.781\n",
      "========================\n",
      "lr:4.6543e-04\n",
      "Epoch: 92 time 0:01:47.752774 train SE 12.806 test SE 12.981\n",
      "========================\n",
      "lr:4.6292e-04\n",
      "Epoch: 93 time 0:01:54.889684 train SE 12.867 test SE 13.505\n",
      "========================\n",
      "lr:4.6046e-04\n",
      "Epoch: 94 time 0:01:55.098126 train SE 12.771 test SE 12.194\n",
      "========================\n",
      "lr:4.5803e-04\n",
      "Epoch: 95 time 0:01:55.580359 train SE 13.000 test SE 13.830\n",
      "========================\n",
      "lr:4.5563e-04\n",
      "Epoch: 96 time 0:01:54.281312 train SE 13.339 test SE 14.005\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.5328e-04\n",
      "Epoch: 97 time 0:01:55.151001 train SE 12.805 test SE 11.751\n",
      "========================\n",
      "lr:4.5096e-04\n",
      "Epoch: 98 time 0:01:54.307242 train SE 12.852 test SE 12.331\n",
      "========================\n",
      "lr:4.4868e-04\n",
      "Epoch: 99 time 0:01:54.217002 train SE 12.984 test SE 13.632\n",
      "========================\n",
      "lr:4.4643e-04\n",
      "Epoch: 100 time 0:01:55.042276 train SE 13.078 test SE 13.164\n",
      "========================\n",
      "lr:4.4421e-04\n",
      "Epoch: 101 time 0:01:54.366084 train SE 13.018 test SE 13.358\n",
      "========================\n",
      "lr:4.4203e-04\n",
      "Epoch: 102 time 0:01:54.657840 train SE 13.367 test SE 14.342\n",
      "Model saved!\n",
      "========================\n",
      "lr:4.3988e-04\n",
      "Epoch: 103 time 0:01:55.767842 train SE 13.135 test SE 13.893\n",
      "========================\n",
      "lr:4.3776e-04\n",
      "Epoch: 104 time 0:01:54.644857 train SE 12.461 test SE 13.367\n",
      "========================\n",
      "lr:4.3567e-04\n",
      "Epoch: 105 time 0:01:55.568879 train SE 13.185 test SE 12.947\n",
      "========================\n",
      "lr:4.3361e-04\n",
      "Epoch: 106 time 0:01:54.483769 train SE 13.374 test SE 11.880\n",
      "========================\n",
      "lr:4.3158e-04\n",
      "Epoch: 107 time 0:01:54.624393 train SE 12.908 test SE 12.394\n",
      "========================\n",
      "lr:4.2958e-04\n",
      "Epoch: 108 time 0:01:54.502718 train SE 12.690 test SE 12.982\n",
      "========================\n",
      "lr:4.2760e-04\n",
      "Epoch: 109 time 0:01:55.086676 train SE 13.131 test SE 12.653\n",
      "========================\n",
      "lr:4.2565e-04\n",
      "Epoch: 110 time 0:01:53.912803 train SE 13.226 test SE 13.049\n",
      "========================\n",
      "lr:4.2373e-04\n",
      "Epoch: 111 time 0:01:58.541933 train SE 12.763 test SE 12.621\n",
      "========================\n",
      "lr:4.2184e-04\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\CODE_WQF\\Model_driven_WMMSE.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m N\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)\n",
      "\u001b[1;32md:\\CODE_WQF\\Model_driven_WMMSE.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m b_x \u001b[39m=\u001b[39m b_x\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m num \u001b[39m=\u001b[39m b_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m out2 \u001b[39m=\u001b[39m net_BS(b_x, parm_set)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func1(b_x, out2, parm_set)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m train_SE \u001b[39m=\u001b[39m train_SE \u001b[39m-\u001b[39m loss\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch110\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\CODE_WQF\\Model_driven_WMMSE.ipynb Cell 7\u001b[0m in \u001b[0;36mRIS_SDMA_Precoding.forward\u001b[1;34m(self, H, parm_set)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m(k\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     B_pri \u001b[39m=\u001b[39m W[:,:,k]\u001b[39m.\u001b[39mreshape(batch,Nc,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mabs(U[:,:,k]\u001b[39m.\u001b[39mreshape(batch,Nc,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m hk \u001b[39m@\u001b[39m Hermitian(hk)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     B_pri \u001b[39m=\u001b[39m B_pri \u001b[39m+\u001b[39m (mu \u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39;49meye(K)\u001b[39m.\u001b[39;49mcuda()\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mcomplex128))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# B_pub = pub2[:,:,k].reshape(batch,Nc,1,1) * hk @ Hermitian(hk)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39m# B_pub = B_pub + (sigma_pub.reshape(batch,Nc,1,1) * torch.eye(K).cuda().to(torch.complex128))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m#二分法算mu\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CODE_WQF/Model_driven_WMMSE.ipynb#X10sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     B_pri \u001b[39m=\u001b[39m B_pri \u001b[39m+\u001b[39m W[:,:,k]\u001b[39m.\u001b[39mreshape(batch,Nc,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mabs(U[:,:,k]\u001b[39m.\u001b[39mreshape(batch,Nc,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m hk \u001b[39m@\u001b[39m Hermitian(hk)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "Nc = 32\n",
    "#N = 2  # 多径数\n",
    "Nt = 64 # 基站端天线数\n",
    "Nr = 1 # 用户端天线数\n",
    "# B = 30\n",
    "\n",
    "L = 8  # 用户端接收的观测，即OFDM个数\n",
    "SNR_dB = 10\n",
    "K = 2  # 用户数\n",
    "snr =  10**(SNR_dB/10)/K\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 180\n",
    "\n",
    "B = 40  # 反馈bit数\n",
    "N=6\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_SDMA_Precoding(parm_set,model_name_CSI_FB,Nc, N, Nt, B, Nr, L, SNR_dB, K, EPOCH, BATCH_SIZE):\n",
    "    \n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr = 10 ** (SNR_dB / 10) / K\n",
    "    parm_set = [Nc, Nt, Nr, snr, B, K]\n",
    "    \n",
    "    #数据集导入\n",
    "    train = 'H_train_6.mat'\n",
    "    mat = h5py.File(train)\n",
    "    H_train = mat['H_train']\n",
    "    H_train = np.transpose(H_train, [3, 2, 1, 0])\n",
    "    H_train = H_train.astype('float32')  # 训练变量类型转换\n",
    "    print(H_train.shape)\n",
    "    test = 'H_val_6.mat'\n",
    "    mat = h5py.File(test)\n",
    "    H_test = mat['H_val']\n",
    "    H_test = np.transpose(H_test, [3, 2, 1, 0])\n",
    "    H_test = H_test.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "    net_US = AutoEncoder(NUM_FEEDBACK_BITS,CHANNEL_SHAPE_DIM1,CHANNEL_SHAPE_DIM2,CHANNEL_SHAPE_DIM3)\n",
    "    net_US = torch.load(model_name_CSI_FB)\n",
    "\n",
    "    net_BS = RIS_SDMA_Precoding(parm_set).cuda() \n",
    "\n",
    "    opt = NoamOpt(256, 1, 4000, torch.optim.Adam(net_BS.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    best_SE = 0\n",
    "    torch_dataset_train = DatasetFolder(H_train)\n",
    "    loader_train = Data.DataLoader(\n",
    "        dataset=torch_dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "\n",
    "    torch_dataset_test = DatasetFolder(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=0,  # 每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print('========================')\n",
    "        print('lr:%.4e' % opt.optimizer.param_groups[0]['lr'])\n",
    "        train_SE = 0\n",
    "        num_train = 0\n",
    "        test_SE = 0\n",
    "        num_test = 0\n",
    "\n",
    "        for step, b_x in enumerate(loader_train):\n",
    "            num_train = num_train + 1\n",
    "            net_BS.train()  # 训练模式\n",
    "            net_US.eval()\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "\n",
    "            H1 = torch.zeros(num,K,Nc,2*Nt)  #信道反馈重构出来的信道\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for k in range(K):\n",
    "                    H0 = net_US(parm_set,b_x[:,k,:,:]).detach_()  #针对单用户的信道反馈\n",
    "                    H1[:,k,:,:] = H0\n",
    "            \n",
    "            #nmse =NMSE(H1,b_x)\n",
    "            out2 = net_BS(H1, parm_set)\n",
    "            loss = loss_func1(H1, out2, parm_set)\n",
    "            train_SE = train_SE - loss\n",
    "            opt.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        train_SE = train_SE / num_train\n",
    "        # scheduler_BS.step()\n",
    "        net_BS.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, b_x in enumerate(loader_test):\n",
    "                num_test = num_test + 1\n",
    "                net_BS.eval()  # 验证模式\n",
    "                net_US.eval()\n",
    "                b_x = b_x.cuda()\n",
    "                num = b_x.shape[0]\n",
    "                \n",
    "                H1 = torch.zeros(num,K,Nc,2*Nt)  #信道反馈重构出来的信道\n",
    "                with torch.no_grad():\n",
    "                    for k in range(K):\n",
    "                        H0 = net_US(parm_set,b_x[:,k,:,:]).detach_()  #针对单用户的信道反馈\n",
    "                        H1[:,k,:,:] = H0\n",
    "            \n",
    "                #nmse =NMSE(H1,b_x)\n",
    "                out2 = net_BS(H1, parm_set)\n",
    "                loss = loss_func1(H1, out2, parm_set)\n",
    "\n",
    "                test_SE = test_SE - loss\n",
    "            test_SE = test_SE / num_test\n",
    "\n",
    "        time0 = datetime.datetime.now() - start\n",
    "        print('Epoch:', epoch, 'time', time0, 'train SE %.3f' % train_SE.cpu(), 'test SE %.3f' % test_SE.cpu(),'test SE %.3f')\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        if test_SE > best_SE:\n",
    "            best_SE = test_SE\n",
    "            torch.save(net_BS,\n",
    "                       './Model_driven_WMMSE' + str(B) + 'B' + str(N) + 'Ncl' + str(L) + 'L' + str(K) + 'K' + '_8L_UPA_T01_Trans_warm.pth')\n",
    "            print('Model saved!')\n",
    "        shoulian[epoch] = test_SE.cpu()\n",
    "    print('The best SE is: %.3f' % best_SE.cpu())\n",
    "    print(shoulian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
